[{"content":"","date":null,"permalink":"/tags/argocd/","section":"Tags","summary":"","title":"Argocd"},{"content":"","date":null,"permalink":"/blog/","section":"Blogs","summary":"","title":"Blogs"},{"content":"","date":null,"permalink":"/tags/config-management-plugin/","section":"Tags","summary":"","title":"Config Management Plugin"},{"content":"","date":null,"permalink":"/tags/helm/","section":"Tags","summary":"","title":"Helm"},{"content":"Introduction #With the current version of Helm, it\u0026rsquo;s not possible to merge multiple values files. It will override the list from the preceding values file. While trying to solve this problem, I stumbled upon this custom plugin that can take a folder of values files and split them into individual files. This plugin is what I was looking for to solve my problem. Finally, I want to integrate this plugin into my ArgoCD instance.\nYou\u0026rsquo;ll need to perform the following steps:\nInstall Helm Plugin using InitContainer of the repo server. The configuration management plugin installation entails two steps. First, create a ConfigMap of your CMP configuration, then configure the plugin via sidecar for the repo server. For additional details, see the ArgoCD documentation. Finally, create an Argo Application to test the multi-values plugin. Prerequisites # Make sure you have access to the OpenShift cluster with cluster-admin privileges. The OpenShift GitOps Operator must be installed and configured It would help if you had some understanding of ArgoCD and Helm Charts. Install Helm Plugin in repo server #We\u0026rsquo;ll use the custom tooling methodology to add the helm multivalues plugin to the repo server. Update the ArgoCD operator CR to add an init container to the repo spec and volumes to install the helm plugin. We must also add Helm environment variables to override default data, config, and cache paths.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: argoproj.io/v1alpha1 kind: ArgoCD metadata: name: openshift-gitops namespace: openshift-gitops spec: repo: volumes: - configMap: name: helm-multivalues-plugin name: helm-multivalues-plugin - name: helm-custom-config emptyDir: {} env: - name: HELM_CACHE_HOME value: /.config/helm/cache - name: HELM_CONFIG_HOME value: /.config/helm/cache - name: HELM_DATA_HOME value: /.config/helm/data initContainers: - name: install-helm-multivalues-plugin image: registry.redhat.io/openshift-gitops-1/argocd-rhel8@sha256:baec6d73a77b832df8131bac3c5a86dc405ef89f600e27a22f164ed3c72816db command: [sh, -c] args: - \u0026#34;helm plugin install https://github.com/nico-ulbricht/helm-multivalues\u0026#34; env: - name: HELM_CACHE_HOME value: /helm-custom-config/cache - name: HELM_CONFIG_HOME value: /helm-custom-config/cache - name: HELM_DATA_HOME value: /helm-custom-config/data volumeMounts: - mountPath: /helm-custom-config name: helm-custom-config volumeMounts: - mountPath: /.config/helm name: helm-custom-config Configure Argo CMP #Next, create a ConfigMap with the configuration for the plugin. The generate command under spec section runs in the Application source directory each time manifests need to generate output. As per the plugin documentation, we need to run the helm multivalues [ORIGINAL_COMMAND] -f [VALUES_FOLDER_PATH] [ORIGINAL_ARGS] to generate the output as a stdout\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: ConfigMap metadata: name: helm-multivalues-plugin namespace: openshift-gitops data: plugin.yaml: | apiVersion: argoproj.io/v1alpha1 kind: ConfigManagementPlugin metadata: name: helm-multivalues-plugin spec: generate: command: [sh, -c, \u0026#39;helm multivalues template $ARGOCD_APP_NAME $ARGOCD_ENV_CHART_PATH --values $ARGOCD_ENV_VALUES_FILE -f $ARGOCD_ENV_VALUES_FOLDER\u0026#39;] The next step is to add the sidecar to the ArgoCD operator CR.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 apiVersion: argoproj.io/v1alpha1 kind: ArgoCD metadata: name: openshift-gitops namespace: openshift-gitops spec: repo: sidecarContainers: - name: helm-multivalues-plugin command: [/var/run/argocd/argocd-cmp-server] image: registry.redhat.io/openshift-gitops-1/argocd-rhel8@sha256:baec6d73a77b832df8131bac3c5a86dc405ef89f600e27a22f164ed3c72816db env: - name: HELM_CACHE_HOME value: /.config/helm/cache - name: HELM_CONFIG_HOME value: /.config/helm/cache - name: HELM_DATA_HOME value: /.config/helm/data volumeMounts: - mountPath: /var/run/argocd name: var-files - mountPath: /home/argocd/cmp-server/plugins name: plugins - mountPath: /tmp name: tmp - mountPath: /home/argocd/cmp-server/config/plugin.yaml subPath: plugin.yaml name: helm-multivalues-plugin - mountPath: /.config/helm name: helm-custom-config volumes: - configMap: name: helm-multivalues-plugin name: helm-multivalues-plugin - name: helm-custom-config emptyDir: {} env: - name: HELM_CACHE_HOME value: /.config/helm/cache - name: HELM_CONFIG_HOME value: /.config/helm/cache - name: HELM_DATA_HOME value: /.config/helm/data initContainers: - name: install-helm-multivalues-plugin image: registry.redhat.io/openshift-gitops-1/argocd-rhel8@sha256:baec6d73a77b832df8131bac3c5a86dc405ef89f600e27a22f164ed3c72816db command: [sh, -c] args: - \u0026#34;helm plugin install https://github.com/nico-ulbricht/helm-multivalues\u0026#34; env: - name: HELM_CACHE_HOME value: /helm-custom-config/cache - name: HELM_CONFIG_HOME value: /helm-custom-config/cache - name: HELM_DATA_HOME value: /helm-custom-config/data volumeMounts: - mountPath: /helm-custom-config name: helm-custom-config volumeMounts: - mountPath: /.config/helm name: helm-custom-config A few things to review:\nThe code snippet above uses an argocd image to mount the plugin we installed in the earlier step and plugin.yaml¬†in the container using the ConfigMap we just created. Other mounts from Argo CD are necessary, like¬†/var/run/argocd¬†(that contains the¬†argocd-cmp-server¬†command that needs to run) and¬†/home/argocd/cmp-server/plugins. Apply the changes; Operator will rollout a latest changes of the repo-server pod.\nArgo Application to Validate #Let\u0026rsquo;s set up an ArgoCD application to test the changes\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: tenant-gitops-config labels: type: config spec: destination: server: \u0026#34;https://kubernetes.default.svc\u0026#34; project: default source: path: . repoURL: \u0026#34;https://github.com/Vikaspogu/openshift-multicluster.git\u0026#34; targetRevision: main plugin: name: helm-multivalues-plugin env: - name: CHART_PATH value: helm/charts/tenant-gitops - name: VALUES_FOLDER value: kustomize/cluster-overlays/dev-acm/tenant-gitops-helm/apps - name: VALUES_FILE value: kustomize/cluster-overlays/dev-acm/tenant-gitops-helm/values.yaml syncPolicy: retry: backoff: duration: 120s factor: 3 maxDuration: 10m0s limit: 10 syncOptions: - SkipDryRunOnMissingResource=true - Validate=false - ApplyOutOfSyncOnly=true - RespectIgnoreDifferences=true - Prune=true Deployed Helm Chart Application Success! The Config Management Plugin ran the commands in the generate section to render out the YAML and Argo CD applied that YAML to the destination cluster.\nResources # Helm Chart Repository Helm MultiValues Folder ","date":"23 April 2024","permalink":"/blog/argo-config-plugin-helm-multivalues/","section":"Blogs","summary":"Introduction #With the current version of Helm, it\u0026rsquo;s not possible to merge multiple values files.","title":"Helm Multi-Values Plugin with Argo's CMP"},{"content":"","date":null,"permalink":"/tags/multi-values/","section":"Tags","summary":"","title":"Multi Values"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" My Unfiltered Journey Through the Ever-Evolving World of Evolving Technologies ","date":null,"permalink":"/","section":"Welcome to Tech Chronicles! üéâ","summary":" My Unfiltered Journey Through the Ever-Evolving World of Evolving Technologies ","title":"Welcome to Tech Chronicles! üéâ"},{"content":"Introduction #OpenShift Pipelines is a cloud-native CI/CD solution built on Tekton that allows you to define and run pipelines for your applications. Helm charts are a convenient way to package and deploy applications on Kubernetes, including OpenShift. To ensure the quality and reliability of Helm charts, it\u0026rsquo;s important to perform tests on them. In this blog, we will explore using the Chart Testing tool to perform tests on Helm charts within OpenShift Pipelines.\nPrerequisites #Before we begin, make sure you have the following prerequisites installed:\nOpenShift Cluster OpenShift Pipelines Operator installed Understanding of OpenShift Pipelines and Tasks Chart Testing Tasks # ct is the the tool for testing Helm charts. It is meant to be used for linting and testing pull requests. It automatically detects charts changed against the target branch.\nThe ct install command includes helm install and helm test; helm test performs port-forwarding to validate the installation\u0026rsquo;s success. Since the chart testing commands are run inside a pod, we need to ensure it can connect to the cluster to perform those tasks. If the pod cannot connect to the cluster, you\u0026rsquo;ll see the error below.\nThe connection to the server localhost:8080 was refused - did you specify the right host or port? Error printing details: failed waiting for process: exit status 1 Error printing logs: failed running process: exit status 1 Deleting release \u0026#34;deploy-jgyw6ew05y\u0026#34;... release \u0026#34;deploy-jgyw6ew05y\u0026#34; uninstalled ------------------------------------------------------------------------------------------------------------------------ ‚úñÔ∏é deploy =\u0026gt; (version: \u0026#34;0.1.0\u0026#34;, path: \u0026#34;deploy\u0026#34;) \u0026gt; failed running process: exit status 1 ------------------------------------------------------------------------------------------------------------------------ failed installing charts: failed processing charts Error: failed installing charts: failed processing charts To solve the above error, we\u0026rsquo;ll follow below steps:\nCreate a new service account and secret token from the service account. Grant appropriate permissions to the service account. Create a kubeconfig file using the kubeconfig creator task. By mounting the service account token as an environment variable from the secret in the task This task\u0026rsquo;s output will be the kubeconfig file stored in the workspace and shared across tasks. Create a new service account\noc create sa ct-helm-task Create a token secret from serviceaccount,\noc apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: ct-helm-task annotations: kubernetes.io/service-account.name: ct-helm-task type: kubernetes.io/service-account-token EOF Grant permissions to the service account\noc policy add-role-to-user edit -z ct-helm-task -n pipelines-helm The task will use the kubeconfigwriter image and the provided parameters to create a kubeconfig file that can be used by other tasks in the pipeline to access the target cluster. The kubeconfig will be placed at /workspace/\u0026lt;workspace-name\u0026gt;/kubeconfig.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion: tekton.dev/v1beta1 kind: Task metadata: annotations: tekton.dev/categories: \u0026#39;Deployment, Kubernetes\u0026#39; tekton.dev/displayName: kubeconfig creator tekton.dev/pipelines.minVersion: 0.12.1 tekton.dev/platforms: \u0026#39;linux/amd64,linux/s390x,linux/ppc64le\u0026#39; tekton.dev/tags: deploy name: kubeconfig-creator spec: params: - default: \u0026#39;\u0026#39; description: name of the cluster name: name type: string - description: address of the cluster name: url type: string - default: \u0026#39;\u0026lt;CLUSTER_URL\u0026gt;\u0026#39; description: bearer token for authentication to the cluster name: token type: string - default: \u0026#39;true\u0026#39; description: \u0026gt;- to indicate server should be accessed without verifying the TLS certificate name: insecure type: string steps: - args: - \u0026#39;-clusterConfig\u0026#39; - \u0026gt;- { \u0026#34;name\u0026#34;:\u0026#34;$(params.name)\u0026#34;, \u0026#34;url\u0026#34;:\u0026#34;$(params.url)\u0026#34;, \u0026#34;username\u0026#34;:\u0026#34;$(params.username)\u0026#34;, \u0026#34;password\u0026#34;:\u0026#34;$(params.password)\u0026#34;, \u0026#34;cadata\u0026#34;:\u0026#34;$(params.cadata)\u0026#34;, \u0026#34;clientKeyData\u0026#34;:\u0026#34;$(params.clientKeyData)\u0026#34;, \u0026#34;clientCertificateData\u0026#34;:\u0026#34;$(params.clientCertificateData)\u0026#34;, \u0026#34;namespace\u0026#34;:\u0026#34;$(params.namespace)\u0026#34;, \u0026#34;token\u0026#34;:\u0026#34;$(SA_TOKEN)\u0026#34;, \u0026#34;Insecure\u0026#34;:$(params.insecure) } - \u0026#39;-destinationDir\u0026#39; - $(workspaces.output.path) command: - /ko-app/kubeconfigwriter env: - name: SA_TOKEN valueFrom: secretKeyRef: key: token name: ct-helm-task image: \u0026gt;- gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/kubeconfigwriter@sha256:b2c6d0962cda88fb3095128b6202da9b0e6c9c0df3ef6cf7863505ffd25072fd name: write resources: {} workspaces: - name: output Finally, create a task using the chart-testing image. We\u0026rsquo;ll export the KUBECONFIG variable with the kubeconfig location from the workspace to the helm chart testing task.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 apiVersion: tekton.dev/v1beta1 kind: Task metadata: name: helm-ci-task spec: description: \u0026gt;- This task can be used to replace fields in YAML files. For example for altering helm charts on GitOps repos. workspaces: - name: source description: A workspace that contains the file which needs to be altered. params: - name: image type: string description: The chart testing image to use. default: quay.io/helmpack/chart-testing:v3.7.1 - name: chart-dirs type: string description: directory of the charts default: \u0026#34;charts/\u0026#34; - name: charts type: string description: name of the chart to test default: \u0026#34;\u0026#34; - name: namespace type: string description: namespace to run ci tests default: \u0026#34;helm-tekton-pipeline\u0026#34; - name: ct-config-file type: string description: config file for running ct commands default: \u0026#34;ct.yaml\u0026#34; steps: - name: ct-script image: quay.io/helmpack/chart-testing:v3.5.1 workingDir: $(workspaces.source.path) script: | ## export kubeconfig file from previous step export KUBECONFIG=\u0026#34;$(workspaces.source.path)/kubeconfig\u0026#34; ct lint-and-install --chart-dirs $(params.chart-dirs) --charts $(params.charts) --namespace $(params.namespace) --config $(params.ct-config-file) Setting Up OpenShift Pipeline #Create a pipeline resource to perform git-clone of the helm chart repository, then create a kubeconfig file and chart-testing task to run lint and install to validate the chart changes.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion: tekton.dev/v1beta1 kind: Pipeline metadata: name: helm-ct-pipeline spec: workspaces: - name: shared-workspace params: - name: git-url type: string description: url of the git repo for the code of deployment - name: git-revision type: string description: revision to be used from repo of the code for deployment default: master tasks: - name: fetch-repository taskRef: name: git-clone kind: ClusterTask workspaces: - name: output workspace: shared-workspace params: - name: url value: $(params.git-url) - name: subdirectory value: \u0026#34;\u0026#34; - name: deleteExisting value: \u0026#34;true\u0026#34; - name: revision value: $(params.git-revision) - name: kubeconfig-creator params: - name: url value: \u0026#39;https://api.cluster.example.com:6443\u0026#39; runAfter: - fetch-repository taskRef: kind: Task name: kubeconfig-creator workspaces: - name: output workspace: shared-workspace - name: run-ct-test taskRef: name: helm-ci-task kind: Task params: - name: charts value: deploy workspaces: - name: source workspace: shared-workspace runAfter: - kubeconfig-creator Conclusion #In this blog, we have demonstrated how to perform tests on Helm charts using the Chart Testing tool within OpenShift Pipelines. By setting up a pipeline with tasks for linting and testing, you can ensure the quality and reliability of your Helm charts before deploying them to your OpenShift cluster.\n","date":"17 March 2024","permalink":"/blog/chart-testing-tekton-pipelines/","section":"Blogs","summary":"Introduction #OpenShift Pipelines is a cloud-native CI/CD solution built on Tekton that allows you to define and run pipelines for your applications.","title":"Helm Chart Testing on OpenShift Pipelines"},{"content":"","date":null,"permalink":"/tags/cloudflared/","section":"Tags","summary":"","title":"Cloudflared"},{"content":"","date":null,"permalink":"/tags/openshift/","section":"Tags","summary":"","title":"OpenShift"},{"content":"Introduction #Cloudflare\u0026rsquo;s Tunnel, powered by Cloudflared, provides a secure way to expose your web applications to the internet without exposing your origin server\u0026rsquo;s IP address. This guide walks us through the steps to set up a Cloudflared tunnel on OpenShift, a popular container platform.\nThis setup will expose the OpenShift console and Ingress routes only, not the API server, for oc login Prerequisites # An OpenShift cluster, either on-premises or in a cloud provider Cloudflare account Basic knowledge of OpenShift and Cloudflare Install Cloudflared on Your Local Machine #Download and Install the Cloudflared binary for your operating system from the Cloudflare website.\nbrew install cloudflared Verify the installation\n$ cloudflared version cloudflared version 2024.2.1 (built 2024-02-20T16:25:25Z) Create a Cloudflare Tunnel #Authenticate Cloudflared: A browser window will open and prompt you to log in to your Cloudflare account. After logging in, select your hostname.\ncloudflared tunnel login Create a tunnel and give it a name. From the command\u0026rsquo;s output, note the Tunnel\u0026rsquo;s UUID and the path to your Tunnel\u0026rsquo;s credentials file.\ncloudflared tunnel create openshift Configure Cloudflared for OpenShift #Create an OpenShift secret from the Tunnel\u0026rsquo;s credentials file.\noc create secret generic cloudflared --from-file ~/.cloudflared/\u0026lt;tunnel_id\u0026gt;.json To deploy the cloudflared tunnel image, I\u0026rsquo;ll be using a helm chart\nCreate a values.yaml with a cloudflared configuration.\nreplicaCount: 3 image: repository: docker.io/cloudflare/cloudflared tag: \u0026#34;2024.2.1\u0026#34; cloudflared: tunnelID: \u0026#34;\u0026lt;REPLACE_WITH_TUNNEL_ID\u0026gt;\u0026#34; existingSecret: cloudflared ingress: - hostname: \u0026#34;oauth-openshift.apps.\u0026lt;DOMAIN\u0026gt;\u0026#34; originRequest: noTLSVerify: true service: https://oauth-openshift.openshift-authentication - hostname: \u0026#34;*.apps.\u0026lt;DOMAIN\u0026gt;\u0026#34; originRequest: noTLSVerify: true service: https://router-internal-default.openshift-ingress.svc.cluster.local - service: http_status:404 Install helm chart\nhelm repo add xunholy-charts https://xunholy.github.io/charts/ helm install cloudflared xunholy-charts/cloudflared -f values.yaml -n networking Cloudflared will authenticate with Cloudflare using the credentials in the configuration file and establish a secure tunnel to your OpenShift service.\nVerify that cloudflared pods are running.\noc get pods NAME READY STATUS RESTARTS AGE cloudflared-fd8478945-l7xhc 1/1 Running 0 119m cloudflared-fd8478945-v9nq5 1/1 Running 0 119m cloudflared-fd8478945-wt2z7 1/1 Running 0 119m Create a DNS record #Follow this guide to create a DNS record for the tunnel. For the CNAME record name, add *.apps.\u0026lt;DOMAIN_NAME\u0026gt; with the target as \u0026lt;TUNNEL_ID\u0026gt;,cfargotunnel.com\nVerify the Tunnel #Open a web browser and navigate to the OpenShift console.\nCongratulations! You have successfully set up a Cloudflared tunnel on OpenShift, which allows you to expose your web application securely to the internet.\n","date":"11 March 2024","permalink":"/blog/cloudflared-tunnel-openshift/","section":"Blogs","summary":"Introduction #Cloudflare\u0026rsquo;s Tunnel, powered by Cloudflared, provides a secure way to expose your web applications to the internet without exposing your origin server\u0026rsquo;s IP address.","title":"Setting Up Cloudflared Tunnel on OpenShift"},{"content":"","date":null,"permalink":"/tags/backstage/","section":"Tags","summary":"","title":"Backstage"},{"content":"In this continuation of our getting started with Developer Hub series, we\u0026rsquo;re diving into the art of crafting templates. These templates will not only streamline your workflow but also serve as the cornerstone for automated deployments.\nHigh-level overview # Overview The developer initiates the workflow by leveraging the backstage self-service template. This template, in turn, pulls essential skeleton resources from a dedicated software template repository, ensuring consistency and best practices. The developer\u0026rsquo;s actions trigger the creation of a pull request to the GitOps repository, introducing version-controlled changes to the infrastructure. An admin reviews and approves the pull request. Then, ArgoCD, a continuous delivery tool, provides the database resources according to the approved changes. This seamless integration of backstage, GitOps, and ArgoCD accelerates the development lifecycle. It fosters a collaborative environment where developers and administrators work harmoniously to deliver robust and scalable database solutions.\nDefine Your Infrastructure #Before diving into templates, define the infrastructure you want to provision. In our case, let\u0026rsquo;s consider provisioning a PostgreSQL database using CloudNative-PG. Let\u0026rsquo;s start with the installation of the CloudNative-PG operator on OpenShift cluster.\nPrerequisites # Ensure you have access to the cluster with cluster-admin privileges. Verify access to the OpenShift Container Platform web console. Procedure # Log in to the OpenShift Container Platform web console. Navigate to Operators ‚Üí OperatorHub. Enter cloudnativepg into the filter box. Select the CloudNativePG and click Install. CloudnativePG Operator CloudnativePG Operator installation Backstage Software Template #Create a software template under a kind Template. This template will automate the process of opening a GitHub pull request for provisioning.\nI recommend referring to documentation on creating templates and using Template Editor from the developer hub for quick feedback.\nTemplate Editor Menu Template Editor forms The outcome of the template would be as follows, based on the flow we discussed in the overview. Feel free to modify the YAML as required and test the changes in the template form editor for quicker feedback\n--- apiVersion: scaffolder.backstage.io/v1beta3 kind: Template metadata: name: postgresql-cluster title: Postgresql Cluster template description: scaffolder v1beta3 template to create pgsql cluster on existing git repository tags: - database - postgresql links: - title: Documentation url: https://backstage.io/docs/features/software-templates icon: docs - title: Source url: https://github.com/Vikaspogu/software-templates/blob/main/scaffolder-templates/pgsql-cluster/template.yaml icon: github spec: owner: group:ops type: service parameters: - title: Postgresql Definition required: - namespace properties: environment: title: Select environment type: string default: dev enum: - prod - stage - dev enumNames: - \u0026#39;production (prod)\u0026#39; - \u0026#39;staging (stage)\u0026#39; - \u0026#39;development (dev)\u0026#39; cluster: title: Select openshift cluster type: string default: acm enum: - acm - dev - prod enumNames: - \u0026#39;acm (acm)\u0026#39; - \u0026#39;development (dev)\u0026#39; - \u0026#39;production (prod)\u0026#39; namespace: title: Namespace type: string description: Namespace of the postgresql cluster create_namespace: title: Create a namespace type: boolean instances: title: Instances type: integer description: Number of instances required in the cluster default: 1 storage_size: title: Storage Size type: string description: Size of the storage (ex. 1Gi) default: 1Gi owner: title: Owner type: string description: Owner of the component ui:field: OwnerPicker ui:options: catalogFilter: kind: Group - title: Provide information about the GitHub location required: - repoOwner - repoName - owner properties: repoOwner: title: Repository Owner type: string default: vikaspogu owner: title: Owner type: string description: Owner of the component repoName: title: Repository Name type: string default: cluster-components steps: - id: fetch-base name: Fetch Base action: fetch:template input: url: ./skeleton values: cluster: ${{parameters.cluster}} environment: ${{parameters.environment}} name: ${{parameters.namespace}}-${{parameters.environment}}-db namespace: ${{parameters.namespace}}-${{parameters.environment}} instances: ${{parameters.instances}} storage_size: ${{parameters.storage_size}} create_namespace: ${{parameters.create_namespace}} repoOwner: ${{ parameters.repoOwner }} repoName: ${{ parameters.repoName }} owner: ${{ parameters.owner }} applicationType: resources - id: publish name: Create a pull request action: publish:github:pull-request input: repoUrl: github.com?owner=${{ parameters.repoOwner }}\u0026amp;repo=${{ parameters.repoName }} branchName: pqsql/${{parameters.namespace}}-${{parameters.environment}} title: \u0026#34;Create a new pgsql database in ${{parameters.namespace}}\u0026#34; description: | ### Requesting new pgsql database in ${{parameters.namespace}} This pull request is generated by IDP catalog - id: label_pr name: Add labels to PR action: github:issues:label input: repoUrl: github.com?owner=${{ parameters.repoOwner }}\u0026amp;repo=${{ parameters.repoName }} number: \u0026#39;${{ steps.publish.output.pullRequestNumber }}\u0026#39; labels: - postgresql - created-by-backstage - ${{ parameters.environment }} - ${{parameters.namespace}} output: links: - title: \u0026#39;Pull request link\u0026#39; url: ${{ steps.publish.output.remoteUrl }} icon: github Adding template #To incorporate the template files, I\u0026rsquo;m adopting a static location configuration. In this approach, we specify the template URL within the catalog section of the app-config-rhdh configmap, as demonstrated below\ncatalog: rules: - allow: [Component, System, API, Resource, Location, Template] locations: - type: url target: https://github.com/Vikaspogu/software-templates/blob/main/scaffolder-templates/pgsql-cluster/template.yaml rules: - allow: [Template] After updating the configmap, recreate the pod. Once the pod is ready, you\u0026rsquo;ll discover the template in the create menu.\nAvailable Templates Choose the Template Fill out information Create template Success Pull request ArgoCD Application Manifest #Create an ArgoCD application manifest that corresponds to your Backstage template. This manifest specifies the desired state of your application.\napiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cluster-components namespace: openshift-gitops labels: type: config spec: destination: server: \u0026#34;https://kubernetes.default.svc\u0026#34; project: default source: directory: recurse: true exclude: \u0026#34;**/catalog-info.yaml\u0026#34; path: acm repoURL: \u0026#34;https://github.com/Vikaspogu/cluster-components\u0026#34; syncPolicy: automated: prune: true selfHeal: true syncOptions: - CreateNamespace=true - RespectIgnoreDifferences=true - ApplyOutOfSyncOnly=true Test Your Workflow #Merge the pull request generated from the template, then witness ArgoCD autonomously deploying the changes according to the updated desired state. Confirm that the PostgreSQL database cluster has been provisioned as anticipated.\nConclusion #Congratulations! You\u0026rsquo;ve now crafted a powerful template that seamlessly integrates Backstage and ArgoCD. This template not only simplifies your deployment workflow but also introduces automation through GitHub pull requests.\n","date":"14 January 2024","permalink":"/blog/developer-hub-getting-started-part-2/","section":"Blogs","summary":"In this continuation of our getting started with Developer Hub series, we\u0026rsquo;re diving into the art of crafting templates.","title":"Getting Started with Red Hat Developer Hub - Part 2"},{"content":"","date":null,"permalink":"/tags/idp/","section":"Tags","summary":"","title":"IDP"},{"content":"","date":null,"permalink":"/tags/red-hat-developer-hub/","section":"Tags","summary":"","title":"Red Hat Developer Hub"},{"content":"Introduction #Red Hat Developer Hub is based on Spotify\u0026rsquo;s open-sourced backstage project. Backstage is an open-source developer portal and a platform for building developer experiences. It\u0026rsquo;s designed to centralize and streamline various aspects of the software development life cycle, providing a unified platform for developers, product managers, and other stakeholders. Here are some key features and advantages of Spotify Backstage: Unified Platform, Service Catalog, Documentation Hub, Plugin Architecture and more.\nToday, I\u0026rsquo;m excited to share my learnings on the journey of getting started with the Red Hat Developer Hub. Join me as we explore the how to deploy it using Helm chart, enabling diverse plugins, and configuring integrations with GitHub, ArgoCD, Open Cluster Management. To add a cherry on top, we\u0026rsquo;ll create a software template that opens a GitHub pull request to provision a database cluster using CloudNative-PG.\nInstallation #Deploying the Red Hat Developer Hub is a breeze, and Helm charts make it a cinch. First, add the helm chart repo for the developer hub. Then, we\u0026rsquo;ll download the values yaml since we need to update a few values accordingly.\nhelm repo add openshift-helm-charts https://charts.openshift.io/ helm show values openshift-helm-charts/redhat-developer-hub \u0026gt; values.yaml Update the global.clusterRouterBase according to OpenShift router host and modify values if needed\n1 2 3 4 5 6 7 8 global: auth: backend: enabled: true existingSecret: \u0026#34;\u0026#34; value: \u0026#34;\u0026#34; clusterRouterBase: apps.example.com ... Install the helm chart with modified values\noc new-project developer-hub helm install developer-hub developer-hub/developer-hub -f values.yaml Wait for the developer hub and postgresql database pods to be in ready status\noc get pods NAME READY STATUS RESTARTS AGE developer-hub-64d8cff99c-k8k9r 1/1 Running 0 2d developer-hub-postgresql-0 1/1 Running 0 2d Navigate to the developer hub in a web browser by grabbing the route host information as shown below\noc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD developer-hub developer-hub-developer-hub.apps.example.com / developer-hub http-backend edge/Redirect None Voila, you have successfully deployed the developer hub!\nPlugins #Red Hat Developer Hub\u0026rsquo;s power lies in its extensibility through plugins. Let\u0026rsquo;s delve into enabling some key plugins\nIn this post, I\u0026rsquo;ll we will cover how to integrate below plugins\nKubernetes GitHub for authentication Open Cluster management to show cluster details ArgoCD for Sync status Enable plugins #Developer Hub images are pre-packaged with some dynamic plugins. Most of these plugins are disabled by default since they must be configured. To enable plugins, add a package, set the disabled to false in the Helm Chart values under the dynamic plugins section\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 global: auth: backend: enabled: true existingSecret: \u0026#34;\u0026#34; value: \u0026#34;\u0026#34; clusterRouterBase: apps.example.com dynamic: includes: - dynamic-plugins.default.yaml plugins: - disabled: false package: ./dynamic-plugins/dist/backstage-plugin-catalog-backend-module-github-dynamic - disabled: false package: ./dynamic-plugins/dist/roadiehq-backstage-plugin-security-insights - disabled: false package: ./dynamic-plugins/dist/backstage-plugin-kubernetes-backend-dynamic - disabled: false package: ./dynamic-plugins/dist/backstage-plugin-kubernetes - disabled: false package: ./dynamic-plugins/dist/roadiehq-backstage-plugin-argo-cd-backend-dynamic - disabled: false package: ./dynamic-plugins/dist/roadiehq-scaffolder-backend-argocd-dynamic - disabled: false package: ./dynamic-plugins/dist/roadiehq-backstage-plugin-argo-cd - disabled: false package: ./dynamic-plugins/dist/janus-idp-backstage-plugin-ocm - disabled: false package: ./dynamic-plugins/dist/janus-idp-backstage-plugin-ocm-backend-dynamic - disabled: false package: ./dynamic-plugins/dist/roadiehq-scaffolder-backend-module-utils-dynamic ... Re-install the chart with updated values\nhelm upgrade --install developer-hub developer-hub/developer-hub -f values.yaml Verify if plugins are installed in the install-dynamic-plugins init container within the Developer Hub pod‚Äôs log\n======= Skipping disabled dynamic plugin ./dynamic-plugins/dist/backstage-plugin-scaffolder-backend-module-gitlab-dynamic ======= Installing dynamic plugin ./dynamic-plugins/dist/backstage-plugin-kubernetes-backend-dynamic ==\u0026gt; Grabbing package archive through `npm pack` ==\u0026gt; Removing previous plugin directory /dynamic-plugins-root/backstage-plugin-kubernetes-backend-dynamic-0.13.0 ==\u0026gt; Extracting package archive /dynamic-plugins-root/backstage-plugin-kubernetes-backend-dynamic-0.13.0.tgz ==\u0026gt; Removing package archive /dynamic-plugins-root/backstage-plugin-kubernetes-backend-dynamic-0.13.0.tgz ==\u0026gt; Merging plugin-specific configuration ==\u0026gt; Successfully installed dynamic plugin /opt/app-root/src/dynamic-plugins/dist/backstage-plugin-kubernetes-backend-dynamic ======= Installing dynamic plugin ./dynamic-plugins/dist/backstage-plugin-kubernetes ==\u0026gt; Grabbing package archive through `npm pack` ==\u0026gt; Removing previous plugin directory /dynamic-plugins-root/backstage-plugin-kubernetes-0.11.0 ==\u0026gt; Extracting package archive /dynamic-plugins-root/backstage-plugin-kubernetes-0.11.0.tgz ==\u0026gt; Removing package archive /dynamic-plugins-root/backstage-plugin-kubernetes-0.11.0.tgz ==\u0026gt; Merging plugin-specific configuration ==\u0026gt; Successfully installed dynamic plugin /opt/app-root/src/dynamic-plugins/dist/backstage-plugin-kubernetes ======= Installing dynamic plugin ./dynamic-plugins/dist/janus-idp-backstage-plugin-topology Plugin Integrations # In the spirit of brevity and to ensure accuracy, I\u0026rsquo;ve posted the configuration code snippets for your convenience. However, for a more detailed and nuanced understanding of the integration process, I highly recommend referring to the official product documentation.\nIntegration code snippets are added to the configmap; for reference, here is the complete configmap Externalize configuration #Create a configmap to add configuration to the developer hub as described in the documentation\nConfigMap Mount the ConfigMap, add extraAppConfig to the values file under upstream.backstage section\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 upstream: backstage: appConfig: app: baseUrl: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; backend: baseUrl: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; cors: origin: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; database: connection: password: ${POSTGRESQL_ADMIN_PASSWORD} user: postgres auth: keys: - secret: ${BACKEND_SECRET} args: - \u0026#34;--config\u0026#34; - dynamic-plugins-root/app-config.dynamic-plugins.yaml command: [] extraAppConfig: - configMapRef: app-config-rhdh filename: app-config-rhdh.yaml Re-install the chart with updated values\nhelm upgrade --install developer-hub developer-hub/developer-hub -f values.yaml GitHub Integration #Seamlessly connect your projects with GitHub by configuring the integration.\nCreate a secret named rhdh-secrets as mentioned in documentation with GitHub clientId, secret and token\noc create secret generic rhdh-secrets --from-literal=GITHUB_APP_CLIENT_ID=dummy --from-literal=GITHUB_APP_CLIENT_SECRET=dummy --from-literal=GITHUB_TOKEN=dummy Mount the rhdh-secrets, add extraEnvVarsSecrets to the values file under upstream.backstage section\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 upstream: backstage: appConfig: app: baseUrl: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; backend: baseUrl: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; cors: origin: \u0026#39;https://{{- include \u0026#34;janus-idp.hostname\u0026#34; . }}\u0026#39; database: connection: password: ${POSTGRESQL_ADMIN_PASSWORD} user: postgres auth: keys: - secret: ${BACKEND_SECRET} args: - \u0026#34;--config\u0026#34; - dynamic-plugins-root/app-config.dynamic-plugins.yaml command: [] extraEnvVarsSecrets: - rhdh-secrets extraAppConfig: - configMapRef: app-config-rhdh filename: app-config-rhdh.yaml Update the app-config-rhdh configmap and add below configuration\nintegrations: github: - host: github.com token: ${GITHUB_TOKEN} auth: # see https://backstage.io/docs/auth/ to learn about auth providers environment: development providers: github: development: clientId: ${GITHUB_APP_CLIENT_ID} clientSecret: ${GITHUB_APP_CLIENT_SECRET} Re-installing the chart will recreate a new pod with updated values and configmap\nhelm upgrade --install developer-hub developer-hub/developer-hub -f values.yaml GitHub Auth Navigating Clusters with Open Cluster Management #Open Cluster Management extends our reach to manage and monitor multiple clusters effortlessly. To enable we can add kubernetes configuration and reference the cluster in catalog provider\nWe are setting up authentication for the Kubernetes cluster using the serviceaccount token. Create a serviceaccount in the developer-hub namespace\noc create sa developer-hub -n developer-hub Obtain a long-lived token by creating a secret\noc apply -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Secret metadata: name: developer-hub-sa namespace: developer-hub annotations: kubernetes.io/service-account.name: developer-hub type: kubernetes.io/service-account-token EOF Add a new entry in the extraEnvVars to mount the serviceaccount token secret as an environment variables in the values file under upstream.backstage section\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 upstream: backstage: ... extraEnvVarsSecrets: - rhdh-secrets extraAppConfig: - configMapRef: app-config-rhdh filename: app-config-rhdh.yaml extraEnvVars: - name: BACKEND_SECRET valueFrom: secretKeyRef: key: backend-secret name: \u0026#39;{{ include \u0026#34;janus-idp.backend-secret-name\u0026#34; $ }}\u0026#39; - name: POSTGRESQL_ADMIN_PASSWORD valueFrom: secretKeyRef: key: postgres-password name: \u0026#39;{{- include \u0026#34;janus-idp.postgresql.secretName\u0026#34; . }}\u0026#39; - name: KUBE_TOKEN valueFrom: secretKeyRef: key: token name: developer-hub-sa Update the app-config-rhdh configmap with below configuration\nkubernetes: serviceLocatorMethod: type: \u0026#34;multiTenant\u0026#34; clusterLocatorMethods: - type: \u0026#34;config\u0026#34; clusters: - url: https://api.example.com:6443 name: acm authProvider: \u0026#34;serviceAccount\u0026#34; skipTLSVerify: true serviceAccountToken: ${KUBE_TOKEN} dashboardApp: openshift dashboardUrl: https://console-openshift-console.apps.example.com/ catalog: providers: ocm: default: kubernetesPluginRef: acm #same as the clusters name in kubernetes section name: multiclusterhub owner: group:ops schedule: frequency: seconds: 10 timeout: seconds: 60 Re-installing the chart will recreate a new pod with updated values and mount serviceaccount token secret as an environment variable\nhelm upgrade --install developer-hub developer-hub/developer-hub -f values.yaml Multicluster Synchronizing with ArgoCD #The ArgoCD Backstage plugin provides synced, health status and updates the history of your services to your Developer Portal.\nGenerate a new token from the ArgoCD instance\nArgoCD token Create a new secret from the ArgoCD token\noc create secret generic argocd-token --from-literal=ARGOCD_TOKEN=dummy Add a new entry in the extraEnvVars to mount the secret as an environment variables in the values file under upstream.backstage section\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 upstream: backstage: ... extraEnvVarsSecrets: - rhdh-secrets extraAppConfig: - configMapRef: app-config-rhdh filename: app-config-rhdh.yaml extraEnvVars: - name: BACKEND_SECRET valueFrom: secretKeyRef: key: backend-secret name: \u0026#39;{{ include \u0026#34;janus-idp.backend-secret-name\u0026#34; $ }}\u0026#39; - name: POSTGRESQL_ADMIN_PASSWORD valueFrom: secretKeyRef: key: postgres-password name: \u0026#39;{{- include \u0026#34;janus-idp.postgresql.secretName\u0026#34; . }}\u0026#39; - name: KUBE_TOKEN valueFrom: secretKeyRef: key: token name: developer-hub-sa - name: ARGOCD_TOKEN valueFrom: secretKeyRef: key: ARGOCD_TOKEN name: argocd-token Update the app-config-rhdh configmap with below configuration\nargocd: appLocatorMethods: - instances: - name: main token: ${ARGOCD_TOKEN} url: https://openshift-gitops-server-openshift-gitops.apps.example.com type: config Re-installing the chart\nhelm upgrade --install developer-hub developer-hub/developer-hub -f values.yaml ArgoCD Conclusion #üõ†Ô∏è That\u0026rsquo;s a Wrap for Part 1! Stay Tuned for Part 2\nIn Part 1, we\u0026rsquo;ve navigated through installation, enabled plugins, configured the integration, and set the stage for a seamless developer experience.\nIn our upcoming Part 2, we\u0026rsquo;ll dive into the magic of software template creation.\n","date":"13 January 2024","permalink":"/blog/developer-hub-getting-started-part-1/","section":"Blogs","summary":"Introduction #Red Hat Developer Hub is based on Spotify\u0026rsquo;s open-sourced backstage project.","title":"Getting Started with Red Hat Developer Hub - Part 1"},{"content":"Introduction #In the ever-evolving landscape of cloud-native technologies, staying up-to-date with the latest software versions is crucial for ensuring optimal performance, security, and access to new features. This blog post will guide you through the process of upgrading a PostgreSQL database instance from version 15 to version 16 in a cloud-native environment.\nBefore You Begin #Before diving into the upgrade process, it\u0026rsquo;s essential to perform a few preliminary steps to ensure a smooth transition:\nBackup Your Database #Begin by creating a comprehensive backup of your PostgreSQL database. This step is critical in case anything goes wrong during the upgrade process, allowing you to restore your database to its previous state.\nCheck Compatibility #Verify that your applications and any other dependencies are compatible with PostgreSQL 16. Review release notes and documentation to identify any potential issues that may arise.\nUpdate Extensions and Dependencies #Make sure that all extensions and dependencies are compatible with PostgreSQL 16. Update them as needed to avoid compatibility issues after the upgrade.\nUpgrading Cloud-Native PostgreSQL Database #Review Release Notes #Start by carefully reviewing the release notes for PostgreSQL 16. Take note of any changes, new features, and potential issues that may affect your database.\nStop Database Activity #Before initiating the upgrade, stop all database activity to prevent any data inconsistencies or corruption during the process. Notify users in advance to minimize disruptions.\nUpgrading PostgreSQL Cluster #This post doesn‚Äôt cover in-place upgrades; instead, it will require you to spin a newer postgresql cluster with version 16 and bootstrap from another cluster.\nUse the new Postgres version 16 as the cluster image\napiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: upgrade-cluster-16 spec: instances: 3 imageName: ghcr.io/cloudnative-pg/postgresql:16.1-13 Add external cluster configuration to perform a monolith import using the version 15 cluster as the connection parameters.\napiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: upgrade-cluster-16 spec: instances: 3 imageName: ghcr.io/cloudnative-pg/postgresql:16.1-13 externalClusters: - name: cluster-pg96 connectionParameters: # Use the correct IP or host name for the source database host: pg96.local user: postgres dbname: postgres password: name: cluster-pg96-superuser key: password Configure bootstrap source with cluster name from above\napiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: upgrade-cluster-16 spec: instances: 3 imageName: ghcr.io/cloudnative-pg/postgresql:16.1-13 externalClusters: - name: cluster-pg96 connectionParameters: # Use the correct IP or host name for the source database host: pg96.local user: postgres dbname: postgres password: name: cluster-pg96-superuser key: password bootstrap: initdb: import: type: monolith databases: - \u0026#34;*\u0026#34; roles: - \u0026#34;*\u0026#34; source: externalCluster: cluster-pg96 Add configuration to perform backup after the bootstrap import to an object store\napiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: upgrade-cluster-16 spec: instances: 3 imageName: ghcr.io/cloudnative-pg/postgresql:16.1-13 externalClusters: - name: cluster-pg96 connectionParameters: # Use the correct IP or host name for the source database host: pg96.local user: postgres dbname: postgres password: name: cluster-pg96-superuser key: password bootstrap: initdb: import: type: monolith databases: - \u0026#34;*\u0026#34; roles: - \u0026#34;*\u0026#34; source: externalCluster: cluster-pg96 backup: barmanObjectStore: destinationPath: \u0026#34;\u0026lt;destination path here\u0026gt;\u0026#34; serverName: upgrade-cluster-16 s3Credentials: accessKeyId: name: aws-creds key: ACCESS_KEY_ID secretAccessKey: name: aws-creds key: ACCESS_SECRET_KEY retentionPolicy: \u0026#34;30d\u0026#34; Apply changes to the cluster and wait for the cluster to be in a healthy state\nkubectl apply -f upgrade-cluster-16.yaml Finally, swap the bootstrap recovery source to the new backup generated from earlier steps\napiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: name: main spec: instances: 3 imageName: ghcr.io/cloudnative-pg/postgresql:16.1-13 bootstrap: recovery: source: upgrade-cluster-16 Delete the cluster created in steps 1-4\nkubectl delete -f upgrade-cluster-16.yaml Conclusion #Upgrading a cloud-native PostgreSQL database instance from version 15 to 16 requires careful planning and execution. By following the steps outlined in this blog post, you can ensure a seamless transition, taking advantage of the latest features and improvements while maintaining the integrity of your data. Keep in mind that each cloud provider may have specific guidelines or tools for managing PostgreSQL upgrades, so consult their documentation for additional guidance.\n","date":"12 January 2024","permalink":"/blog/cloudnative-pg-upgrade/","section":"Blogs","summary":"Introduction #In the ever-evolving landscape of cloud-native technologies, staying up-to-date with the latest software versions is crucial for ensuring optimal performance, security, and access to new features.","title":" A Seamless Upgrade: Cloud-Native PostgreSQL Database Version 15 to 16"},{"content":"","date":null,"permalink":"/tags/cloudnative-pg/","section":"Tags","summary":"","title":"Cloudnative-Pg"},{"content":"","date":null,"permalink":"/tags/postgresql/","section":"Tags","summary":"","title":"Postgresql"},{"content":"","date":null,"permalink":"/tags/ansible/","section":"Tags","summary":"","title":"Ansible"},{"content":"","date":null,"permalink":"/tags/custom-execution-image/","section":"Tags","summary":"","title":"Custom Execution Image"},{"content":"","date":null,"permalink":"/tags/sops/","section":"Tags","summary":"","title":"Sops"},{"content":"Introduction #Today, I am excited to share how to set up an Ansible execution environment to control SOPS secrets. The Ansible execution image is a pre-built container containing all the necessary components for decrypting SOPS secrets in Ansible playbooks.\nAnsible Builder #Ansible Builder is a tool that can help with building the image\nCreate a working directory #Let\u0026rsquo;s start by creating a new working directory\nmkdir custom-ansible-execution; cd custom-ansible-execution Builder definition #Create an execution-environment.yml file. ansible-builder build container images with the definition file execution-environment.yml\nIn Ansible Builder, the `additional_build_steps`` configuration allows you to specify additional commands or steps to be executed during the build process. This can be useful for performing custom tasks, installing specific dependencies, or executing scripts as part of the Ansible collection build.\nHighlighted code block shows how to add SOPS rpm package via additional build steps\n1 2 3 4 5 6 7 8 9 10 version: 1 build_arg_defaults: EE_BASE_IMAGE: \u0026#39;registry.redhat.io/ansible-automation-platform-22/ee-supported-rhel8:latest\u0026#39; additional_build_steps: prepend: | RUN rpm -i https://github.com/mozilla/sops/releases/download/v3.7.3/sops-3.7.3-1.x86_64.rpm \u0026amp;\u0026amp; \\ sops -v \u0026amp;\u0026amp; \\ mkdir -p /home/runner/.config/sops/age dependencies: galaxy: requirements.yml Ansible collections #Create and add the SOPS collection to the requirements.yml\ncollections: - community.sops Create context files #ansible-buidler create Creates a build context, which can be used by podman to build an image.\nansible-builder create Complete! The build context can be found at: custom-ansible-execution/context Build and push image #Build the container image using Podman and push to a registry\npodman build -f context/Containerfile -t localhost/custom-ee:1.0 context podman push ... Execution environments in jobs #Follow this guide to add an execution environment in Jobs\nMount options #Mount the SOPS keys to decrypt using Execution environment mount options\nCopy sops keys #Finally, SSH into the ansible controller and change to awx user to copy the key into the appropriate directory\nssh aap-controller sudo su - awx sudo mkdir -p /var/lib/awx/.sops-key sudo mv keys.txt /var/lib/awx/.sops-key Thank you for taking the time to read my blog post. Until next time, take care!\n","date":"29 April 2023","permalink":"/blog/ansible-sops/","section":"Blogs","summary":"Introduction #Today, I am excited to share how to set up an Ansible execution environment to control SOPS secrets.","title":"SOPS Ansible Execution Image"},{"content":"","date":null,"permalink":"/tags/upgrade/","section":"Tags","summary":"","title":"Upgrade"},{"content":"I have been running an instance of bitnami\u0026rsquo;s version of Postgresql 12 for a while, and I have recently upgraded/migrated data from version 12 to 15.\nThis post doesn\u0026rsquo;t cover in-place upgrades; instead, it will require you to spin a newer container version.\nDeploy New Container Image #The first step is to deploy a new Postgres container using the updated image version.\nThis container MUST NOT mount the same volume as the older Postgres container.\nInstead, it will need to mount a new volume for the database. The new Postgres server will fail if you mount the older Postgres server volume. This is because Postgres requires the data to be migrated before it can load it.\nBackup #Install the PostgreSQL command line tool on your local machine\nbrew install PostgreSQL Start Kubernetes port-forward for postgres service to expose the port on the local system to perform a data dump\nkubectl port-forward svc/postgresql-kube 5432 \u0026amp; Run pg_dumpall, a utility for writing out (dumping) all of your PostgreSQL databases of a cluster\npg_dumpall -h 127.0.0.1 -p 5432 -U postgres -W -f database.sql If everything goes successfully, a new file is created on the local filesystem.\nImport #We\u0026rsquo;ll use the Kubernetes copy(cp) command to transfer the dump file into the container.\nkubectl cp database.sql postgresql-0:/tmp/database.sql Open a terminal shell on the new postgresql container.\nkubectl exec -it postgresql-0 bash Finally, import the databases.\nI have no name!@postgresql-0:/$ psql -U postgres -W -f /tmp/database.sql Verify the import and stop the old container. You have completed the upgrade successfully now. Congrats!\n","date":"21 January 2023","permalink":"/blog/upgrading-postgresql-image-versions-kubernetes/","section":"Blogs","summary":"I have been running an instance of bitnami\u0026rsquo;s version of Postgresql 12 for a while, and I have recently upgraded/migrated data from version 12 to 15.","title":"Upgrading PostgreSQL in Kubernetes"},{"content":"It is common for organizations to send logs to multiple systems‚Äîfor example, container logs to NewRelic, and audit logs to Splunk for the InfoSec team.\nDeploying NewRelic using helm chart will also deploy the NewRelic logging deamonset, which utilizes a custom Fluent Bit image with a NewRelic output plugin to forward logs easily. Since the NewRelic Fluent Bit image uses an upstream fluent bit as a base image, it is pre-baked with the Splunk plugin.\nNow, let\u0026rsquo;s look at how to configure both NewRelic for container logs and Splunk for audit logs using the helm chart.\nThree things have to be configured in helm chart values\nConfigure tail input for audit log paths Update the Match tag in the Kubernetes filter to match all records that start with audit.* Configure Splunk output with HEC token and host details Configure audit input #Input Plugins gather information from different sources; some collect data from log files. tail input plugin allows monitoring one or several text files. It has similar behavior to the tail -f shell command.\nIn this case, we want to tail /var/log/audit/audit.log; to configure new input in NewRelic logging. Add tail input for audit logs in values.yaml file under the fluentBit config -\u0026gt; inputs section, as shown below.\nfluentBit: config: inputs: | [INPUT] Name tail Tag kube.* Path ${PATH} Parser ${LOG_PARSER} DB ${FB_DB} Mem_Buf_Limit 7MB Skip_Long_Lines On Refresh_Interval 10 [INPUT] Name tail Tag audit.* Path \u0026#34;/var/log/audit/audit.log\u0026#34; Parser ${LOG_PARSER} DB ${FB_DB} Mem_Buf_Limit 7MB Skip_Long_Lines On Refresh_Interval 10 Update Match rule #A Match represents a simple rule to select Events whose Tags match a defined rule.\nIn this scenario, we are checking the audit.* tags. Only one unique filter is allowed; we\u0026rsquo;ll append it to the existing kube.* tag.\nfluentBit: config: [FILTER] Name kubernetes Match kube.* audit.* Kube_URL https://kubernetes.default.svc.cluster.local:443 Buffer_Size ${K8S_BUFFER_SIZE} K8S-Logging.Exclude ${K8S_LOGGING_EXCLUDE} [FILTER] Name record_modifier Match * Record cluster_name ${CLUSTER_NAME} Configure Splunk output #The output interface allows the definition of destinations for the data. For example, our destination for audit logs is Splunk.\nTo configure new output in NewRelic logging. Add splunk output for audit.* logs in the values.yaml file under the fluentBit config -\u0026gt; outputs section, as shown below.\nfluentBit: config: outputs: | [OUTPUT] Name newrelic Match * licenseKey ${LICENSE_KEY} endpoint ${ENDPOINT} lowDataMode ${LOW_DATA_MODE} Retry_Limit ${RETRY_LIMIT} [OUTPUT] Name splunk Match audit.* Host splunk-hec.host Port 443 TLS On Splunk_Token ${SPLUNK_TOKEN} #Create a secret with Splunk token and mount into container Finally, recreate the logging pods. Audit logs should be routed to Splunk along with container logs to NewRelic.\n","date":"11 January 2023","permalink":"/blog/openshift-audit-logs-splunk-newrelic-fluent-bit/","section":"Blogs","summary":"It is common for organizations to send logs to multiple systems‚Äîfor example, container logs to NewRelic, and audit logs to Splunk for the InfoSec team.","title":"Forward Openshift Audit Logs to Splunk using Newrelic Fluent Bit"},{"content":"I am a big fan of FluxCD and its integration with secrets management; however, recently, I decided to tinker with ArgoCD. One of the challenges I encountered was secrets management. Although ArgoCD provides flexible integration with most secrets management tools, it requires little extra configuration.\nSo I started my journey to configure ArgoCD with SOPS; there isn\u0026rsquo;t a direct integration with SOPS. So we will have to use the Kustomize plugin called ksops, which is the suggested tool on Argo\u0026rsquo;s website and it has pretty good instructions for Argo integration.\nWhat is KSOPS? #KSOPS, or kustomize-SOPS, is a kustomize plugin for managing SOPS encrypted resources. KSOPS is used to decrypt any Kubernetes resources but is most commonly used to decrypt encrypted Kubernetes Secrets and ConfigMaps. The main goal of KSOPS is to manage encrypted resources the same way we manage the Kubernetes manifests.\nSetup #I am setting up this integration on my OpenShift cluster and deploying ArgoCD via Operator. For file encryption, I\u0026rsquo;ll use Age encryption tool recommended over PGP by SOPS documentation.\nGenerate age key #Install age package\nGenerating an age key using age-keygen:\n$ age-keygen -o age.agekey Public key: age1helqcqsh9464r8chnwc2fzj8uv7vr5ntnsft0tn45v2xtz0hpfwq98cmsg Create a secret with the age private key; the key name must end with .agekey to be detected as an age key: Secret #Create a kubernetes secret from the age key in openshift-gitops project\n$ cat age.agekey | kubectl create secret generic sops-age --namespace=openshift-gitops \\ --from-file=key.txt=/dev/stdin Configuration #Update the `spec.repo`` definition of ArgoCD CR to configure KSOPS custom tooling. Below configuration mounts the age secret and installs the KSOPS tool using initContainer\nrepo: env: - name: XDG_CONFIG_HOME value: /.config - name: SOPS_AGE_KEY_FILE value: /.config/sops/age/keys.txt volumes: - name: custom-tools emptyDir: {} - name: sops-age secret: secretName: sops-age initContainers: - name: install-ksops image: viaductoss/ksops:v3.0.2 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: - \u0026#39;echo \u0026#34;Installing KSOPS...\u0026#34;; cp ksops /custom-tools/; cp $GOPATH/bin/kustomize /custom-tools/; echo \u0026#34;Done.\u0026#34;;\u0026#39; volumeMounts: - mountPath: /custom-tools name: custom-tools volumeMounts: - mountPath: /usr/local/bin/kustomize name: custom-tools subPath: kustomize - mountPath: /.config/kustomize/plugin/viaduct.ai/v1/ksops/ksops name: custom-tools subPath: ksops - mountPath: /.config/sops/age/keys.txt name: sops-age subPath: keys.txt Here is a guide for creating and testing the resources\n","date":"10 August 2022","permalink":"/blog/argo-operator-ksops-age/","section":"Blogs","summary":"I am a big fan of FluxCD and its integration with secrets management; however, recently, I decided to tinker with ArgoCD.","title":"ArgoCD with Kustomize and KSOPS using Age encryption"},{"content":"","date":null,"permalink":"/tags/gitops/","section":"Tags","summary":"","title":"Gitops"},{"content":"I recently started using proxmox and was planning on monitoring metrics using InfluxDB and Grafana, which I have already deployed on my Kubernetes cluster. However, during that process, I encountered two issues:\nAuthentication \u0026ldquo;Database not found\u0026rdquo; Authentication #For the Authentication issue, I found out that you have to use the Authorization header with the Token yourAuthToken value to access the bucket. So, configure the header name in the jsonData field, and we should configure the header value in secureJsonData for grafana helm chart values.\nsecureJsonData: httpHeaderValue1: \u0026#39;Token ${INFLUXDB_TOKEN}\u0026#39; jsonData: httpHeaderName1: \u0026#39;Authorization\u0026#39; Complete datasources helm values\ndatasources: datasources.yaml: apiVersion: 1 deleteDatasources: - name: Loki orgId: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-operated:9090 access: proxy isDefault: true - name: InfluxDB type: influxdb url: \u0026#34;http://influxdb.default.svc.cluster.local:80\u0026#34; database: \u0026#34;default\u0026#34; secureJsonData: httpHeaderValue1: \u0026#39;Token ${INFLUXDB_TOKEN}\u0026#39; access: proxy jsonData: httpHeaderName1: \u0026#39;Authorization\u0026#39; httpMode: GET \u0026ldquo;Database not found\u0026rdquo; #This one took me a while to figure out. In InfluxDB v1, data gets stored in databases and retention policies. In InfluxDB v2, data gets stored in buckets. Because InfluxQL uses the v1 data model, before querying in InfluxQL, we must map a bucket to a database and retention policy. Find more information here.\nAs shown below, I am using the InfluxDB API method to create DBRP mappings for unmapped buckets.\ncurl --request POST http://localhost:8086/api/v2/dbrps \\ --header \u0026#34;Authorization: Token YourAuthToken\u0026#34; \\ --header \u0026#39;Content-type: application/json\u0026#39; \\ --data \u0026#39;{ \u0026#34;bucketID\u0026#34;: \u0026#34;00oxo0oXx000x0Xo\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;example-db\u0026#34;, \u0026#34;default\u0026#34;: true, \u0026#34;orgID\u0026#34;: \u0026#34;00oxo0oXx000x0Xo\u0026#34;, \u0026#34;retention_policy\u0026#34;: \u0026#34;example-rp\u0026#34; }\u0026#39; Make sure to update Host, Token, bucketID, database, orgID, retention_policy with your values.\nLastly, go to Grafana, choose the InfluxDB data source, and try to run a query to see that the setup process was successful.\n","date":"8 August 2022","permalink":"/blog/influxdb-grafana/","section":"Blogs","summary":"I recently started using proxmox and was planning on monitoring metrics using InfluxDB and Grafana, which I have already deployed on my Kubernetes cluster.","title":"Configure Influxdb with Grafana"},{"content":"","date":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana"},{"content":"","date":null,"permalink":"/tags/influxdb/","section":"Tags","summary":"","title":"Influxdb"},{"content":"","date":null,"permalink":"/tags/nas/","section":"Tags","summary":"","title":"NAS"},{"content":"","date":null,"permalink":"/tags/pxe/","section":"Tags","summary":"","title":"PXE"},{"content":"In this post, I have documented the steps I followed to install RHEL 8 by booting from a PXE server over the network with a Kickstart file using Synology NAS as TFTP, and HTTP server.\nConfigure TFTP Install \u0026amp; Configure HTTP server PXE Boot Prepare Installation Kickstart file PXE boot TFTP #Trivial File Transfer Protocol (TFTP) is a simple file transfer protocol, generally used for transferring configurations or boot files when authentication is not required.\nSynology NAS comes with TFTP file services.\nConfigure TFTP #Go to Control Panel \u0026gt; File Services \u0026gt; Advanced\nSelect Enable TFTP Service In the TFTP root folder field, specify which folder on the Synology NAS can be accessed by TFTP clients HTTP server #We also need a service to host our image repository; we can use FTP or HTTP, or NFS to host our image repository. Synology NAS comes with web hosting features. In Web Station, we will create a port-based virtual host to host the contents of our image repository.\nInstall Packages #Install the following packages from the Synology package center\nWeb Station Apache HTTP Server 2.4 Configure #Open Web Station and go to Web Service Portal \u0026gt; Select Create Service Portal\nSelect Virtual Host\nIn the virtual host wizard\nSelect Port-based with protocol as HTTP and enter port number 8080 Select the Document root directory, which is the exact PXE location as TFTP Select Apache HTTP Server as the back-end server Click Create to save settings We need to enable simple directory browsing on the virtual host to access the contents of images and configuration files to perform PXE Boot.\nIn the Document root directory, create a file named .htaccess with the following content.\nOptions +Indexes Navigate to IP:8080, and you should see a similar screen with files in the folder.\nPXE Boot #Prepare Installation # Download the ISO image file of the complete installation DVD Create a directory named images/rhel/8.6 under the root directory of the TFTP/HTTP server Mount the ISO image Copy all the files from the ISO to the previously created directory on TFTP/HTTP server. You will need around 10GB of storage to copy all the files from the ISO to the local directory. mount rhel-8.6-x86_64-dvd.iso /mnt scp -pr /mnt/* nas-user@nas-address:/media/PXE/images/rhel/8.6 To perform the UEFI PXE Boot installation, we will need the following PXE boot files.\ngrubx64.efi provided by grub2-efi-x64 rpm shimx64.efi provided by shim-x64 rpm BOOTX64.EFI provided by shim-x64 rpm cp -pr /mnt/BaseOS/Packages/grub2-efi-x64-2.02-81.el8.x86_64.rpm /tmp cp -pr /mnt/BaseOS/Packages/shim-x64-15-11.el8.x86_64.rpm /tmp/ Extract the packages\ncd /tmp rpm2cpio shim-x64-15-11.el8.x86_64.rpm | cpio -dimv rpm2cpio grub2-efi-x64-2.02-81.el8.x86_64.rpm | cpio -dimv Copy the EFI boot images from the /tmp directory to the PXE root directory\nscp /tmp/boot/efi/EFI/BOOT/BOOTX64.EFI nas-user@nas-address:/media/PXE scp /tmp/boot/efi/EFI/centos/shimx64.efi nas-user@nas-address:/media/PXE scp /tmp/boot/efi/EFI/centos/grubx64.efi nas-user@nas-address:/media/PXE Add a configuration file named grub.cfg to the TFTP/HTTP root directory. We need initrd and vmlinuz files to load the Operating System until the hard disk and other interfaces are detected. In the grub.cfg take note of linuxefi and initrdefi. Please update it to TFTP/HTTP image directory location.\nset timeout=30 menuentry \u0026#39;Install RHEL 8.6 on T7910\u0026#39; { linuxefi images/rhel/8.6/images/pxeboot/vmlinuz inst.stage2=http://192.168.100.160:8080/images/rhel/8.6 quiet initrdefi images/rhel/8.6/images/pxeboot/initrd.img } Kickstart file #Next, we will create our kickstart file for an unattended automated installation. I have used the kickstart configuration tool available at https://access.redhat.com/labs/kickstartconfig/ in the Red Hat Customer Portal Labs. This tool will walk you through basic configuration and allows you to download the resulting Kickstart file.\nCreate a new directory named ks under the root directory of the TFTP/HTTP server to store the kickstart file for the UEFI PXE Boot purpose Copy the downloaded kickstart file into the previously created location scp kickstart.cfg nas-user@nas-address:/media/PXE/ks Update the grub.cfg to use the kickstart file during installation. For the inst.ks= boot option, specify kickstart location The final grub.cfg will look like this:\nset timeout=30 menuentry \u0026#39;Install RHEL 8.6 on T7910\u0026#39; { linuxefi images/rhel/8.6/images/pxeboot/vmlinuz inst.ks=http://192.168.100.160:8080/ks/rhel8-t7910.cfg inst.stage2=http://192.168.100.160:8080/images/rhel/8.6 quiet initrdefi images/rhel/8.6/images/pxeboot/initrd.img } PXE boot #We are ready to perform UEFI PXE Boot. The shortcut button to boot over the network may vary for different hardware, F12 on most common hardware.\nIf your UEFI PXE boot server configuration is accurate, then the TFTP files should acquire successfully.\n","date":"13 May 2022","permalink":"/blog/tftp-udm-pxe/","section":"Blogs","summary":"In this post, I have documented the steps I followed to install RHEL 8 by booting from a PXE server over the network with a Kickstart file using Synology NAS as TFTP, and HTTP server.","title":"PXE boot with Synology NAS"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/categories/dict/","section":"Categories","summary":"","title":"Dict"},{"content":"","date":null,"permalink":"/categories/helm/","section":"Categories","summary":"","title":"Helm"},{"content":"Recently I was in a situation where I ended up writing multiple if-else statements in the helm template. As a result, the code block wasn\u0026rsquo;t clean, and I began to explore alternative ways to achieve the same behavior concisely.\nHelm dict function perfectly fits my use case.\nBefore\n{{- if eq .Values.imageType \u0026#34;java\u0026#34; }} name: openjdk:latest {{- else if eq .Values.imageType \u0026#34;nodejs\u0026#34; }} name: nodejs:latest {{- else if eq .Values.imageType \u0026#34;golang\u0026#34; }} name: golang:latest {{- else if eq .Values.imageType \u0026#34;python\u0026#34; }} name: python:latest {{- end }} After\n{{- $imageTypes := dict }} {{- $_ := set $imageTypes \u0026#34;java\u0026#34; \u0026#34;openjdk:latest\u0026#34; }} {{- $_ := set $imageTypes \u0026#34;nodejs\u0026#34; \u0026#34;nodejs:latest\u0026#34; }} {{- $_ := set $imageTypes \u0026#34;python\u0026#34; \u0026#34;openjdk:latest\u0026#34; }} {{- $_ := set $imageTypes \u0026#34;java\u0026#34; \u0026#34;openjdk:latest\u0026#34; }} ... name: {{ get $imageTypes .Values.imageType }} ","date":"8 July 2021","permalink":"/blog/helm-dict-function/","section":"Blogs","summary":"Recently I was in a situation where I ended up writing multiple if-else statements in the helm template.","title":"Helm alternative for multiple If-Else conditions"},{"content":"There are three different strategies to build multi-platform images that Buildx and Dockerfiles support:\nUsing the QEMU emulation support in the kernel Building on multiple native nodes using the same builder instance Using a stage in Dockerfile to cross-compile to different architectures I\u0026rsquo;ll focus on the first option, cross-building with emulation using buildx.\ndocker buildx build --platform linux/amd64,linux/arm64 . Setup #Steps might differ from platform to platform, but the pipelines will remain the same. So I\u0026rsquo;ve constructed a basic buildx setup for the TektonCD task.\nPipeline #I am taking the upstream docker-build task and modifying it to my needs\nWhat\u0026rsquo;s modified\nEnable DOCKER_BUILDKIT and DOCKER_CLI_EXPERIMENTAL Install docker-buildx plugin Run the latest docker/binfmt tag to use its qemu parts Docker login (using secret for values) Docker build with docker buildx build --platform Create a secret holding docker username and token kubectl create secret generic docker-token \\ --from-literal=username=\u0026#34;${CONTAINER_REGISTRY_USER}\u0026#34; \\ --from-literal=password=\u0026#34;${CONTAINER_REGISTRY_PASSWORD}\u0026#34; apiVersion: tekton.dev/v1beta1 kind: ClusterTask metadata: name: docker-buildx labels: app.kubernetes.io/version: \u0026#34;0.1\u0026#34; annotations: tekton.dev/pipelines.minVersion: \u0026#34;0.12.1\u0026#34; tekton.dev/tags: docker, build-image, push-image, dind, buildx, multi-arch tekton.dev/displayName: docker-buildx spec: description: \u0026gt;- This task will build and push an image using docker buildx. The task will build an out image out of a Dockerfile. This image will be pushed to an image registry. The image will be built and pushed using a dind sidecar over TCP+TLS. params: - name: image description: Reference of the image docker will produce. - name: builder_image description: The location of the docker builder image. default: docker.io/library/docker:19.03.14 - name: dockerfile description: Path to the Dockerfile to build. default: ./Dockerfile - name: context description: Path to the directory to use as context. default: . - name: build_extra_args description: Extra parameters passed for the build command when building images. default: \u0026#34;--platform linux/amd64,linux/arm/v7 --no-cache\u0026#34; - name: push_extra_args description: Extra parameters passed for the push command when pushing images. default: \u0026#34;--push\u0026#34; - name: insecure_registry description: Allows the user to push to an insecure registry that has been specified default: \u0026#34;\u0026#34; - name: docker-token-secret type: string description: name of the secret holding the docker-token default: docker-token workspaces: - name: source results: - name: IMAGE_DIGEST description: Digest of the image just built. steps: - name: buildx-build-push image: $(params.builder_image) env: # Connect to the sidecar over TCP, with TLS. - name: DOCKER_HOST value: tcp://localhost:2376 # Verify TLS. - name: DOCKER_TLS_VERIFY value: \u0026#39;1\u0026#39; # Use the certs generated by the sidecar daemon. - name: DOCKER_CERT_PATH value: /certs/client - name: DOCKERHUB_USER valueFrom: secretKeyRef: name: $(params.docker-token-secret) key: username - name: DOCKERHUB_PASS valueFrom: secretKeyRef: name: $(params.docker-token-secret) key: password workingDir: $(workspaces.source.path) script: | # install depends apk add curl jq # enable experimental buildx features export DOCKER_BUILDKIT=1 export DOCKER_CLI_EXPERIMENTAL=enabled # Download latest buildx bin from github mkdir -p ~/.docker/cli-plugins/ BUILDX_LATEST_BIN_URI=$(curl -s -L https://github.com/docker/buildx/releases/latest | grep \u0026#39;linux-amd64\u0026#39; | grep \u0026#39;href\u0026#39; | sed \u0026#39;s/.*href=\u0026#34;/https:\\/\\/github.com/g; s/amd64\u0026#34;.*/amd64/g\u0026#39;) curl -s -L ${BUILDX_LATEST_BIN_URI} -o ~/.docker/cli-plugins/docker-buildx chmod a+x ~/.docker/cli-plugins/docker-buildx # Get and run the latest docker/binfmt tag to use its qemu parts BINFMT_IMAGE_TAG=$(curl -s https://registry.hub.docker.com/v2/repositories/docker/binfmt/tags | jq \u0026#39;.results | sort_by(.last_updated)[-1].name\u0026#39; -r) docker run --rm --privileged docker/binfmt:${BINFMT_IMAGE_TAG} docker context create tls-environment # create the multibuilder docker buildx create --name multibuilder --use tls-environment docker buildx use multibuilder # login to a registry echo ${DOCKERHUB_PASS} | docker login -u ${DOCKERHUB_USER} --password-stdin # build the containers and push them to the registry then display the images docker buildx build $(params.build_extra_args) \\ -f $(params.dockerfile) -t $(params.image) $(params.context) $(params.push_extra_args) volumeMounts: - mountPath: /certs/client name: dind-certs sidecars: - image: docker:19.03.14-dind name: server args: - --storage-driver=vfs - --userland-proxy=false - --debug securityContext: privileged: true env: # Write generated certs to the path shared with the client. - name: DOCKER_TLS_CERTDIR value: /certs volumeMounts: - mountPath: /certs/client name: dind-certs # Wait for the dind daemon to generate the certs it will share with the # client. readinessProbe: periodSeconds: 1 exec: command: [\u0026#39;ls\u0026#39;, \u0026#39;/certs/client/ca.pem\u0026#39;] volumes: - name: dind-certs emptyDir: {} Make sure you have a base image that supports multi-arch like alpine or one of these base images\n","date":"19 January 2021","permalink":"/blog/tekton-multiarch-buildx/","section":"Blogs","summary":"There are three different strategies to build multi-platform images that Buildx and Dockerfiles support:","title":"Build multi-arch docker image using Tekton"},{"content":"","date":null,"permalink":"/tags/buildx/","section":"Tags","summary":"","title":"Buildx"},{"content":"","date":null,"permalink":"/tags/cel/","section":"Tags","summary":"","title":"Cel"},{"content":"","date":null,"permalink":"/tags/multi-arch/","section":"Tags","summary":"","title":"Multi-Arch"},{"content":"","date":null,"permalink":"/tags/tekton/","section":"Tags","summary":"","title":"Tekton"},{"content":"Tekton Triggers work by having EventListeners receive incoming webhook notifications, processing them using an Interceptor, and creating Kubernetes resources from templates if the interceptor allows it, with the extraction of fields from the body of the webhook\nCEL Interceptors can filter or modify incoming events. For example, you can truncate the commit id from the webhook body.\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: shared-listener namespace: default spec: serviceAccountName: build-bot triggers: - name: shared-pipeline-trigger interceptors: - cel: overlays: - key: intercepted.commit_id_short expression: \u0026#34;body.head_commit.id.truncate(7)\u0026#34; bindings: - ref: pipeline-binding template: ref: pipeline-template The applied overlay uses an extension body in the binding. As shown in below example as $(extensions.\u0026lt;overlay_key\u0026gt;)\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: pipeline-binding namespace: default spec: params: - name: git-repo-name value: $(body.repository.name) - name: git-repo-url value: $(body.repository.url) - name: image-tag value: $(extensions.intercepted.commit_id_short) ","date":"19 January 2021","permalink":"/blog/tekton-triggers-cel-interception/","section":"Blogs","summary":"Tekton Triggers work by having EventListeners receive incoming webhook notifications, processing them using an Interceptor, and creating Kubernetes resources from templates if the interceptor allows it, with the extraction of fields from the body of the webhook","title":"Tekton triggers and Interceptors"},{"content":"","date":null,"permalink":"/tags/triggers/","section":"Tags","summary":"","title":"Triggers"},{"content":"In this post, I will:\nUse Tekton to build and publish an image to the docker registry Trigger a Tekton pipeline from GitHub Use ArgoCD to deploy an application Getting Started #Pre-requisites:\nAn OpenShift 4 cluster with ArgoCD and OpenShift Pipelines installed. If not, you follow instructions to OpenShift Pipelines and ArgoCD Operator Basic understanding of ArgoCD and Tekton concepts Create an OpenShift project called node-web-project\noc new-project node-web-project Using Private Registries #Create a docker secret with registry authentication details\noc create secret docker-registry container-registry-secret \\ --docker-server=$CONTAINER_REGISTRY_SERVER \\ --docker-username=$CONTAINER_REGISTRY_USER \\ --docker-password=$CONTAINER_REGISTRY_PASSWORD -n node-web-project Create a service account named build-bot\noc create sa -n node-web-project build-bot serviceaccount/build-bot created Add docker secret container-registry-secret to newly created service account build-bot\noc patch serviceaccount build-bot \\ -p \u0026#39;{\u0026#34;secrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;container-registry-secret\u0026#34;}]}\u0026#39; serviceaccount/build-bot patched Verify if the service account has the secret added:\noc get sa -n node-web-project build-bot -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2021-01-12T03:34:32Z\u0026#34; name: build-bot namespace: node-web-project resourceVersion: \u0026#34;53879\u0026#34; selfLink: /api/v1/namespaces/node-web-project/serviceaccounts/build-bot uid: 628067fd-91d1-4cdd-b6a6-88b4f7280ff0 secrets: - name: container-registry-secret - name: build-bot-token-8nl2v Create Pipeline #A Pipeline is a collection of tasks you want to run as part of your workflow. Each Task in a Pipeline gets executed in a pod and runs in parallel by default. However, you can specify the order by using runAfter.\nThe below pipeline consists of three tasks.\nCloning source code Build and push images using the Buildah cluster task Synchronize Argo deployment apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: node-web-app-pipeline namespace: node-web-project spec: workspaces: - name: shared-workspace params: - name: node-web-app-source - name: node-web-app-image tasks: - name: fetch-repository taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-workspace params: - name: url value: $(params.node-web-app-source) - name: build-and-publish-image params: - name: IMAGE value: $(params.node-web-app-image) - name: TLSVERIFY value: \u0026#39;false\u0026#39; taskRef: kind: ClusterTask name: buildah runAfter: - fetch-repository workspaces: - name: source workspace: shared-workspace - name: sync-application taskRef: name: argocd-task-sync-and-wait params: - name: application-name value: node-web-app - name: flags value: --insecure --grpc-web - name: argocd-version value: v1.7.11 runAfter: - build-and-publish-image Triggers #Now that the pipeline is ready, the next step is to set up a GitHub webhook to trigger the pipeline. But, first, we need to create the following resources:\nA TriggerTemplate act as a blueprint for pipeline resources. We can also use it to define parameters that can be substituted anywhere within the resource template(s).\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: pipeline-template namespace: node-web-project spec: params: - name: git-repo-url - name: git-repo-name - name: git-revision - name: image-name resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: $(tt.params.git-repo-name)-pipeline-run- namespace: node-web-project spec: params: - name: source value: $(tt.params.git-repo-url) - name: image value: $(tt.params.image-name) pipelineRef: name: node-web-app-pipeline serviceAccountName: build-bot timeout: 1h0m0s workspaces: - name: shared-workspace persistentVolumeClaim: claimName: tekton-workspace-pvc A TriggerBinding binds the incoming event data to the template (i.e., git URL, repo name, revision, etc\u0026hellip;.)\napiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: pipeline-binding namespace: node-web-project spec: params: - name: git-repo-name value: $(body.repository.name) - name: git-repo-url value: $(body.repository.url) - name: git-revision value: $(body.head_commit.id) - name: image-name value: docker.io/vikaspogu/$(body.repository.name):$(body.head_commit.id) An EventListener that will create a pod application bringing together a binding and a template\napiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: el-node-web-app namespace: default spec: serviceAccountName: build-bot triggers: - name: node-web-app-trigger bindings: - ref: pipeline-binding template: ref: pipeline-template An OpenShift Route to expose the Event Listener\n$ oc expose svc el-node-web-app $ echo \u0026#34;URL: $(oc get route el-node-web-app --template=\u0026#39;http://{{.spec.host}}\u0026#39;)\u0026#34; http://el-node-web-app-node-web-project.apps.cluster-7d51.sandbox659.opentlc.com You can learn about Tekton Triggers and OpenShift Pipelines\nCreate ArgoCD App for Web App Resources #Create an ArgoCD application via the GUI or command line\nProject: default cluster: (URL Of your OpenShift Cluster) namespace should be the name of your OpenShift Project repo URL: git repo Target Revision: Head PATH: deployment AutoSync Disabled Configure Webhooks #GitHub webHook for Tekton event listener to start a Tekton build on git push.\nMake a code change and commit, look at the build # Push an empty commit to the repo\ngit commit -m \u0026#34;empty-commit\u0026#34; --allow-empty \u0026amp;\u0026amp; git push In OpenShift Console, you should see a pipeline run\nOnce the pipeline is finished. Use the OpenShift route to verify the app\nWorkspace PVC #It would be best if you had a ReadWriteMany PVC to use with multiple pipelines.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: tekton-workspace-pvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 2Gi ","date":"14 January 2021","permalink":"/blog/tekton-argocd-gitops/","section":"Blogs","summary":"In this post, I will:","title":"GitOps with Tekton and ArgoCD"},{"content":"","date":null,"permalink":"/tags/openshift-pipelines/","section":"Tags","summary":"","title":"OpenShift-Pipelines"},{"content":"","date":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker"},{"content":"","date":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux"},{"content":"Docker buildx on Linux #Installation instructions for Ubuntu\nTo execute docker commands without sudo. We need to add the username to the docker group.\nFind the current username by typing who Add username to docker group Reboot system $ who vikaspogu :0 2020-12-05 10:10 (:0) $ sudo usermod -aG docker vikaspogu $ sudo reboot Enable buildx #Create a new config.json file under the ~.docker folder and enable the experimental feature\n$ mkdir ~/.docker $ vim ~/.docker/config.json { \u0026#34;experimental\u0026#34;: \u0026#34;enabled\u0026#34; } Now you should be able to do multi-arch builds\ndocker buildx build -t vikaspogu/test:v0.0.1 . --push --platform linux/arm64 If you encounter the below error when trying to build the image using buildx. It would help if you created new build instance\nfailed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to load LLB: runtime execution on platform linux/arm64 not supported Create build instance #Follow the below steps to create a new build instance for arm64.\nList existing buildx platforms Create a new build instance named arm64 Inspect build instance to verify arm64 platform exists Set builder instance Build multi-platform image $ docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS default docker default default running linux/amd64, linux/386 # create the builder $ docker buildx create --name amr64 --platform linux/amd64,linux/arm64 # start the buildx $ docker buildx inspect arm64 --bootstrap Name: arm64 Driver: docker-container Nodes: Name: arm640 Endpoint: unix:///var/run/docker.sock Status: running Platforms: linux/amd64*, linux/arm64*, linux/riscv64, linux/ppc64le, linux/386, linux/arm/v7, linux/arm/v6, linux/s390x # set current builder instance $ docker buildx use amr64 # build the multi images $ docker buildx build -t vikaspogu/test:v0.0.1 . --push --platform linux/arm64 If you encounter /bin/sh: Invalid ELF image for this architecture error. Run the following docker image and then build\ndocker run --rm --privileged multiarch/qemu-user-static --reset -p yes ","date":"5 December 2020","permalink":"/blog/docker-buildx-setup/","section":"Blogs","summary":"Docker buildx on Linux #Installation instructions for Ubuntu","title":"Setting up docker buildx on Linux"},{"content":"","date":null,"permalink":"/tags/boot/","section":"Tags","summary":"","title":"Boot"},{"content":"First, make sure you have secure boot disabled from the firmware settings. Once you are in the grub command line, type ls to list all partitions\ngrub\u0026gt;ls (hd0) (hd0,gpt1) (hd1) (hd1,gpt8) (cd0)) Type ls (cd0) to get the UUID of the device.\ngrub\u0026gt;ls (hd0,gpt1) Partition hd0,gpt1: Filesystem type fat - Label `CES_X64FREV`, UUID 4099-DBD9 Partition start-512 Sectors... Note the UUID of your USB drive, shown in the above command\nType the following commands\ninsmod part_gpt insmod fat insmod search_fs_uuid insmod chain search --fs-uuid --set=root 409-DBD9 Replace the UUID of your device\nNow we select the EFI file to boot from\nchainloader /efi/boot/bootx64.efi boot That\u0026rsquo;s it; that should boot the USB drive.\n","date":"1 December 2020","permalink":"/blog/boot-from-usb-through-grub-menu/","section":"Blogs","summary":"First, make sure you have secure boot disabled from the firmware settings.","title":"Boot from USB through grub menu"},{"content":"","date":null,"permalink":"/tags/grub/","section":"Tags","summary":"","title":"Grub"},{"content":"","date":null,"permalink":"/tags/authentication/","section":"Tags","summary":"","title":"Authentication"},{"content":"","date":null,"permalink":"/tags/authorization/","section":"Tags","summary":"","title":"Authorization"},{"content":"In this article, I will share the setup for enabling Authentication and Authorization in OpenShift Service Mesh with Keycloak.\nInstalling OpenShift Service Mesh #Follow the Installing Red Hat OpenShift Service Mesh guide for setup\nEnable the following configuration in your ServiceMeshControlPlane resource\nStrict mTLS across the mesh Automatic istio route creation apiVersion: maistra.io/v2 kind: ServiceMeshControlPlane spec: version: v1.1 security: controlPlane: mtls: true gateways: OpenShiftRoute: enabled: true Keycloak #Keycloak is an open-source identity and access management application that uses open protocols and is easily integrated with other providers. It is the open-source project base of Red Hat Single Sign-on\nDeploying Red Hat Single Sign-on #The easiest way to deploy SSO is from the operator hub.\nFollow the Keycloak identity provider article for adding a new security realm, client, role, user\nDeploying Bookinfo example application #Create a new namespace\noc new-project bookinfo Edit the default Service Mesh Member Roll YAML and add bookinfo to the member\u0026rsquo;s list\napiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - bookinfo From the CLI, deploy the Bookinfo application in the bookinfo project by applying the bookinfo.yaml file\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create the ingress gateway by applying the bookinfo-gateway.yaml file\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Set the value for the GATEWAY_URL parameter\noc get routes -n istio-system | grep bookinfo export GATEWAY_URL=$(oc -n istio-system get route bookinfo-gateway-pl2rw -o jsonpath=\u0026#39;{.spec.host}\u0026#39;) Adding default destination rules\nI have enabled global mutual TLS in the control plane, so I‚Äôll deploy the destination rule with all mtls\noc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all-mtls.yaml Verifying the Bookinfo installation\nRun the following command to confirm that the Bookinfo application is up and running\ncurl -o /dev/null -s -w \u0026#34;%{http_code}\\n\u0026#34; http://$GATEWAY_URL/productpage Authentication #Enabling User-End Authentication #Now it is time to enable end-user authentication.\nThe first thing you need to do is validate that it is possible to communicate between all services without authentication.\ncurl -k -o /dev/null -w \u0026#34;%{http_code}\u0026#34; http://$GATEWAY_URL/productpage 200 You can create the end-user authentication policy.\ncat \u0026lt;\u0026lt;EOF | oc apply -n bookinfo -f - apiVersion: authentication.istio.io/v1alpha1 kind: Policy metadata: name: \u0026#34;productpage-jwt\u0026#34; namespace: \u0026#34;bookinfo\u0026#34; spec: targets: - name: productpage peers: - mtls: {} origins: - jwt: issuer: \u0026#34;https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio\u0026#34; jwksUri: \u0026#34;https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio/protocol/openid-connect/certs\u0026#34; audiences: - customer triggerRules: - excludedPaths: - prefix: /healthz principalBinding: USE_ORIGIN EOF NOTE: If you see Origin authentication failed. after passing the correct access token and URL. Verify your istio-pilot pods for any x509: certificate signed by unknown authority; if that is the case, follow this workaround\nThen let‚Äôs rerun the curl.\ncurl -k http://$GATEWAY_URL/productpage And you will see something like\nOrigin authentication failed. Set the value for the TOKEN parameter from keyclock\nexport TOKEN=$(curl -sk --data \u0026#34;username=demo\u0026amp;password=demo\u0026amp;grant_type=password\u0026amp;client_id=istio\u0026#34; https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio/protocol/openid-connect/token | jq \u0026#34;.access_token\u0026#34;) Then let‚Äôs rerun the curl, this time with the token.\ncurl -k -o /dev/null -w \u0026#34;%{http_code}\u0026#34; -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; http://$GATEWAY_URL/productpage 200 Authorization #Create a deny-all policy in the namespace. The policy doesn‚Äôt have a selector field, which applies to every workload in the namespace. The spec: field in the policy has the empty value {}, this means that no traffic is permitted, effectively denying all requests\n$ cat \u0026lt;\u0026lt;EOF | oc apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all spec: {} EOF Once the policy takes effect, verify that mesh rejected the curl connection to the workload.\n$ curl -k -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; http://$GATEWAY_URL/productpage RBAC: access denied To give read access to the product page workload, create the policy that applies to the workload with label app: product page and allows users with roles customer to access it with all method.\n$ cat \u0026lt;\u0026lt;EOF | oc apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: \u0026#34;productpage-authz\u0026#34; namespace: \u0026#34;bookinfo\u0026#34; spec: selector: matchLabels: app: productpage rules: - to: - operation: methods: [\u0026#34;*\u0026#34;] when: - key: request.auth.claims[roles] values: [\u0026#34;customer\u0026#34;] EOF Wait for the newly defined policy to take effect.\nAfter the policy takes effect, verify the connection to the httpbin workload succeeds.\ncurl -k -o /dev/null -w \u0026#34;%{http_code}\u0026#34; -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; http://$GATEWAY_URL/productpage 200 However, you can see the following errors on the page.\nError fetching product details Error fetching product reviews on the page Run the following command to create the details-viewer policy to allow the productpage workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-productpage service account, to access the details workload via GET methods.\noc apply -f - \u0026lt;\u0026lt;EOF apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;details-viewer\u0026#34; namespace: bookinfo spec: selector: matchLabels: app: details rules: - from: - source: principals: [\u0026#34;cluster.local/ns/bookinfo/sa/bookinfo-productpage\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] EOF Run the following command to create a policy reviews-viewer to allow the productpage workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-productpage service account, to access the reviews workload through GET methods\noc apply -f - \u0026lt;\u0026lt;EOF apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;reviews-viewer\u0026#34; namespace: bookinfo spec: selector: matchLabels: app: reviews rules: - from: - source: principals: [\u0026#34;cluster.local/ns/bookinfo/sa/bookinfo-productpage\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] EOF Point your browser at the Bookinfo productpage (http://$GATEWAY_URL/productpage). Now, you should see the ‚ÄúBookinfo Sample‚Äù page with ‚ÄúBook Details‚Äù on the lower left part and ‚ÄúBook Reviews‚Äù on the lower right part.\nHowever, in the ‚ÄúBook Reviews‚Äù section, you\u0026rsquo;ll see an error Rating service is currently unavailable. This error is because the reviews workload doesn‚Äôt permit access to the ratings workload. To fix this issue, you need to grant the reviews workload access to the ratings workload. Next, configure a policy to grant access to the reviews workload.\nRun the following command to create the ratings-viewer policy to allow the reviews workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-reviews service account, to access the ratings workload through GET methods\noc apply -f - \u0026lt;\u0026lt;EOF apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;AuthorizationPolicy\u0026#34; metadata: name: \u0026#34;ratings-viewer\u0026#34; namespace: bookinfo spec: selector: matchLabels: app: ratings rules: - from: - source: principals: [\u0026#34;cluster.local/ns/bookinfo/sa/bookinfo-reviews\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;] EOF Point your browser at the Bookinfo productpage (http://$GATEWAY_URL/productpage). You should see the ‚Äúblack‚Äù and ‚Äúred‚Äù ratings in the ‚ÄúBook Reviews‚Äù section.\n","date":"12 November 2020","permalink":"/blog/servicemesh-jwt-auth-authz-keycloack/","section":"Blogs","summary":"In this article, I will share the setup for enabling Authentication and Authorization in OpenShift Service Mesh with Keycloak.","title":"End User Auth and Authz with OpenShift Service Mesh and Keycloak"},{"content":"","date":null,"permalink":"/tags/istio/","section":"Tags","summary":"","title":"Istio"},{"content":"","date":null,"permalink":"/tags/jwt/","section":"Tags","summary":"","title":"JWT"},{"content":"","date":null,"permalink":"/tags/keycloak/","section":"Tags","summary":"","title":"Keycloak"},{"content":"","date":null,"permalink":"/tags/service-mesh/","section":"Tags","summary":"","title":"Service Mesh"},{"content":"","date":null,"permalink":"/tags/loki/","section":"Tags","summary":"","title":"Loki"},{"content":"","date":null,"permalink":"/tags/rsyslog/","section":"Tags","summary":"","title":"Rsyslog"},{"content":"Enable Syslog, do this on each host, and replace target IP (and maybe port) with your Syslog externalIP that is in helm values for promtail\npromtail: serviceMonitor: enabled: true extraScrapeConfigs: - job_name: syslog syslog: listen_address: 0.0.0.0:1514 label_structured_data: yes labels: job: \u0026#34;syslog\u0026#34; relabel_configs: - source_labels: [\u0026#34;__syslog_connection_ip_address\u0026#34;] target_label: \u0026#34;ip_address\u0026#34; - source_labels: [\u0026#34;__syslog_message_severity\u0026#34;] target_label: \u0026#34;severity\u0026#34; - source_labels: [\u0026#34;__syslog_message_facility\u0026#34;] target_label: \u0026#34;facility\u0026#34; - source_labels: [\u0026#34;__syslog_message_hostname\u0026#34;] target_label: \u0026#34;host\u0026#34; syslogService: enabled: true type: LoadBalancer port: 1514 externalIPs: - 192.168.42.155 Create file /etc/rsyslog.d/50-promtail.conf with the following content:\nmodule(load=\u0026#34;omprog\u0026#34;) module(load=\u0026#34;mmutf8fix\u0026#34;) action(type=\u0026#34;mmutf8fix\u0026#34; replacementChar=\u0026#34;?\u0026#34;) action(type=\u0026#34;omfwd\u0026#34; protocol=\u0026#34;tcp\u0026#34; target=\u0026#34;192.168.42.155\u0026#34; port=\u0026#34;1514\u0026#34; Template=\u0026#34;RSYSLOG_SyslogProtocol23Format\u0026#34; TCP_Framing=\u0026#34;octet-counted\u0026#34; KeepAlive=\u0026#34;on\u0026#34;) Restart rsyslog and view status\nsudo systemctl restart rsyslog sudo systemctl status rsyslog In Grafana, on the explore tab, you should now be able to view your host\u0026rsquo;s logs, e.g., this query {host=\u0026quot;k3s-master\u0026quot;}.\n","date":"4 November 2020","permalink":"/blog/loki-rsyslog-k3s/","section":"Blogs","summary":"Enable Syslog, do this on each host, and replace target IP (and maybe port) with your Syslog externalIP that is in helm values for promtail","title":"Syslog with Loki Promtail"},{"content":"The key benefit of Helm is that it helps reduce the configuration a user needs to provide to deploy applications to Kubernetes. With Helm, we can have a single chart that can deploy all the microservices.\nUnique ServiceAccount #Recently we wanted to create a unique service account for each microservice, so all microservices don\u0026rsquo;t share the same service account using the helm template. In our example, we will append the release name with the service account name.\nUsing join function #{{ (list .Values.serviceAccount.name (include \u0026#34;openjdk.fullname\u0026#34; .) | join \u0026#34;-\u0026#34;) }} Adding fail condition #{{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include \u0026#34;openjdk.fullname\u0026#34; .) | join \u0026#34;-\u0026#34;) }} {{- else -}} {{- fail \u0026#34;Please enter a name for service account to create.\u0026#34; }} {{- end -}} Full example\n{{- define \u0026#34;openjdk.serviceAccountName\u0026#34; -}} {{- if .Values.serviceAccount.create -}} {{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include \u0026#34;openjdk.fullname\u0026#34; .) | join \u0026#34;-\u0026#34;) }} {{- else -}} {{- fail \u0026#34;Please enter a name for service account to create.\u0026#34; }} {{- end -}} {{- else -}} {{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include \u0026#34;openjdk.fullname\u0026#34; .) | join \u0026#34;-\u0026#34;) }} {{- else -}} \u0026#34;default\u0026#34; {{- end -}} {{- end -}} {{- end -}} Another way to manipulate strings in helpers.tpl is using printf\n{{- define \u0026#34;ocp-openjdk.hostname\u0026#34; -}} {{- printf \u0026#34;%s-%s%s\u0026#34; .Release.Name .Release.Namespace (.Values.subdomain | default \u0026#34;.apps.amp01.nonprod\u0026#34; ) -}} {{- end -}} Resources # Sprig functions ","date":"8 October 2020","permalink":"/blog/helm-join-function/","section":"Blogs","summary":"The key benefit of Helm is that it helps reduce the configuration a user needs to provide to deploy applications to Kubernetes.","title":"Helm join strings in a named template"},{"content":"","date":null,"permalink":"/tags/ad-blocker/","section":"Tags","summary":"","title":"Ad-Blocker"},{"content":"","date":null,"permalink":"/tags/adguard/","section":"Tags","summary":"","title":"AdGuard"},{"content":"AdGuard #AdGuard Home is a network-wide software for blocking ads \u0026amp; tracking\nAdguard is similar to Pi-Hole with more features. Comparison of Adguard to Pi-Hole\nThe below configuration has worked for me. I am a big fan of helm charts, and I\u0026rsquo;ll be using AdGuard chart\nConfigure chart #Pull chart locally\nhelm repo add billimek https://billimek.com/billimek-charts/ helm fetch billimek/adguard-home Update deployment to use hostNetwork\n#templates/deployment.yaml ... spec: hostNetwork: true securityContext: ... Enable configAsCode, update bind_host to Kubernetes host IP in values.yaml\n# values.yaml configAsCode: enabled: true config: bind_host: 192.168.0.101 bind_port: 3000 dns: bind_host: 192.168.0.101 Update securityContext to run as a privileged pod, drop all capabilities and add NET_BIND_SERVICE\n# values.yaml securityContext: privileged: true capabilities: drop: - ALL add: - NET_BIND_SERVICE Add nodeSelector to assign a pod to that node.\n# values.yaml nodeSelector: kubernetes.io/hostname: node3 Deploy helm chart\nhelm install adguard-home Wait for AdGuard pods.\nkubectl get pods NAME READY STATUS RESTARTS AGE adguard-adguard-home-67975f7768-v6bg9 1/1 Running 0 43h Configure your devices to use your AdGuard Home #Once we\u0026rsquo;ve established that AdGuard Home has deployed, you can use it on other computers in your network by changing their system DNS settings to use the Kubernetes node\u0026rsquo;s IP address (which is 192.168.0.101 in our case).\n","date":"19 August 2020","permalink":"/blog/adguardhome-kubernetes/","section":"Blogs","summary":"AdGuard #AdGuard Home is a network-wide software for blocking ads \u0026amp; tracking","title":"AdGuard on Kubernetes"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"Kubernetes"},{"content":"","date":null,"permalink":"/tags/configuration-as-code-plugin/","section":"Tags","summary":"","title":"Configuration-as-Code-Plugin"},{"content":"In one of my previous posts, I discussed configuring multibranch pipeline seed jobs using Jenkins configuration as code plugin\nBelow is an example of configuring a declarative pipeline job from a bitbucket repo, running at midnight every day\n- script: \u0026gt; pipelineJob(\u0026#39;sample-job\u0026#39;) { definition { cpsScm { scriptPath \u0026#39;job1/Jenkinsfile\u0026#39; ## If the Jenkins job is in a nested folder scm { git { remote { url \u0026#39;https://github.com/Vikaspogu/sample-repo\u0026#39; credentials \u0026#39;sample-creds\u0026#39; } branch \u0026#39;*/master\u0026#39; extensions {} } } triggers { cron(\u0026#39;@midnight\u0026#39;) } } } } ","date":"18 August 2020","permalink":"/blog/jenkins-seed-job/","section":"Blogs","summary":"In one of my previous posts, I discussed configuring multibranch pipeline seed jobs using Jenkins configuration as code plugin","title":"Configure Jenkins pipeline job"},{"content":"","date":null,"permalink":"/tags/gatekeeper/","section":"Tags","summary":"","title":"Gatekeeper"},{"content":"","date":null,"permalink":"/tags/jenkins/","section":"Tags","summary":"","title":"Jenkins"},{"content":"","date":null,"permalink":"/tags/opa/","section":"Tags","summary":"","title":"OPA"},{"content":"Every organization has policies. Some are essential to meet governance and legal requirements. Others help ensure adherence to best practices and institutional conventions. Attempting to ensure compliance manually would be error-prone and frustrating.\nOPA allows users to a specific policy as code using OPA\u0026rsquo;s policy language Rego\nIn this post, I\u0026rsquo;ll share my experience deploying OPA Gatekeeper on OpenShift and creating a few policies for demonstrations. This post is not an introduction to OPA refer to for an intro\nGatekeeper introduces native Kubernetes CRDs for instantiating policies.\nInstallation #For installation, make sure you have cluster-admin permissions.\nLet\u0026rsquo;s start by adding the admission.gatekeeper.sh/ignore label to non-user namespaces so that all the resources in the labeled project are exempted from the admission webhook.\noc login --token=l4xjpLh0e722B2_i7iWAbPsUNOb6vPDaAXnqhH563oU --server=https://api.cluster-1d4d.sandbox702.opentlc.com:6443 for namespace in $(oc get namespaces -o jsonpath=\u0026#39;{.items[*].metadata.name}\u0026#39; | xargs); do if [[ \u0026#34;${namespace}\u0026#34; =~ OpenShift.* ]] || [[ \u0026#34;${namespace}\u0026#34; =~ kube.* ]] || [[ \u0026#34;${namespace}\u0026#34; =~ default ]]; then oc patch namespace/${namespace} -p=\u0026#39;{\u0026#34;metadata\u0026#34;:{\u0026#34;labels\u0026#34;:{\u0026#34;admission.gatekeeper.sh/ignore\u0026#34;:\u0026#34;true\u0026#34;}}}\u0026#39; else # Probably a user project, so leave it alone echo \u0026#34;Skipping: ${namespace}\u0026#34; fi done Deploying a Release using Prebuilt Image #Deploy Gatekeeper with a prebuilt image\noc apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml Remove securityContext and annotations from the deployments.\noc patch Deployment/gatekeeper-audit --type json -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/metadata/annotations\u0026#34;}]\u0026#39; -n gatekeeper-system oc patch Deployment/gatekeeper-controller-manager --type json -p=\u0026#39;[{\u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/metadata/annotations\u0026#34;}]\u0026#39; -n gatekeeper-system oc patch Deployment/gatekeeper-audit --type json --patch \u0026#39;[{ \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/securityContext\u0026#34; }]\u0026#39; -n gatekeeper-system oc patch Deployment/gatekeeper-controller-manager --type json --patch \u0026#39;[{ \u0026#34;op\u0026#34;: \u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/spec/template/spec/containers/0/securityContext\u0026#34; }]\u0026#39; -n gatekeeper-system Wait for Gatekeeper to be ready.\noc get pods -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c84869dbf-r5p87 1/1 Running 0 2m34s gatekeeper-controller-manager-ff58b6688-9tbxx 1/1 Running 0 3m23s gatekeeper-controller-manager-ff58b6688-njfnd 1/1 Running 0 2m54s gatekeeper-controller-manager-ff58b6688-vlrsl 1/1 Running 0 3m11s Gatekeeper uses the OPA Constraint Framework to describe and enforce policy\nDefining constraints #Users can define constraints by creating a CRD (CustomResourceDefinition) with the template of the constraint they want. For example, let‚Äôs look at a template that only enforces users must create secure routes on the cluster.\napiVersion: templates. Gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8sallowedroutes spec: crd: spec: names: kind: K8sAllowedRoutes targets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedroutes violation[{\u0026#34;msg\u0026#34;: msg}] { not input.review.object.spec.tls msg := sprintf(\u0026#34;\u0026#39;%v\u0026#39; route must be a secured route. non secured routes are not permitted\u0026#34;, [input.review.object.metadata.name]) } It will get the route and check if the tls object is specified. If this is not true, the violation block continues, and the violation is triggered with the corresponding message.\nEnforcing constraints #Define a template using earlier constraints to enforce.\napiVersion: constraints. Gatekeeper.sh/v1beta1 kind: K8sAllowedRoutes metadata: name: secure-route spec: match: kinds: - apiGroups: [\u0026#34;route.OpenShift.io\u0026#34;] kinds: [\u0026#34;Route\u0026#34;] The above policy uses the CRD ‚ÄúK8sAllowedRoutes‚Äù, which we had already defined. Enforcement takes place by matching the API group.\nSome constraints are impossible to write without access to more states than the object under test. For example, it is impossible to know if a route\u0026rsquo;s hostname is unique among all routes unless a rule has access to all other routes. To make such laws possible, we enable syncing of data into OPA.\napiVersion: config. Gatekeeper.sh/v1alpha1 kind: Config metadata: name: config namespace: \u0026#34;gatekeeper-system\u0026#34; spec: sync: syncOnly: - group: \u0026#34;\u0026#34; version: \u0026#34;v1\u0026#34; kind: \u0026#34;Namespace\u0026#34; - group: \u0026#34;route.OpenShift.io\u0026#34; version: \u0026#34;v1\u0026#34; kind: \u0026#34;Route\u0026#34; Let\u0026rsquo;s create another policy that prevents conflicting routes from being created.\napiVersion: templates. Gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8suniqueroutehost spec: crd: spec: names: kind: K8sUniqueRouteHost targets: - target: admission.k8s.gatekeeper.sh rego: | package k8suniqueroutehost identical(obj, review) { obj.metadata.namespace == review.object.metadata.namespace obj.metadata.name == review.object.metadata.name } violation[{\u0026#34;msg\u0026#34;: msg}] { input.review.kind.kind == \u0026#34;Route\u0026#34; re_match(\u0026#34;^(route.OpenShift.io)$\u0026#34;, input.review.kind.group) host := input.review.object.spec.host other := data.inventory.namespace[ns][otherapiversion][\u0026#34;Route\u0026#34;][name] re_match(\u0026#34;^(route.OpenShift.io)/.+$\u0026#34;, otherapiversion) other.spec.host == host not identical(other, input.review) msg := sprintf(\u0026#34;Route host conflicts with an existing route \u0026lt;%v\u0026gt;\u0026#34;, [host]) } apiVersion: constraints. Gatekeeper.sh/v1beta1 kind: K8sUniqueRouteHost metadata: name: unique-route-host spec: match: kinds: - apiGroups: [\u0026#34;route.OpenShift.io\u0026#34;] kinds: [\u0026#34;Route\u0026#34;] Resources # Open Policy Agent OPA Gatekeeper Test framework ","date":"18 August 2020","permalink":"/blog/opa-gatekeeper-openshift/","section":"Blogs","summary":"Every organization has policies.","title":"OPA Gatekeeper on OpenShift"},{"content":"","date":null,"permalink":"/tags/pipeline-job/","section":"Tags","summary":"","title":"Pipeline-Job"},{"content":"","date":null,"permalink":"/tags/date/","section":"Tags","summary":"","title":"Date"},{"content":"","date":null,"permalink":"/tags/groovy/","section":"Tags","summary":"","title":"Groovy"},{"content":"The below snippet shows how to calculate days between two dates in the Jenkins pipeline. @NonCPS annotation is functional when you have methods that use objects that aren\u0026rsquo;t serializable.\nimport java.text.SimpleDateFormat stages { stage(\u0026#34;Calculate days\u0026#34;) { steps { script { def daysRemaining = getRemainingDays(\u0026#34;2020-06-25\u0026#34;) } } } } } @NonCPS def getRemainingDays(previousDate){ def currentDate = new Date() String currentTimeFormat= currentDate.format(\u0026#34;yyyy-MM-dd\u0026#34;) def oldDate = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;).parse(previousDate) return currentTimeFormat-oldDate } Calculate if the date is greater than 14 days\nimport java.time.LocalDate import java.time.format.DateTimeFormatter stages { stage(\u0026#34;Check if date is older than 14 days\u0026#34;) { steps { script { if (checkIfOtherThan(\u0026#34;2020-06-25\u0026#34;)){ println \u0026#34;Date is greater than 14 days\u0026#34; } } } } } } @NonCPS def checkIfOlderThan(previousDate){ def dateFormat = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd\u0026#34;) def currentDate = LocalDate.now().format(dateFormat); def projectpreviousDate = LocalDate.parse(previousDate, dateFormat) if (LocalDate.parse(currentDate, dateFormat).minusDays(14) \u0026gt; projectpreviousDate) { return true; } return false; } ","date":"7 July 2020","permalink":"/blog/jenkins-date-difference/","section":"Blogs","summary":"The below snippet shows how to calculate days between two dates in the Jenkins pipeline.","title":"Jenkins pipeline date helper functions"},{"content":"","date":null,"permalink":"/tags/jcasc/","section":"Tags","summary":"","title":"Jcasc"},{"content":"Deploying Jenkins on Kubernetes provides significant benefits over a standard VM-based deployment. For example, we are gaining the ability to have project-specific Jenkins slaves (agents) on demand instead of having a pool of VMs idle waiting for a job.\nIn vanilla Kubernetes, we can deploy Jenkins using Helm, and In OpenShift, we can deploy Jenkins via the developer\u0026rsquo;s catalog.\nAs everyone has experienced, setting up Jenkins is a complex process. Both Jenkins and its plugins require some tuning and Configuration, with dozens of parameters to set within the web UI manage section.\nJenkins Config as code #Configuration as code gives you an opinionated way to configure Jenkins based on yaml files\nThis post will cover Jenkins configuration code on OpenShift.\nMount Jenkins config as a configmap and load configuration Updated configuration file to automate the creation of credentials script approval signatures shared libraries multibranch pipeline seed jobs Install Plugin #First, let us install configuration-as-code plugins in Jenkins. In OpenShift, you can easily install plugins by adding the INSTALL_PLUGINS environment variable to the deployment config\n.... env: - name: INSTALL_PLUGINS value: \u0026#34;configuration-as-code:1.35,configuration-as-code-support:1.18,configuration-as-code-groovy:1.1 Create ConfigMap #Create a configmap from jenkins-config yaml and mount it as a volume at /var/jenkins_config location. Configuration can be now loaded from /var/jenkins_config/jenkins-config.yaml path\noc create configmap jenkins-config --from-file jenkins-config.yaml .... volumeMounts: - mountPath: /var/jenkins_config name: jenkins-config dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: jenkins-config configMap: name: jenkins-config Mount Secret #Since we can have Jenkins configuration in the git repo, we don\u0026rsquo;t want to hardcode the secrets as it is a security risk. The easiest way to automate credentials in Jenkins configuration is to create an OpenShift secret, and add that secret as an environment variable to the deployment config\noc create secret generic github-ssh --from-file=ssh-privatekey=github-ssh/ oc create secret generic jenkins-credentials --from-literal=OCP_SA=\u0026#34;qwerty26sds99ie9kcsd\u0026#34; --from-literal=GITHUB_PASSWORD=\u0026#34;dummypassword\u0026#34; Mount secrets into the container\n.... env: .... - name: OCP_SA valueFrom: secretKeyRef: name: jenkins-credentials key: OCP_SA - name: GITHUB_SSH valueFrom: secretKeyRef: name: github-ssh key: ssh-privatekey - name: GITHUB_PASSWORD valueFrom: secretKeyRef: name: jenkins-credentials key: GITHUB_PASSWORD .... credentials: system: domainCredentials: - credentials: - fileSystemServiceAccountCredential: id: \u0026#34;1a12dfa4-7fc5-47a7-aa17-cc56572a41c7\u0026#34; scope: GLOBAL - basicSSHUserPrivateKey: description: \u0026#34;github ssh credentials\u0026#34; id: \u0026#34;github-ssh\u0026#34; privateKeySource: directEntry: privateKey: ${GITHUB_SSH} scope: GLOBAL username: \u0026#34;github-user\u0026#34; - OpenShiftToken: id: \u0026#34;jenkins-ocp-token\u0026#34; scope: GLOBAL secret: ${OCP_SA} - usernamePassword: description: \u0026#34;github user credentials\u0026#34; id: \u0026#34;github-user\u0026#34; password: ${GITHUB_PASSWORD} scope: GLOBAL username: \u0026#34;github-user\u0026#34; Example Configs #Security script approval #security: scriptApproval: approvedSignatures: - \u0026#34;method java.text.DateFormat parse java.lang.String\u0026#34; Global shared library #unclassified: globalLibraries: libraries: - defaultVersion: \u0026#34;master\u0026#34; name: \u0026#34;shared-pipeline\u0026#34; retriever: modernSCM: scm: git: credentialsId: \u0026#34;github-ssh\u0026#34; id: \u0026#34;shared-pipeline\u0026#34; remote: \u0026#34;git@github.com:vikaspogu/jenkins-shared.git\u0026#34; Multibranch job #Multibranch seed job with periodic polling, traits for branch discovery\njobs: - script: \u0026gt; multibranchPipelineJob(\u0026#34;sample-repo\u0026#34;) { branchSources { branchSource { source { github { //This is a unique identifier; if not set, the pipeline will be //indexed, and jobs will be kicked off every time new config is applied id(\u0026#34;5bb970c2-766b-4588-8cf8-e077bfec23a0\u0026#34;) credentialsId(\u0026#34;github-user\u0026#34;) repoOwner(\u0026#34;vikaspogu\u0026#34;) repository(\u0026#34;sample-repo\u0026#34;) traits { cloneOptionTrait { extension { shallow (false) noTags (false) reference (null) depth(1) honorRefspec (false) timeout (10) } } } } } } } configure { def traits = it / sources / data / \u0026#39;jenkins.branch.BranchSource\u0026#39; / source / traits traits \u0026lt;\u0026lt; \u0026#39;com.cloudbees.jenkins.plugins.bitbucket.BranchDiscoveryTrait\u0026#39; { strategyId(3) // detect all branches -refer the plugin source code for various options } } configure { def traits = it / sources / data / \u0026#39;jenkins.branch.BranchSource\u0026#39; / source / traits traits \u0026lt;\u0026lt; \u0026#39;com.cloudbees.jenkins.plugins.bitbucket.OriginPullRequestDiscoveryTrait\u0026#39; { strategyId(2) } } configure { def traits = it / sources / data / \u0026#39;jenkins.branch.BranchSource\u0026#39; / source / traits traits \u0026lt;\u0026lt; \u0026#39;com.cloudbees.jenkins.plugins.bitbucket.TagDiscoveryTrait\u0026#39; {} } orphanedItemStrategy { discardOldItems { numToKeep(15) } } triggers { periodic(2) } } ","date":"6 July 2020","permalink":"/blog/jenkins-config-as-code-openshift/","section":"Blogs","summary":"Deploying Jenkins on Kubernetes provides significant benefits over a standard VM-based deployment.","title":"OpenShift Jenkins configuration via JCasC plugin"},{"content":"","date":null,"permalink":"/tags/garage-door/","section":"Tags","summary":"","title":"Garage-Door"},{"content":"","date":null,"permalink":"/tags/nodejs/","section":"Tags","summary":"","title":"NodeJS"},{"content":"Many articles are out there which demonstrate how to use a raspberry pi as a DIY garage door opener project. Few are outdated and not deployed using container images. I found a few reasonable solutions on google, but I couldn\u0026rsquo;t run them on the Kubernetes cluster due to older packages or insufficient information. I decided to build my solution from different sources of information I found\nWhat we\u0026rsquo;ll cover in this post\nsetup nodejs project to simulate a button create a container from nodejs application deploy the container on the Kubernetes cluster Kudos! to the author for this excellent article for showing us how to connect the relay and magnetic switch to raspberry pi for our purpose.\nOnce finished wiring up all components. Let\u0026rsquo;s start setting up our Nodejs application.\nApplication setup #Create an npm project\nmkdir garage-pi \u0026amp;\u0026amp; cd garage-pi npm init -y We\u0026rsquo;ll be using node-rpio package, which provides access to the Raspberry Pi GPIO interface\nInstall node-rpio and express packages\nnpm i rpio express -S Create an express app that starts on port 8080 in server.js file\n\u0026#34;use strict\u0026#34;; const express = require(\u0026#34;express\u0026#34;); const rpio = require(\u0026#34;rpio\u0026#34;); const app = express(); const PORT = 8080; app.use(\u0026#34;/assets\u0026#34;, express.static(\u0026#34;assets\u0026#34;)); app.listen(PORT); console.log(\u0026#34;Running on http://localhost:\u0026#34; + PORT); The below code lets you simulate a button press; In our case, PIN is 19. First, we want to output to low and set the pin to high after 1000ms. Please refer to rpio repo for more explanation\nconst openPin = process.env.OPEN_PIN || 19; const relayPin = process.env.RELAY_PIN || 11; app.get(\u0026#34;/relay\u0026#34;, function (req, res) { // Simulate a button press rpio.write(relayPin, rpio.LOW); setTimeout(function () { rpio.write(relayPin, rpio.HIGH); res.send(\u0026#34;done\u0026#34;); }, 1000); }); To get the state of pin.\nfunction getState() { return { open: !rpio.read(openPin), }; } app.get(\u0026#34;/status\u0026#34;, function (req, res) { res.send(JSON.stringify(getState())); }); Complete server.js file\n\u0026#34;use strict\u0026#34;; const express = require(\u0026#34;express\u0026#34;); const rpio = require(\u0026#34;rpio\u0026#34;); const app = express(); const PORT = 8080; const openPin = process.env.OPEN_PIN || 19; const relayPin = process.env.RELAY_PIN || 11; app.use(\u0026#34;/assets\u0026#34;, express.static(\u0026#34;assets\u0026#34;)); function getState() { return { open: !rpio.read(openPin), }; } app.get(\u0026#34;/status\u0026#34;, function (req, res) { res.send(JSON.stringify(getState())); }); app.get(\u0026#34;/relay\u0026#34;, function (req, res) { // Simulate a button press rpio.write(relayPin, rpio.LOW); setTimeout(function () { rpio.write(relayPin, rpio.HIGH); res.send(\u0026#34;done\u0026#34;); }, 1000); }); app.listen(PORT); console.log(\u0026#34;Running on http://localhost:\u0026#34; + PORT); Container image # Create a dockerfile with multi-stage builds using nodejs arm image Install the necessary python package for rpio Build docker image Publish the docker image to your repo in docker hub Create new Kubernetes deployment and service # Fetch node_modules for backend; nothing here except # the node_modules dir ends up in the final image FROM arm32v7/node:12.18-alpine as builder RUN mkdir /app WORKDIR /app ENV PATH /app/node_modules/.bin:$PATH COPY package.json /app/package.json RUN apk add --no-cache make gcc g++ python \u0026amp;\u0026amp; \\ npm install --production --silent \u0026amp;\u0026amp; \\ apk del make gcc g++ python RUN npm install # Add the files to the arm image FROM arm32v7/node:12.18-alpine RUN mkdir /app WORKDIR /app ENV PATH /app/node_modules/.bin:$PATH # Same as earlier, be specific or copy everything ADD package.json /app/package.json ADD package-lock.json /app/package-lock.json ADD . /app COPY --from=builder /app/node_modules /app/node_modules ENV PORT=8080 EXPOSE 8080 CMD [ \u0026#34;npm\u0026#34;, \u0026#34;start\u0026#34; ] Docker buildx feature lets you build arm-based images on mac or windows system\ndocker buildx build --platform linux/arm64 -t \u0026lt;docker-username\u0026gt;/garage-pi . docker push \u0026lt;docker-username\u0026gt;/garage-pi Create a new deployment named garage-pi that runs the earlier published image.\nkind: Deployment apiVersion: apps/v1 metadata: name: garage-pi labels: app.kubernetes.io/name: garage-pi spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: garage-pi app.kubernetes.io/name: garage-pi template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: garage-pi app.kubernetes.io/name: garage-pi spec: volumes: - name: dev-snd hostPath: path: /dev/mem type: \u0026#39;\u0026#39; containers: - name: garage-pi image: \u0026#39;\u0026lt;docker-username\u0026gt;/garage-pi:latest\u0026#39; ##update username here ports: - name: http containerPort: 8080 protocol: TCP resources: {} volumeMounts: - name: dev-snd mountPath: /dev/mem livenessProbe: httpGet: path: / port: http scheme: HTTP readinessProbe: httpGet: path: / port: http scheme: HTTP imagePullPolicy: Always securityContext: privileged: true restartPolicy: Always Create a service for a garage-pi deployment, which serves on port 8080 and connects to the containers on port 8080.\nkubectl expose deployment nginx --port=8080 --target-port=8080 That\u0026rsquo;s it; you should be able to hit the endpoint and simulate button click!\nFrontend #For frontend of application, we\u0026rsquo;ll use pugjs templating engine\nInstall pug package\nnpm i pug -S add the views folder in the root directory and create a new index.pug file in the views folder wire up templating engine with application render index page on root / endpoint integrate button with /relay endpoint, which will open/closed garage door doctype html head meta(charset=\u0026#39;utf-8\u0026#39;) meta(name=\u0026#39;viewport\u0026#39;, content=\u0026#39;width=device-width, initial-scale=1, shrink-to-fit=no\u0026#39;) meta(name=\u0026#39;description\u0026#39;, content=\u0026#39;\u0026#39;) meta(name=\u0026#39;author\u0026#39;, content=\u0026#39;\u0026#39;) title Garage Opener .text-center form.form-signin(method=\u0026#39;POST\u0026#39;, action=\u0026#39;/relay\u0026#39;) h1.h1.mb-2.font-weight-normal(style=\u0026#39;color: #FFFFFF\u0026#39;) Garage Door .text-center .form-signin .text-center #open h1.h2.mb-3.font-weight-normal(style=\u0026#39;color: #CF6679\u0026#39;) The Door is Open \u0026amp;#xF62B; button#mainButton.btn.btn-lg.btn-primary.btn-block(type=\u0026#39;submit\u0026#39;) Close app.set(\u0026#34;views\u0026#34;, path.join(__dirname, \u0026#34;views\u0026#34;)); app.set(\u0026#34;view engine\u0026#34;, \u0026#34;pug\u0026#34;); app.get(\u0026#34;/\u0026#34;, function (req, res) { res.render(\u0026#34;index\u0026#34;, getState()); }); Twilio Integration (Optional) #Install dotenv, twilio and node-schedule packages\nnpm i dotenv node-schedule twilio -S Create a .env file at the root of the project and add your Twilio auth key, account sid, and phone number\nTWILIO_ACCOUNT_SID= TWILIO_AUTH_TOKEN= TWILIO_PHONE_NUMBER= Create a twilio.js file, add the below code\nrequire(\u0026#34;dotenv\u0026#34;).config(); const accountSid = process.env.TWILIO_ACCOUNT_SID; const authToken = process.env.TWILIO_AUTH_TOKEN; const sendSms = (phone, message) =\u0026gt; { const client = require(\u0026#34;twilio\u0026#34;)(accountSid, authToken); client.messages .create({ body: message, from: process.env.TWILIO_PHONE_NUMBER, to: phone, }) .then((message) =\u0026gt; console.log(message.sid)); }; module.exports = sendSms; Schedule a job for every 15mins to check the state of the garage door. If the door is open, we\u0026rsquo;ll send a message\nconst sendSms = require(\u0026#34;./twilio\u0026#34;); ... ... schedule.scheduleJob(\u0026#34;*/15 * * * *\u0026#34;, function () { var status = JSON.parse(JSON.stringify(getState())); if (status.open) { sendSms(\u0026#34;\u0026lt;YOUR-NUMBER\u0026gt;\u0026#34;, \u0026#34;Garage door is open üî•\u0026#34;); } }); ","date":"5 July 2020","permalink":"/blog/pi-garage-k3s/","section":"Blogs","summary":"Many articles are out there which demonstrate how to use a raspberry pi as a DIY garage door opener project.","title":"Raspberry Pi garage opener on k3s cluster"},{"content":"","date":null,"permalink":"/tags/raspberry-pi/","section":"Tags","summary":"","title":"Raspberry-Pi"},{"content":"Build your slack bot in a few steps. In this post, we\u0026rsquo;ll navigate the process of creating the bot.\nSlack setup #First, create a slack workspace\nGive your workspace a name Create a new bot at slack apps\nGive your new application a name Choose the workspace you created before installing the bot application Then go to the Features \u0026gt; OAuth \u0026amp; Permissions screen to scroll down to Bot Token Scopes to specify the OAuth scopes, and select app_mentions and chat_write to enable the bot to send messages.\nBefore jumping into the application setup, copy the signing secret and verification token from the basic information page. We\u0026rsquo;ll be using this later in our NodeJS application.\nApplication setup #Create an npm project, install @slack/bolt and dotenv packages\nmkdir test-bot \u0026amp;\u0026amp; cd test-bot npm init -y npm i dotenv @slack/bolt -S Add start command to scripts if necessary.\n... \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;node index.js\u0026#34; } Create a .env file and add SLACK_SIGNING_SECRET, SLACK_BOT_TOKEN\nNote: Don\u0026rsquo;t commit this file to any repo\nSLACK_BOT_TOKEN= #token goes here SLACK_SIGNING_SECRET= #signing secret goes here In your index.js file, require the Bolt package, and initialize an app with credentials.\nrequire(\u0026#34;dotenv\u0026#34;).config(); const { App } = require(\u0026#34;@slack/bolt\u0026#34;); const bot = new App({ signingSecret: process.env.SLACK_SIGNING_SECRET, token: process.env.SLACK_BOT_TOKEN, endpoints: \u0026#34;/slack/events\u0026#34;, }); (async () =\u0026gt; { // Start the app await bot.start(process.env.PORT || 3000); console.log(\u0026#34;‚ö°Ô∏è Bolt app is running!\u0026#34;); })(); Deploy the application to a live server like ngrok.\nEvent Setup #We\u0026rsquo;ll need to subscribe to events so that when a Slack event happens (like a user mentions an app), the app server will receive an event payload.\nGo to Event Subscriptions from the left-hand menu, and turn the toggle switch on to enable events\nEnter your Request URL\nSubscribe to app_mention event\nInstall app to workspace\nYou should see the bot in your workspace now!\nHandling Events #To listen to any Events API events from Slack, use the event() method. This method allows your app to take action on Slack events. In this scenario, it\u0026rsquo;s triggered when a user mentions the app.\nbot.event(\u0026#34;app_mention\u0026#34;, async ({ context, event }) =\u0026gt; { try { const command = event.text; let reply; if (command.includes(\u0026#34;Hi\u0026#34;)) { reply = `Hi \u0026lt;@${event.user}\u0026gt;, you mentioned me`; } else { reply = \u0026#34;How can I help you?\u0026#34;; } await bot.client.chat.postMessage({ token: context.botToken, channel: event.channel, text: `${reply}`, }); } catch (e) { console.log(`error responding ${e}`); } }); Okay, let\u0026rsquo;s try the app!\nAdd the app to a channel and mention the app. You should see a response from the bot!\nTroubleshooting #Reinstall the app if you don\u0026rsquo;t see any responses from the bot\n","date":"4 July 2020","permalink":"/blog/nodejs-slack-bot/","section":"Blogs","summary":"Build your slack bot in a few steps.","title":"Slack bot with Nodejs"},{"content":"","date":null,"permalink":"/tags/slack-bot/","section":"Tags","summary":"","title":"Slack-Bot"},{"content":"","date":null,"permalink":"/tags/k3s/","section":"Tags","summary":"","title":"K3s"},{"content":"Pi-hole is a fantastic tool that blocks DNS requests to ad servers. That means you can surf the web without looking at ads on every page.\nPi-Hole in Kubernetes #We are going to deploy a modified version of this pihole helm chart\nLet\u0026rsquo;s start by cloning the repo.\ngit clone https://github.com/ChrisPhillips-cminion/pihole-helm.git cd pihole-helm We now need to make a few updates to the chart.\nUpdate ServerIP with container host IP Update image to v5.1.1 tag Add WEB_PASSWORD, and TZ environment variables if needed Update values.yaml accordingly spec: replicas: 1 template: metadata: labels: app: {{ template \u0026#34;fullname\u0026#34; . }} spec: # hostNetwork: true hostAliases: - ip: 127.0.0.1 hostnames: - pi.hole nodeSelector: kubernetes.io/hostname: randomstore containers: - name: {{ .Chart.Name }} image: pihole/pihole:v5.1.1 imagePullPolicy: {{ .Values.image.pullPolicy }} stdin: true tty: true resources: limits: memory: 1Gi env: - name: \u0026#39;ServerIP\u0026#39; value: \u0026#39;192.168.1.132\u0026#39; - name: \u0026#39;DNS1\u0026#39; value: \u0026#39;8.8.8.8\u0026#39; - name: \u0026#39;DNS2\u0026#39; value: \u0026#39;8.8.4.4\u0026#39; - name: TZ value: \u0026#34;America/New_York\u0026#34; - name: WEBPASSWORD value: \u0026#34;somepassword\u0026#34; #values.yaml configData: |- server=/local/192.168.1.1 address=/.vikaspogu.com/192.168.1.132 ingress: host: pi-hole.vikaspogu.com Install chart # Create a new namespace (optional) Install chart in namespace kubectl create ns pi-hole helm install pi-hole. Wait for pods kubectl get pods NAME READY STATUS RESTARTS AGE pi-hole-pihole-5bb56b5bd-b2wl7 1/1 Running 0 61m Navigate to the ingress route in my case (pi-hole.vikaspogu.com) and log in with WEBPASSWORD used in the deployment DNS Server #Configure Verizon FiOS router to use Pi-Hole as the DNS server:\nOn the top navigation menu\nClick My Network On the left menu list\nClick Network Connections Click Broadband Connection (Ethernet/Coax)\u0026gt;Settings\nClick the drop-down for DNS Server and select \u0026ldquo;Use The Following DNS Server Addresses\u0026rdquo;\nType in the static IP Address of your pi (Or Pi-hole server)\nClick Apply\n","date":"7 March 2020","permalink":"/blog/pi-hole-kubernetes/","section":"Blogs","summary":"Pi-hole is a fantastic tool that blocks DNS requests to ad servers.","title":"Pi Hole on k3s cluster"},{"content":"","date":null,"permalink":"/tags/pi-hole/","section":"Tags","summary":"","title":"Pi-Hole"},{"content":"Jenkins SSL #Problem #I recently encountered an issue while authenticating to OpenShift Jenkins using the OpenShift OAuth plugin, where trusted certificates provided by CA aren\u0026rsquo;t included in the default JRE TrustStore.\nLogs from Jenkins pod\n2020-02-10 21:19:07.335+0000 [id=17] INFO o.o.j.p.o.OpenShiftOAuth2SecurityRealm#transportToUse: OpenShift OAuth got an SSL error when accessing the issuer\u0026#39;s token endpoint when using the SA certificate2020-02-10 21:19:07.348+0000 [id=17] INFO o.o.j.p.o.OpenShiftOAuth2SecurityRealm#transportToUse: OpenShift OAuth provider token endpoint failed unexpectedly using the JVMs default keystore sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141) at java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126) at java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297) at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434) Caused: sun.security.validator.ValidatorException: PKIX path building failed at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439) at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306) at java.base/sun.security.validator.Validator.validate(Validator.java:264) at java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313) at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:222) at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:129) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:629) Caused: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:320) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263) Solution #By default, Java Applications (as Jenkins) use the JVM TrustStore. Therefore, if a Java Application needs to use an additional TrustStore, it must be configured.\nFirst, create a secret with the JKS Keystore and password.\noc create secret generic jenkins-https-jks --from-literal=https-jks-password=changeit \\ --from-file=custom-keystore.jks Mount jenkins-https-jks keystore secret as a volume to /var/jenkins_keystore location.\n... terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/jenkins name: jenkins-data - mountPath: /var/jenkins_keystore name: jenkins-https-keystore serviceAccount: jenkins serviceAccountName: jenkins volumes: - name: jenkins-data persistentVolumeClaim: claimName: jenkins - name: jenkins-https-keystore secret: defaultMode: 420 items: - key: custom-keystore.jks path: custom-keystore.jks secretName: jenkins-https-jks ... Add the certificate to Jenkins as startup parameters; we can configure the Jenkins server to add the following JAVA properties to the JAVA_TOOL_OPTIONS environment variable.\n-Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks \\ -Djavax.net.ssl.trustStorePassword=changeit spec: containers: - env: - name: JENKINS_HTTPS_KEYSTORE_PASSWORD valueFrom: secretKeyRef: key: https-jks-password name: jenkins-https-jks - name: JAVA_TOOL_OPTIONS value: -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true -Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks -Djavax.net.ssl.trustStorePassword=$(JENKINS_HTTPS_KEYSTORE_PASSWORD) The final deployment config should look like this:\napiVersion: apps.OpenShift.io/v1 kind: DeploymentConfig metadata: annotations: template.alpha.OpenShift.io/wait-for-ready: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2020-02-10T17:49:20Z\u0026#34; generation: 10 labels: app: jenkins-persistent name: jenkins spec: replicas: 1 revisionHistoryLimit: 10 selector: name: jenkins strategy: activeDeadlineSeconds: 21600 recreateParams: timeoutSeconds: 600 resources: {} type: Recreate template: metadata: creationTimestamp: null labels: name: jenkins spec: containers: - env: - name: OpenShift_ENABLE_OAUTH value: \u0026#34;true\u0026#34; - name: OpenShift_ENABLE_REDIRECT_PROMPT value: \u0026#34;true\u0026#34; - name: DISABLE_ADMINISTRATIVE_MONITORS value: \u0026#34;false\u0026#34; - name: KUBERNETES_MASTER value: https://kubernetes.default:443 - name: KUBERNETES_TRUST_CERTIFICATES value: \u0026#34;true\u0026#34; - name: JENKINS_SERVICE_NAME value: jenkins - name: JNLP_SERVICE_NAME value: jenkins-jnlp - name: ENABLE_FATAL_ERROR_LOG_FILE value: \u0026#34;false\u0026#34; - name: JENKINS_UC_INSECURE value: \u0026#34;false\u0026#34; - name: JENKINS_HTTPS_KEYSTORE_PASSWORD valueFrom: secretKeyRef: key: https-jks-password name: jenkins-https-jks - name: JAVA_TOOL_OPTIONS value: -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true -Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks -Djavax.net.ssl.trustStorePassword=$(JENKINS_HTTPS_KEYSTORE_PASSWORD) image: image-registry.OpenShift-image-registry.svc:5000/OpenShift/jenkins@sha256:dd5f1c5d14a8a72aa4ca51224c26a661c2e4f19ea3e5f9b7d8343f4952de5f0d imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 2 httpGet: path: /login port: 8080 scheme: HTTP initialDelaySeconds: 420 periodSeconds: 360 successThreshold: 1 timeoutSeconds: 240 name: jenkins readinessProbe: failureThreshold: 3 httpGet: path: /login port: 8080 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 240 resources: limits: memory: 1Gi securityContext: capabilities: {} privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/jenkins name: jenkins-data - mountPath: /var/jenkins_keystore name: jenkins-https-keystore dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: jenkins serviceAccountName: jenkins terminationGracePeriodSeconds: 30 volumes: - name: jenkins-data persistentVolumeClaim: claimName: jenkins - name: jenkins-https-keystore secret: defaultMode: 420 items: - key: custom-keystore.jks path: custom-keystore.jks secretName: jenkins-https-jks test: false triggers: [] status: {} Start a new deployment.\noc rollout latest jenkins Once new pods are running, login will redirect to the Jenkins home page.\n","date":"10 February 2020","permalink":"/blog/openshift-jenkins-oauth-ssl/","section":"Blogs","summary":"Jenkins SSL #Problem #I recently encountered an issue while authenticating to OpenShift Jenkins using the OpenShift OAuth plugin, where trusted certificates provided by CA aren\u0026rsquo;t included in the default JRE TrustStore.","title":"Jenkins OpenShift OAuth SSL"},{"content":"","date":null,"permalink":"/tags/ssl/","section":"Tags","summary":"","title":"SSL"},{"content":"In my previous post, I went through the k3s cluster home setup. Now, I\u0026rsquo;ll show how to measure the temperature of those Raspberry Pi\u0026rsquo;s using Telegraf, Influxdb, Grafana, and Helm charts.\nWhy Telegraf? #Telegraf has a plugin called exec, which can execute the commands on the host machine at a specific interval and parses those metrics from their output in any one of the accepted input data formats.\nFirst, deploy the influxdb time series database chart.\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: influxdb namespace: kube-system spec: chart: stable/influxdb targetNamespace: monitoring Get Pi temperature #I found this one-liner /sys/class/thermal/thermal_zone0/temp, which returns the temperature of the Pi; divide the output by 1000 to get a result in ¬∞C and use awk to have a float value.\nawk \u0026#39;{print $1/1000}\u0026#39; /sys/class/thermal/thermal_zone0/temp Update Chart values #Update chart values, add [inputs.exec] to config, and deploy it\napiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: telegraf namespace: kube-system spec: chart: stable/telegraf targetNamespace: monitoring valuesContent: |- replicaCount: 2 image: repo: \u0026#34;telegraf\u0026#34; tag: \u0026#34;latest\u0026#34; pullPolicy: IfNotPresent env: - name: HOSTNAME valueFrom: fieldRef: fieldPath: spec.nodeName config: inputs: - exec: commands: [\u0026#34;awk \u0026#39;{print $1/1000}\u0026#39; /sys/class/thermal/thermal_zone0/temp\u0026#34;] name_override: \u0026#34;rpi_temp\u0026#34; data_format: \u0026#34;value\u0026#34; data_type: \u0026#34;float\u0026#34; Add Datasource #Once influxdb and telegraf pods are ready, add influxdb Datasource in grafana.\nGrafana #For Grafana visualization, import this dashboard.\n","date":"1 January 2020","permalink":"/blog/raspberry-pi-temp-telegraf/","section":"Blogs","summary":"In my previous post, I went through the k3s cluster home setup.","title":"Measure Raspberry Pi temperature using Telegraf, Influxdb, Grafana on k3s"},{"content":"","date":null,"permalink":"/tags/telegraf/","section":"Tags","summary":"","title":"Telegraf"},{"content":"In this post, I‚Äôll share my home lab setup for Rancher\u0026rsquo;s k3s Kubernetes cluster.\nUse-case:\nA web interface to be accessible outside of my home so I could check and manage devices while away Way to manage dynamic DNS since I don‚Äôt have a static I.P. Setup # Setting up a master + single node Kubernetes cluster Deploying DNS updater as a Kubernetes CronJob object Deploying Traefik as a Kubernetes Ingress Controller and configuring it to manage SSL with Let‚Äôs Encrypt Setting up a Pi Kubernetes Cluster #I followed an excellent guide by Alex Ellis here to initialize a cluster on the master and then join a single node.\nk3s kubectl get nodes NAME STATUS ROLES AGE VERSION pi-node1 Ready \u0026lt;none\u0026gt; 3d v1.16.3-k3s.2 pi-master Ready master 3d v1.16.3-k3s.2 DNS and Routing # Add a DNS entry for the wildcard domain *.home.vikaspogu.com to point at the dynamic I.P. Open ports 80 and 443 on the router‚Äôs firewall At this point, a short dig on the domain should return your dynamic I.P.\n$ dig +short test.home.vikaspogu.com X.X.X.X. I found this script online, which will update the DNS record if Dynamic I.P. is changed.\n#!/bin/sh zone=example.com # dnsrecord is the A record which the script will update dnsrecord=www.example.com EMAIL=me@cloudflare.com API_KEY=1234567890abcdef1234567890abcdef # Get the current external I.P. address ip=$(dig +short \u0026lt;CURRENT EXTERNAL IP\u0026gt;) echo \u0026#34;Current IP is $ip\u0026#34; if host $dnsrecord 1.1.1.1 | grep \u0026#34;has address\u0026#34; | grep \u0026#34;$ip\u0026#34;; then echo \u0026#34;$dnsrecord is currently set to $ip; no changes needed.\u0026#34; exit fi # if here, the DNS record needs updating # get the zone id for the requested zone zoneid=$(curl -s -X GET \u0026#34;https://api.cloudflare.com/client/v4/zones?name=$zone\u0026amp;status=active\u0026#34; \\ -H \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\ -H \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; | jq -r \u0026#39;{\u0026#34;result\u0026#34;}[] | .[0] | .id\u0026#39;) echo \u0026#34;Zoneid for $zone is $zoneid\u0026#34; # get the DNS record id dnsrecordid=$(curl -s -X GET \u0026#34;https://api.cloudflare.com/client/v4/zones/$zoneid/dns_records?type=A\u0026amp;name=$dnsrecord\u0026#34; \\ -H \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\ -H \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; | jq -r \u0026#39;{\u0026#34;result\u0026#34;}[] | .[0] | .id\u0026#39;) echo \u0026#34;DNSrecordid for $dnsrecord is $dnsrecordid\u0026#34; # update the record curl -s -X PUT \u0026#34;https://api.cloudflare.com/client/v4/zones/$zoneid/dns_records/$dnsrecordid\u0026#34; \\ -H \u0026#34;X-Auth-Email: $EMAIL\u0026#34; \\ -H \u0026#34;X-Auth-Key: $API_KEY\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ --data \u0026#34;{\\\u0026#34;type\\\u0026#34;:\\\u0026#34;A\\\u0026#34;,\\\u0026#34;name\\\u0026#34;:\\\u0026#34;$dnsrecord\\\u0026#34;,\\\u0026#34;content\\\u0026#34;:\\\u0026#34;$ip\\\u0026#34;,\\\u0026#34;ttl\\\u0026#34;:1,\\\u0026#34;proxied\\\u0026#34;:false}\u0026#34; | jq Create a configmap from the script, secret with Cloudflare EMAIL and GLOBAL_API_TOKEN.\n$ k3s create configmap update-script --from-file=cloudfare-dns-update.sh $ k3s kubectl create secret generic cloudflare --from-literal=email=me@cloudflare.com \\ --from-literal=api_key=1234567890abcdef1234567890abcdef Now create a Kubernetes cronjob to update the DNS record to the correct address.\napiVersion: batch/v1beta1 kind: CronJob metadata: name: dns-update namespace: default spec: schedule: \u0026#34;0 0 * * *\u0026#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - name: dns-update image: e2eteam/dnsutils:1.1-linux-arm command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;/scripts/cloudfare-dns-update.sh\u0026#34;] env: - name: API_KEY valueFrom: secretKeyRef: name: cloudflare key: api_key - name: EMAIL valueFrom: secretKeyRef: name: cloudflare key: email volumeMounts: - name: config-volume mountPath: /scripts volumes: - name: config-volume configMap: name: update-script defaultMode: 0744 Traefik and Let‚Äôs Encrypt #With a functioning cluster and the networking setup complete, the next task is to deploy a reverse proxy to manage the application routing.\nIn Kubernetes, we can deploy an Ingress Controller to achieve this. An Ingress Controller implements a reverse proxy that listens for changes to KubernetesIngress resources and updates its configuration accordingly.\nTraefik provides detailed instructions on Kubernetes implementation, but I customized it slightly to get things working with my setup.\nFirst, create RoleBinding.\nk3s kubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-rbac.yaml Configure Let‚Äôs Encrypt to support HTTPS endpoint and automatically fetch certificates. I used Cloudflare as the DNS provider, configuring Traefik to use DNS records for domain validation.\n[acme] email = \u0026#34;me@cloudflare.com\u0026#34; storage=\u0026#34;./acme.json\u0026#34; entryPoint = \u0026#34;https\u0026#34; acmeLogging=true [acme.dnsChallenge] provider = \u0026#34;cloudflare\u0026#34; delayBeforeCheck = 0 [[acme.domains]] main = \u0026#34;*.home.vikaspogu.com\u0026#34; sans = [\u0026#34;home.vikaspogu.com\u0026#34;] The Deployment objects look like this:\napiVersion: v1 data: traefik.toml: | # traefik.toml logLevel = \u0026#34;info\u0026#34; debug = true insecureSkipVerify = true defaultEntryPoints = [\u0026#34;http\u0026#34;,\u0026#34;https\u0026#34;] [entryPoints] [entryPoints.http] address = \u0026#34;:80\u0026#34; [entryPoints.http.redirect] entryPoint = \u0026#34;https\u0026#34; [entryPoints.https] address = \u0026#34;:443\u0026#34; [entryPoints.https.tls] [api] dashboard = true [kubernetes] [acme] email = \u0026#34;me@cloudflare.com\u0026#34; storage=\u0026#34;./acme.json\u0026#34; entryPoint = \u0026#34;https\u0026#34; acmeLogging=true [acme.dnsChallenge] provider = \u0026#34;cloudflare\u0026#34; delayBeforeCheck = 0 [[acme.domains]] main = \u0026#34;*.home.vikaspogu.com\u0026#34; sans = [\u0026#34;home.vikaspogu.com\u0026#34;] kind: ConfigMap metadata: labels: app: traefik name: traefik-config namespace: default --- apiVersion: v1 kind: ServiceAccount metadata: namespace: default name: traefik-ingress-controller labels: app: traefik --- kind: Deployment apiVersion: apps/v1 metadata: namespace: default name: traefik labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: labels: app: traefik spec: serviceAccountName: traefik-ingress-controller containers: - name: traefik image: traefik:1.7.4 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: admin containerPort: 8080 env: - name: CF_API_EMAIL valueFrom: secretKeyRef: name: cloudflare key: email - name: CF_API_KEY valueFrom: secretKeyRef: name: cloudflare key: api_key volumeMounts: - mountPath: \u0026#34;/config\u0026#34; name: \u0026#34;config\u0026#34; args: - --configfile=/config/traefik.toml volumes: - name: config configMap: name: traefik-config --- kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: default spec: selector: app: traefik ports: - protocol: TCP port: 80 name: http - protocol: TCP port: 443 name: https - protocol: TCP port: 8080 name: admin externalIPs: - 192.168.0.101 # This is the node address Deploy Ingress controller for traefik dashboard.\n--- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: default spec: rules: - host: traefik.home.vikaspogu.com http: paths: - path: / backend: serviceName: traefik-ingress-service servicePort: admin Voila!\nDeploy Kubernetes dashboard #Follow these instructions to deploy dashboard U.I..\nCreate an ingress controller to access the dashboard.\n--- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard-ingress namespace: kubernetes-dashboard spec: rules: - host: kubernetes-dashboard.home.vikaspogu.com http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 ","date":"31 December 2019","permalink":"/blog/kubernetes-home-cluster-traefik/","section":"Blogs","summary":"In this post, I‚Äôll share my home lab setup for Rancher\u0026rsquo;s k3s Kubernetes cluster.","title":"k3s cluster with Raspberry-Pi, Traefik"},{"content":"","date":null,"permalink":"/tags/traefik/","section":"Tags","summary":"","title":"Traefik"},{"content":"Recently, I ran into an issue where pushing images to the docker registry after a build fails.\nPushing image docker-registry.default.svc:5000/simple-go-build/simple-go:latest ... Registry server Address: Registry server User Name: serviceaccount Registry server Email: serviceaccount@example.org Registry server Password: \u0026lt;\u0026lt;non-empty\u0026gt;\u0026gt; error: build error: Failed to push image: received unexpected HTTP status: 500 Internal Server Error Registry pods logs show permission denied.\nerr.code=UNKNOWN err.detail=\u0026#34;filesystem: mkdir /registry/docker/registry/v2/repositories/simple-go-build/simple-go/_uploads/c34415b4-c6d8-42ba-9854-aee449efd984: permission denied\u0026#34; One of the Red Hat solutions articles suggested verifying the file ownership of the files and directories in the volume and comparing it to the uid of the registry.\nChanging the owner recursively to the uid of the registry fixed the issue\nroot@master# chown -R 1001 /exports/registry/docker/ ","date":"18 December 2019","permalink":"/blog/ocp-docker-registry-500-err/","section":"Blogs","summary":"Recently, I ran into an issue where pushing images to the docker registry after a build fails.","title":"Permission denied pushing to OpenShift Registry"},{"content":"This short post looks at adding basic authentication to GoLang applications. Below example application uses the gin web framework.\nLet\u0026rsquo;s start by creating a gin router with default middleware. By default, it serves on :8080 unless we define a PORT environment variable.\nfunc main(){ r := gin.Default() r.GET(\u0026#34;/getAllUsers\u0026#34;, basicAuth, handlers.UsersList) _ = r.Run() } Now that we have our primary route let us create a method to add authentication logic. First, get basic auth credentials from the context request and validate them. The browser will prompt an authentication window for username and password details if the user is not authenticated.\nfunc basicAuth(c *gin.Context) { // Get the Basic Authentication credentials user, password, hasAuth := c.Request.BasicAuth() if hasAuth \u0026amp;\u0026amp; user == \u0026#34;testuser\u0026#34; \u0026amp;\u0026amp; password == \u0026#34;testpass\u0026#34; { log.WithFields(log.Fields{ \u0026#34;user\u0026#34;: user, }).Info(\u0026#34;User authenticated\u0026#34;) } else { c.Abort() c.Writer.Header().Set(\u0026#34;WWW-Authenticate\u0026#34;, \u0026#34;Basic realm=Restricted\u0026#34;) return } } Run application\ngo run main.go Verify if authentication works\ncurl -X GET \u0026#34;http://testuser:testpass@localhost:8080/getAllUsers\u0026#34; ","date":"16 December 2019","permalink":"/blog/golang-basicauth-gin/","section":"Blogs","summary":"This short post looks at adding basic authentication to GoLang applications.","title":"Basic Authentication in Go with Gin"},{"content":"","date":null,"permalink":"/tags/basic-auth/","section":"Tags","summary":"","title":"Basic-Auth"},{"content":"","date":null,"permalink":"/tags/gin/","section":"Tags","summary":"","title":"Gin"},{"content":"","date":null,"permalink":"/tags/golang/","section":"Tags","summary":"","title":"Golang"},{"content":"Set up NFS shares that will serve as storage domains on a Red Hat Enterprise Linux server.\nAdd a firewall rule to RHEL server.\nfirewall-cmd --permanent --add-service nfs firewall-cmd --permanent --add-service mountd firewall-cmd --reload Create and add group kvm; set the ownership of your exported directories to 36:36, which gives vdsm:kvm ownership; change the director\u0026rsquo;s permission so that read and write access is granted to the owner.\ngroupadd kvm -g 36 useradd vdsm -u 36 -g 36 mkdir -pv /home/rhv-data-vol chown -R 36:36 /home/rhv-data-vol chmod 0755 /home/rhv-data-vol Add newly created directory to /etc/exports file, which controls file systems exported to remote hosts and specifies options.\necho \u0026#34;/home/rhv-data-vol 192.168.10.1(rw,sync)\u0026#34; \u0026gt;\u0026gt; /etc/exports exportfs -av Start, enable NFS server; checklist of export server using showmount\nsystemctl start nfs systemctl enable nfs systemctl status nfs showmount -e ","date":"2 November 2019","permalink":"/blog/create-nfs-server-rhel-rhv/","section":"Blogs","summary":"Set up NFS shares that will serve as storage domains on a Red Hat Enterprise Linux server.","title":"Create and add NFS storage to RHV"},{"content":"","date":null,"permalink":"/tags/nfs/","section":"Tags","summary":"","title":"NFS"},{"content":"","date":null,"permalink":"/tags/rhv/","section":"Tags","summary":"","title":"RHV"},{"content":"Couchbase SSL #Suppose you have followed dynamic creation of java keystores in OpenShift post and wondered how to use similar concepts for couchbase database and a java application. This post will help you.\nCouchbase setup #Here is the couchbase documentation for configuring server-side certificates, we are interested in last few steps since OpenShift will generate key and cert by adding an annotation to the couchbase service.\nNote: By adding this annotation, you can dynamically create certificates service.alpha.OpenShift.io/serving-cert-secret-name: couchbase-db-certs\ncouchbase service looks like this:\napiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: \u0026#34;\u0026#34; service.alpha.OpenShift.io/serving-cert-secret-name: couchbase-db-certs labels: component: couchbase-db name: couchbase-db namespace: myproject spec: ports: - name: consolerest port: 8091 protocol: TCP targetPort: 8091 To convert certificates as couchbase expects, we will use an init container.\nWe will use an emptyDir volume to store the cert and key in the /opt/couchbase/var/lib/couchbase/inbox/ location so that couchbase can access them.\nInit container will run a sequence of commands to split certificate, place them into /opt/couchbase/var/lib/couchbase/inbox/ location, and name cert file as chain.pem, key as pkey.key.\nInit container will look as follows:\ninitContainers: - args: - \u0026#34;-c\u0026#34; - \u0026gt;- csplit -z -f crt- $crtfile \u0026#39;/-----BEGIN CERTIFICATE-----/\u0026#39; \u0026#39;{*}\u0026#39; \u0026amp;\u0026amp; for file in crt-*; do cat $file \u0026gt; /opt/couchbase/var/lib/couchbase/inbox/service-$file; done \u0026amp;\u0026amp; cat $crtfile \u0026gt; /opt/couchbase/var/lib/couchbase/inbox/chain.pem \u0026amp;\u0026amp; cat $keyfile \u0026gt; /opt/couchbase/var/lib/couchbase/inbox/pkey.key command: - /bin/bash env: - name: keyfile value: /var/run/secrets/OpenShift.io/services_serving_certs/tls.key - name: crtfile value: /var/run/secrets/OpenShift.io/services_serving_certs/tls.crt - name: password value: changeit image: registry.access.redhat.com/redhat-sso-7/sso72-OpenShift:latest imagePullPolicy: Always name: couchbase-ssl volumeMounts: - mountPath: /var/run/secrets/OpenShift.io/services_serving_certs name: couchbase-db-certs - mountPath: /opt/couchbase/var/lib/couchbase/inbox/ name: couchbase-ssl-volume volumes: - emptyDir: {} name: couchbase-ssl-volume - name: couchbase-db-certs secret: defaultMode: 420 secretName: couchbase-db-certs Next, add couchbase-ssl-volume emptyDir volume mount to the actual container so the file can be accessed by couchbase.\nspec: containers: - env: ... volumeMounts: - mountPath: /opt/couchbase/var/lib/couchbase/inbox/ name: couchbase-ssl-volume I am using a rhel7-couchbase image; on startup, it runs an initialization script to set up the cluster; at that time, we will upload the certificate and activate it using these commands.\ncouchbase-cli ssl-manage -c http://localhost:8091 -u Administrator \\ -p password --upload-cluster-ca=${SERVICE_CERT} couchbase-cli ssl-manage -c http://localhost:8091 -u Administrator \\ -p password --set-node-certificate Pass the cert location as environment variable SERVICE_CERT in deployment config.\n- env: - name: SERVICE_CERT value: /opt/couchbase/var/lib/couchbase/inbox/service-crt-01 Verify logs on the container\nSUCCESS: Uploaded cluster certificate to http://localhost:8091 SUCCESS: Node certificate set We can also verify in couchbase UI\nAt this point, we completed the couchbase setup.\nApplication setup #We will be using same steps as SSL client from dynamically-creating-java-keystores-OpenShift post\nTo make a secure connection to the couchbase, it will need the trust store generated by the pem-to-truststore initContainer. Here is the client‚Äôs app deployment config:\n- apiVersion: v1 kind: DeploymentConfig metadata: labels: app: ssl-client name: ssl-client spec: replicas: 1 selector: deploymentconfig: ssl-client template: metadata: labels: app: ssl-client deploymentconfig: ssl-client spec: containers: - name: ssl-client image: ssl-client imagePullPolicy: Always env: - name: JAVA_OPTIONS value: -Djavax.net.ssl.trustStore=/var/run/secrets/java.io/keystores/truststore.jks -Djavax.net.ssl.trustStorePassword=changeit - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace volumeMounts: - mountPath: /var/run/secrets/java.io/keystores name: keystore-volume initContainers: - name: pem-to-truststore image: registry.access.redhat.com/redhat-sso-7/sso71-OpenShift:1.1-16 env: - name: ca_bundle value: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt - name: truststore_jks value: /var/run/secrets/java.io/keystores/truststore.jks - name: password value: changeit command: [\u0026#34;/bin/bash\u0026#34;] args: [ \u0026#34;-c\u0026#34;, \u0026#34;csplit -z -f crt- $ca_bundle \u0026#39;/-----BEGIN CERTIFICATE-----/\u0026#39; \u0026#39;{*}\u0026#39; \u0026amp;\u0026amp; for file in crt-*; do keytool -import -noprompt -keystore $truststore_jks -file $file -storepass changeit -alias service-$file; done\u0026#34;, ] volumeMounts: - mountPath: /var/run/secrets/java.io/keystores name: keystore-volume volumes: - emtpyDir: {} name: keystore-volume The next step is to enable encryption and pass the path and password of the truststore generated by the initContainer\nCouchbaseEnvironment env = DefaultCouchbaseEnvironment.builder().sslEnabled(true) .sslTruststoreFile(\u0026#34;/var/run/secrets/java.io/keystores/truststore.jks\u0026#34;) .sslTruststorePassword(\u0026#34;changeit\u0026#34;).build(); cachedCluster = CouchbaseCluster.create(env, \u0026#34;couchbase-db\u0026#34;) .authenticate(\u0026#34;Administrator\u0026#34;, \u0026#34;password\u0026#34;); Deploy your application; if successful, you should see similar output in container logs\n2019-08-28 15:33:27.952 INFO 1 --- [cTaskExecutor-1] com.couchbase.client.core.CouchbaseCore : CouchbaseEnvironment: {sslEnabled=true, sslKeystoreFile=\u0026#39;null\u0026#39;, sslTruststoreFile=\u0026#39;/var/run/secrets/java.io/keystores/truststore.jks\u0026#39;, sslKeystorePassword=false, sslTruststorePassword=true, sslKeystore=null, sslTruststore=null, bootstrapHttpEnabled=true, bootstrapCarrierEnabled=true, bootstrapHttpDirectPort=8091, ... ","date":"28 August 2019","permalink":"/blog/couchbase-ssl-openshift/","section":"Blogs","summary":"Couchbase SSL #Suppose you have followed dynamic creation of java keystores in OpenShift post and wondered how to use similar concepts for couchbase database and a java application.","title":"Configuring couchbase SSL for dynamic certificates in OpenShift"},{"content":"","date":null,"permalink":"/tags/couchbase/","section":"Tags","summary":"","title":"Couchbase"},{"content":"","date":null,"permalink":"/tags/tektoncd/","section":"Tags","summary":"","title":"Tektoncd"},{"content":"Recently I came across tektoncd project, The Tekton Pipelines project provides Kubernetes-style resources for declaring CI/CD-style pipelines caught my attention, and I started playing with it.\nBasic Concepts # To create a Tekton pipeline, one does the following:\nCreate custom or install existing reusable Tasks Create a Pipeline and PipelineResources to define your application\u0026rsquo;s delivery pipeline Create a PipelineRun to instantiate and invoke the pipeline Installing Tekton on OpenShift #Log in as a user with cluster-admin privileges.\noc new-project tekton-pipelines oc adm policy add-scc-to-user anyuid -z tekton-pipelines-controller oc apply --filename https://storage.googleapis.com/tekton-releases/latest/release.yaml Install dashboard #The dashboard is a general-purpose, web-based UI for Tekton Pipelines. Tekton also has cli client\ncurl -L https://github.com/tektoncd/dashboard/releases/download/v0/gcr-tekton-dashboard.yaml | oc apply -f - Expose tekton-dashboard service as a route\noc expose service tekton-dashboard \\ -n tekton-pipelines \\ --name \u0026#34;tekton-dashboard\u0026#34; \\ --port=\u0026#34;http\u0026#34; Create pipeline #This pipeline builds an image from the source and starts a new rollout.\nCreate a new project\noc new-project tektontutorial Create a service account for running pipelines and enable it to run privileged pods for building images\noc create serviceaccount pipeline oc adm policy add-scc-to-user privileged -z pipeline oc adm policy add-role-to-user edit -z pipeline Optionally, create OpenShift objects(i.e DeploymentConfig, ImageStream, Service, Route)\n--- apiVersion: image.OpenShift.io/v1 kind: ImageStream metadata: labels: app: go-sample name: go-sample --- apiVersion: apps.OpenShift.io/v1 kind: DeploymentConfig metadata: labels: app: go-sample name: go-sample spec: replicas: 1 revisionHistoryLimit: 10 selector: app: go-sample deploymentconfig: go-sample strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: go-sample deploymentconfig: go-sample spec: containers: - image: go-sample:latest imagePullPolicy: Always livenessProbe: failureThreshold: 3 httpGet: path: / port: 8080 scheme: HTTP initialDelaySeconds: 45 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: go-sample ports: - containerPort: 8080 protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: / port: 8080 scheme: HTTP initialDelaySeconds: 45 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - imageChangeParams: containerNames: - go-sample from: kind: ImageStreamTag name: go-sample:latest namespace: tektontutorial type: ImageChange - type: ConfigChange --- apiVersion: v1 kind: Service metadata: labels: app: go-sample name: go-sample spec: ports: - name: 8080-tcp port: 8080 protocol: TCP targetPort: 8080 selector: app: go-sample deploymentconfig: go-sample sessionAffinity: None type: ClusterIP --- apiVersion: route.OpenShift.io/v1 kind: Route metadata: labels: app: go-sample name: go-sample spec: port: targetPort: 8080-tcp to: kind: Service name: go-sample weight: 100 oc apply -f go-sample-template.yml The deployment will not be complete since there are no container images for the go-sample app.\nCreate a s2i-go, OpenShift-cli task. You can find more examples of reusable tasks in the Tekton Catalog and OpenShift Catalog repositories.\nNote: Tasks consist of several steps that get executed sequentially. The pipeline will perform each task in a separate container within the same pod.\noc apply -f https://raw.githubusercontent.com/OpenShift/pipelines-catalog/master/s2i-go/s2i-go-task.yaml oc create -f https://raw.githubusercontent.com/tektoncd/catalog/master/OpenShift-client/OpenShift-client-task.yaml Create pipeline resources\n--- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: go-sample-image spec: type: image params: - name: url value: image-registry.OpenShift-image-registry.svc:5000/tektontutorial/go-sample --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: go-sample-git spec: type: git params: - name: url value: https://github.com/go-training/helloworld oc apply -f resources.yml Create pipeline; pipelines have two tasks here build and deploy. Build uses s2i-go task to create an image from source and deploy uses OpenShift-client task to rollout latest deployment of go-sample\napiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: go-sample-pipeline spec: resources: - name: app-git type: git - name: app-image type: image tasks: - name: build taskRef: name: s2i-go kind: Task params: - name: TLSVERIFY value: \u0026#34;false\u0026#34; resources: inputs: - name: source resource: app-git outputs: - name: image resource: app-image - name: deploy taskRef: name: OpenShift-client kind: ClusterTask runAfter: - build params: - name: ARGS value: \u0026#34;rollout latest go-sample\u0026#34; oc apply -f go-sample-pipeline.yml Start pipeline on the dashboard or by tektoncli\ntkn pipeline start go-sample-pipeline \\ -r app-git=go-sample-git -r app-image=go-sample-image \\ -s pipeline Click on Create PipelineRun, select pipeline, resources, and service account and start the pipeline.\nOnce finished, you should see all tasks marked in green.\nThe go-sample deployment should have one running pod.\n","date":"9 August 2019","permalink":"/blog/intro-tektoncd-ocp/","section":"Blogs","summary":"Recently I came across tektoncd project, The Tekton Pipelines project provides Kubernetes-style resources for declaring CI/CD-style pipelines caught my attention, and I started playing with it.","title":"TektonCD on OpenShift"},{"content":"I recently worked on a React project with Go backend using Gin web framework. Keycloak was the authentication mechanism for the front end; I also wanted to secure the back end using JSON Web Tokens, which Keycloak provided on every login. JWT verification setup in the Go application was easy.\nFirst, copy the RS256 algorithm public key value from Keycloak.\nSend the token as an Authorization header.\naxios .get(BACKEND_URL.concat(\u0026#34;sampleendpoint\u0026#34;), { headers: { Authorization: this.state.token } }) .then(res =\u0026gt; {}); Now Go-backend setup; let\u0026rsquo;s install the jwt-go, gin-cors libraries:\n$ go get -u github.com/dgrijalva/jwt-go $ go get -u github.com/gin-contrib/cors Add cors config to the router to allow the authorization header.\nrouter.Use(cors.New(cors.Config{ AllowOrigins: []string{\u0026#34;*\u0026#34;}, AllowMethods: []string{\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;OPTIONS\u0026#34;, \u0026#34;HEAD\u0026#34;}, AllowHeaders: []string{\u0026#34;Origin\u0026#34;, \u0026#34;content-type\u0026#34;, \u0026#34;accept\u0026#34;, \u0026#34;authorization\u0026#34;}, ExposeHeaders: []string{\u0026#34;Content-Length\u0026#34;}, AllowCredentials: true, MaxAge: 12 * time.Hour, })) Let\u0026rsquo;s create a custom handler; add the public key from Keycloak and pass it to ParseRSAPublicKeyFromPEM, which will return a key. The key and token are then validated.\nfunc VerifyToken(c *gin.Context) { SecretKey := \u0026#34;-----BEGIN CERTIFICATE-----\\n\u0026#34;+ \u0026#34;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEApn ...... +wnyuCHaCHp8P1yCnwIDAQAB\u0026#34; + \u0026#34;\\n-----END CERTIFICATE-----\u0026#34; reqToken := c.GetHeader(\u0026#34;Authorization\u0026#34;) key, er := jwt.ParseRSAPublicKeyFromPEM([]byte(SecretKey)) if er != nil { fmt.Println(er) c.Abort() c.Writer.WriteHeader(http.StatusUnauthorized) c.Writer.Write([]byte(\u0026#34;Unauthorized\u0026#34;)) return } token, err := jwt.Parse(reqToken, func(token *jwt.Token) (interface{}, error) { // Don\u0026#39;t forget to validate the alg is what you expect: if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(\u0026#34;Unexpected signing method: %v\u0026#34;, token.Header[\u0026#34;alg\u0026#34;]) } return key, nil }) if err != nil { fmt.Println(err) c.Abort() c.Writer.WriteHeader(http.StatusUnauthorized) c.Writer.Write([]byte(\u0026#34;Unauthorized\u0026#34;)) return } if _, ok := token.Claims.(jwt.MapClaims); ok \u0026amp;\u0026amp; token.Valid { fmt.Println(\u0026#34;token is valid\u0026#34;) } } Add the handler to the route.\nrouter.GET(\u0026#34;/sample\u0026#34;, VerifyToken(), handlers.SampleEndpoint) That\u0026rsquo;s it; if the token is valid, you will get the data from the backend, or else you\u0026rsquo;ll see 401 Unauthorized.\n","date":"27 July 2019","permalink":"/blog/sso-jwt-golang/","section":"Blogs","summary":"I recently worked on a React project with Go backend using Gin web framework.","title":"Go JWT Authentication with Keycloak"},{"content":"","date":null,"permalink":"/tags/redhat-sso/","section":"Tags","summary":"","title":"RedHat SSO"},{"content":"","date":null,"permalink":"/tags/react/","section":"Tags","summary":"","title":"React"},{"content":"This post will show you how to secure a React app using RedHat SSO (upstream Keycloak). In this case, OpenID-connect is my identity provider.\nInstall the official Keycloak js adapter\nnpm i keycloak-js --save Add host and port information to the client; in my case, it\u0026rsquo;s localhost:9000\nIn App.js, add a JavaScript object with the required configuration; you will find these configurations under Clients-\u0026gt;Installation.\n//keycloak init options const initOptions = { url: \u0026#34;https://localhost:8080/auth\u0026#34;, realm: \u0026#34;test\u0026#34;, clientId: \u0026#34;react-app\u0026#34;, onLoad: \u0026#34;login-required\u0026#34; }; By default, to authenticate, you need to call the login function. However, there are two options available to make the adapter automatically authenticate. First, you can pass login-required or check-sso to the init function. login-required will authenticate the client if the user is logged-in to {project_name} or display the login page if not. check-sso will only authenticate the client if the user is already logged-in; if the user is not logged in, the browser will be redirected back to the application and remain unauthenticated.\ncomponentDidMount() { let keycloak = Keycloak(initOptions); keycloak.init({ onLoad: initOptions.onLoad }).success(authenticated =\u0026gt; {}); } Finally\nimport * as Keycloak from \u0026#34;keycloak-js\u0026#34;; //keycloak init options const initOptions = { url: \u0026#34;https://localhost:8080/auth\u0026#34;, realm: \u0026#34;test\u0026#34;, clientId: \u0026#34;react-app\u0026#34;, onLoad: \u0026#34;login-required\u0026#34; }; class App extends React.Component { constructor(props) { super(props); this.state = { keycloak: null, authenticated: false }; } componentDidMount() { let keycloak = Keycloak(initOptions); keycloak.init({ onLoad: initOptions.onLoad }).success(authenticated =\u0026gt; { this.setState({ keycloak: keycloak, authenticated: authenticated }); }); } render() { const { keycloak, authenticated } = this.state; if (keycloak) { if (authenticated) { return ( \u0026lt;React.Fragment\u0026gt; \u0026lt;AppRouter /\u0026gt; \u0026lt;/React.Fragment\u0026gt; ); } } return \u0026lt;div\u0026gt;Initializing SS0...\u0026lt;/div\u0026gt;; } } Run your app\n$ npm run start Navigate to http://localhost:9000; you should see the login page if the user is not authenticated.\n","date":"25 July 2019","permalink":"/blog/redhat-sso-react/","section":"Blogs","summary":"This post will show you how to secure a React app using RedHat SSO (upstream Keycloak).","title":"React App with RedHat SSO or keycloak"},{"content":"","date":null,"permalink":"/tags/gluster/","section":"Tags","summary":"","title":"Gluster"},{"content":"","date":null,"permalink":"/tags/heketi/","section":"Tags","summary":"","title":"Heketi"},{"content":"Recently I encountered a JWT token expired error on the heketi pod in the OpenShift cluster.\n[jwt] ERROR 2019/07/16 19:17:14 heketi/middleware/jwt.go:66:middleware.(*HeketiJwtClaims).Valid: exp validation failed: Token is expired by 1h48m59s After a lot of google searches, I synchronized clocks across the pod running heketi and the master nodes, which solved the issue\nntpdate -q 0.rhel.pool.ntp.org; systemctl restart ntpd ","date":"16 July 2019","permalink":"/blog/heketi-jwt-error/","section":"Blogs","summary":"Recently I encountered a JWT token expired error on the heketi pod in the OpenShift cluster.","title":"Heketi JWT token expired error"},{"content":"","date":null,"permalink":"/tags/patternfly/","section":"Tags","summary":"","title":"Patternfly"},{"content":"To order to integrate Patternfly framework into a ReactJS application, create a new project or use an existing one\nnpx create-react-app patternfly-setup-react Install patternfly dependencies react-core, react-table and patternfly\nnpm i --save @patternfly/patternfly \\ @patternfly/react-core @patternfly/react-table Note: Import base.css and patternfly.css in your project, or some components may diverge in appearance\n//These imports are a must to render CSS import \u0026#34;@patternfly/react-core/dist/styles/base.css\u0026#34;; import \u0026#34;@patternfly/patternfly/patternfly.css\u0026#34;; To make sure everything is working correctly, update App.js with a demo layout from documentation\nimport React from \u0026#34;react\u0026#34;; import { Avatar, Brand, Button, ButtonVariant ... ... } from \u0026#34;@patternfly/react-core\u0026#34;; import accessibleStyles from \u0026#34;@patternfly/react-styles/css/utilities/Accessibility/accessibility\u0026#34;; import spacingStyles from \u0026#34;@patternfly/react-styles/css/utilities/Spacing/spacing\u0026#34;; import { css } from \u0026#34;@patternfly/react-styles\u0026#34;; import { BellIcon, CogIcon } from \u0026#34;@patternfly/react-icons\u0026#34;; //These imports are a must to render CSS import \u0026#34;@patternfly/react-core/dist/styles/base.css\u0026#34;; import \u0026#34;@patternfly/patternfly/patternfly.css\u0026#34;; class App extends React.Component { ... ... return ( \u0026lt;React.Fragment\u0026gt; \u0026lt;Page header={Header} sidebar={Sidebar} isManagedSidebar skipToContent={PageSkipToContent} \u0026gt; \u0026lt;PageSection variant={PageSectionVariants.light}\u0026gt; \u0026lt;TextContent\u0026gt; \u0026lt;Text component=\u0026#34;h1\u0026#34;\u0026gt;Main Title\u0026lt;/Text\u0026gt; \u0026lt;Text component=\u0026#34;p\u0026#34;\u0026gt; Body text should be Overpass Regular at 16px. It should have leading of 24px because \u0026lt;br /\u0026gt; of its relative line height of 1.5. \u0026lt;/Text\u0026gt; \u0026lt;/TextContent\u0026gt; \u0026lt;/PageSection\u0026gt; \u0026lt;PageSection\u0026gt; \u0026lt;Gallery gutter=\u0026#34;md\u0026#34;\u0026gt; {Array.apply(0, Array(10)).map((x, i) =\u0026gt; ( \u0026lt;GalleryItem key={i}\u0026gt; \u0026lt;Card\u0026gt; \u0026lt;CardBody\u0026gt;This is a card\u0026lt;/CardBody\u0026gt; \u0026lt;/Card\u0026gt; \u0026lt;/GalleryItem\u0026gt; ))} \u0026lt;/Gallery\u0026gt; \u0026lt;/PageSection\u0026gt; \u0026lt;/Page\u0026gt; \u0026lt;/React.Fragment\u0026gt; ); } } export default App; Start the application\nnpm start You should see a patternfly design like this!\n","date":"1 July 2019","permalink":"/blog/patternfly-setup-react/","section":"Blogs","summary":"To order to integrate Patternfly framework into a ReactJS application, create a new project or use an existing one","title":"Patternfly setup in React Application"},{"content":"","date":null,"permalink":"/tags/reactjs/","section":"Tags","summary":"","title":"ReactJS"},{"content":"This post demonstrates how to authenticate a user against LDAP.\nLet\u0026rsquo;s start by installing basic-auth and ldapauth-fork packages\nnpm install ldapauth-fork npm install basic-auth Steps for implementation;\nAdd packages Create an LDAP variable with authentication configuration Basic auth should prompt for your username and password. Once the user is found, verify the given password by trying to bind the user client with the found LDAP user object and the given password. const auth = require(\u0026#34;basic-auth\u0026#34;); var LdapAuth = require(\u0026#34;ldapauth-fork\u0026#34;); var ldap = new LdapAuth({ url: \u0026#34;ldap://ldap-url:389\u0026#34;, bindDN: \u0026#34;uid=rc,ou=AppAccounts,ou=People,ou=Entsys,dc=example.com\u0026#34;, bindCredentials: \u0026#34;credentials\u0026#34;, searchBase: \u0026#34;ou=entsys,dc=example.com\u0026#34;, searchFilter: \u0026#34;(uid={{username}})\u0026#34;, reconnect: true }); app.use(\u0026#34;/API/admin/\u0026#34;, (req, res, next) =\u0026gt; { const credentials = auth(req); if (credentials) { LDAP.authenticate(credentials.name, credentials.pass, function(err, user) { if (err) { console.log(err.message); return res .status(\u0026#34;401\u0026#34;) .set({ \u0026#34;WWW-Authenticate\u0026#34;: \u0026#39;Basic realm=\u0026#34;Access Denied\u0026#34;\u0026#39; }) .end(\u0026#34;access denied\u0026#34;); } req.user = user; next(); }); } else { return res .status(\u0026#34;401\u0026#34;) .set({ \u0026#34;WWW-Authenticate\u0026#34;: \u0026#39;Basic realm=\u0026#34;Access Denied\u0026#34;\u0026#39; }) .end(\u0026#34;access denied\u0026#34;); } }); Visit basic-auth, ldapauth-fork packages for more information on configuration.\n","date":"15 May 2019","permalink":"/blog/node-ldap-auth/","section":"Blogs","summary":"This post demonstrates how to authenticate a user against LDAP.","title":"Authenticate a Node application with LDAP"},{"content":"Recently I faced an issue where one of my projects got stuck in a terminating state for days. The workaround below fixed the problem.\nExport OpenShift project as a JSON Object\noc get project delete-me -o json \u0026gt; ns-without-finalizers.json Replace below from\nspec: finalizers: - kubernetes to\nspec: finalizers: [] On one of the master nodes, execute these commands.\nkubectl proxy \u0026amp; PID=$! curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize \\ -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @ns-without-finalizers.json kill $PID ","date":"15 May 2019","permalink":"/blog/project-terminating/","section":"Blogs","summary":"Recently I faced an issue where one of my projects got stuck in a terminating state for days.","title":"Deleting an OpenShift project stuck in terminating state"},{"content":"","date":null,"permalink":"/tags/ldap/","section":"Tags","summary":"","title":"LDAP"},{"content":"","date":null,"permalink":"/tags/prometheus/","section":"Tags","summary":"","title":"Prometheus"},{"content":"","date":null,"permalink":"/tags/spring-boot/","section":"Tags","summary":"","title":"Spring Boot"},{"content":"Spring Boot Metrics #This post will discuss how to monitor spring boot application metrics using Prometheus and Grafana.\nPrometheus #Prometheus is a monitoring system that collects metrics from configured targets at intervals.\nGrafana #Grafana is an open-source metric analytics \u0026amp; visualization tool.\nMicrometer #The micrometer is a metrics instrumentation library for JVM-based applications.\nSpring Boot Actuator #Spring Boot Actuator helps you monitor and manage your application when it‚Äôs pushed to production. You can control and monitor your application using HTTP or JMX endpoints.\nSetup #Enable Prometheus metrics by adding dependencies in pom.xml\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.micrometer\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;micrometer-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.1.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.micrometer\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;micrometer-registry-prometheus\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; By default Prometheus endpoint is not available and must be enabled in application.properties. You can find more configurations at spring-boot docs\n#Metrics related configurations management.endpoint.metrics.enabled=true management.endpoints.web.exposure.include=* management.endpoint.prometheus.enabled=true management.metrics.export.prometheus.enabled=true management.metrics.distribution.percentiles-histogram.http.server.requests=true management.metrics.distribution.sla.http.server.requests=1ms,5ms management.metrics.distribution.percentiles.http.server.requests=0.5,0.9,0.95,0.99,0.999 Optionally you can configure any number with the MeterRegistryCustomizer registry (such as applying common tags).\n@Bean MeterRegistryCustomizer\u0026lt;MeterRegistry\u0026gt; metricsCommonTags() { return registry -\u0026gt; registry.config().commonTags(\u0026#34;application\u0026#34;, \u0026#34;sample-app\u0026#34;); } Create a new project; deploy the application and Prometheus in OpenShift.\n$ oc project myproject $ oc new-app redhat-openjdk18-OpenShift~\u0026lt;git_repo_URL\u0026gt; -n sample-app oc new-app prom/prometheus -n prometheus To keep the Prometheus image and configuration decoupled, use the ConfigMap object to inject the Prometheus deployment with the appropriate configuration data.\ncat \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; \u0026gt; prometheus.yml global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: \u0026#39;sample-app\u0026#39; metrics_path: \u0026#39;/actuator/prometheus\u0026#39; static_configs: - targets: [\u0026#39;sample-app:8080\u0026#39;] EOF oc create configmap prom-config-example --from-file=prometheus.yml Next, edit the deployment configuration for Prometheus to include this ConfigMap.\noc edit dc/prometheus Add new volume and volume mount.\n- name: prom-config-example-volume configMap: name: prom-config-example defaultMode: 420 - name: prom-config-example-volume mountPath: /etc/prometheus/ Use an OpenShift Template to run Grafana with persistent storage.\n$ oc process -f https://gist.githubusercontent.com/Vikaspogu/4a67495acf8dba5dc94837e031129fde/raw/e88f42515c6ed101c9554c7c2425794e80e10a64/OpenShift-grafana.yaml | oc apply -f- Once deployed, log in to Grafana using the Route provided in the Template and using the default account admin with password admin (it may be a good idea to change the password after this).\nGrafana Data Source # The Grafana template automatically provisions a Prometheus data source, App-Prometheus, which connects to http://prometheus:9090 via a proxy connection.\nThis works if there is a Prometheus service (called Prometheus) in the same project as Grafana. If this is not the case, it is necessary to edit the data source to point to the appropriate location.\nGrafana Dashboard # The Grafana template automatically provisions sample dashboards. These dashboards are not comprehensive, but you can use them as a starting point for further customization. You can find more official \u0026amp; community built grafana dashboards here\n","date":"15 May 2019","permalink":"/blog/springboot-metrics-grafana/","section":"Blogs","summary":"Spring Boot Metrics #This post will discuss how to monitor spring boot application metrics using Prometheus and Grafana.","title":"Spring Boot metrics with Prometheus and Grafana in OpenShift"},{"content":"This post concerns remote debugging an ASP.NET Core application on OpenShift using Visual Studio Code. You can use any Microsoft proprietary debugger engine vsdbg with Visual Studio Code.\nFirst, list the available .Net application pods using the oc command.\n$ oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 0/1 Running 0 6s $ oc rsh MY_APP_NAME-3-1xrsp sh-4.2$ curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /opt/app-root/vsdbg -r linux-x64 Note: If your container is running behind a corporate proxy and cannot access the internet, you\u0026rsquo;ll have to build a base dotnet image with the installed debugger engine vsdbg.\nCreate (or open) the .vscode/launch.json file inside the source directory of the application (i.e., at the same level as the project folder), then add the following:\n{ \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;.NET Core OpenShift Pod Remote Attach\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;coreclr\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, \u0026#34;processId\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;pipeTransport\u0026#34;: { \u0026#34;pipeProgram\u0026#34;: \u0026#34;oc\u0026#34;, \u0026#34;pipeArgs\u0026#34;: [\u0026#34;exec\u0026#34;, \u0026#34;-it\u0026#34;, \u0026#34;\u0026lt;replace-with-pod-name\u0026gt;\u0026#34;, \u0026#34;--\u0026#34;], \u0026#34;quoteArgs\u0026#34;: false, \u0026#34;debuggerPath\u0026#34;: \u0026#34;/opt/app-root/vsdbg/vsdbg\u0026#34;, \u0026#34;pipeCwd\u0026#34;: \u0026#34;${workspaceRoot}\u0026#34; }, \u0026#34;justMyCode\u0026#34;: false, \u0026#34;sourceFileMap\u0026#34;: { \u0026#34;/opt/app-root/src\u0026#34;: \u0026#34;${workspaceRoot}\u0026#34; } } ] } In Launch.json, replace \u0026lt;replace-with-pod-name\u0026gt; with the pod\u0026rsquo;s name.\nConfirm the PID of the dotnet process in the container. If different, replace the processId in launch.json with the appropriate value. Usually, this value is 1.\nIf the application is built with the Release configuration, the default for .NET Core S2I builder images, justMyCode should be false.\nAs S2I images build the source code in the /opt/app-root/src folder, we should specify this path for sourceFileMap.\nStart debugging the .NET Core app by switching to the debug window, then select .NET Core OpenShift Pod Remote Attach as the configuration and click on the green play button (as shown below), or press the F5 key.\nDebug .NET CORE Remember to add the breakpoints in the source code to see debugging in action!\n","date":"14 May 2019","permalink":"/blog/debug-netcore-openshift/","section":"Blogs","summary":"This post concerns remote debugging an ASP.","title":"Debugging a .NET Core application running on OpenShift"},{"content":"This post will discuss debugging a JAVA application running inside a container.\nRed Hat container images #When you bootstrap your JVM, you should have a way to enable JVM to debug. For example, Red Hat S2I images allow you to control classpath and debugging via environment variables.\n# Set debug options if required if [ x\u0026#34;${JAVA_DEBUG}\u0026#34; != x ] \u0026amp;\u0026amp; [ \u0026#34;${JAVA_DEBUG}\u0026#34; != \u0026#34;false\u0026#34; ]; then java_debug_args=\u0026#34;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${JAVA_DEBUG_PORT:-5005}\u0026#34; fi Setting the JAVA_DEBUG environment variable inside the container to true will append debug args to the JVM startup command Configure port forwarding so that you can connect to your application from a remote debugger If you are using the tomcat image, replace the JAVA_DEBUG environment variable with DEBUG\nUsing the oc command, list the available deployment configurations:\noc get dc Enable Debug #Set the JAVA_DEBUG environment variable in the deployment configuration of your application to true, which configures the JVM to open the port number 5005 for debugging.\noc set env dc/MY_APP_NAME JAVA_DEBUG=true Disabling the health checks is not mandatory but recommended because a pod could be restarted during remote debugging while the process is paused. You can remove the readiness check to prevent a forced restart.\nRedeploy #Redeploy the application if it is not set to redeploy automatically on configuration change.\noc rollout latest dc/MY_APP_NAME Configure port forwarding from your local machine to the application pod. List the currently running pods and find one containing your application. $LOCAL_PORT_NUMBER is an unused port number of your choice on your local machine. Remember this number for the remote debugger configuration.\nIf you are using the tomcat image, replace the port 5005 with 8000\noc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 1/1 Running 0 6s ... oc port-forward MY_APP_NAME-3-1xrsp $LOCAL_PORT_NUMBER:5005 IntelliJ Config #Create a new debug configuration for your application in `IntelliJ IDE:\nClick Run ‚Üí Edit Configurations\nIn the list of configurations, add Remote. Creates a new remote debugging configuration\nEnter a suitable name for the configuration in the name field\nSet the port field to the port number that your application is listening on for debugging\nClick Apply\nClick Run -\u0026gt; Debug -\u0026gt; Select Profile\nWhen done debugging, unset the JAVA_DEBUG environment variable in your application pod.\noc set env dc/MY_APP_NAME JAVA_DEBUG- Non-Red Hat container images #If you are using the OpenJDK image to build an application, update ENTRYPOINT as below to pass options to the JVM through the $JAVA_OPTS environment variable\nFROM openjdk:11.0.3-jdk-slim RUN mkdir /usr/myapp COPY target/java-kubernetes.jar /usr/myapp/app.jar WORKDIR /usr/myapp EXPOSE 8080 ENTRYPOINT [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java $JAVA_OPTS -jar app.jar\u0026#34; ] And then set deployments JAVA_OPTS environment variable.\noc set env deployment MY_APP_NAME JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,address=*:5005,server=y,suspend=n ","date":"14 May 2019","permalink":"/blog/debug-java-container/","section":"Blogs","summary":"This post will discuss debugging a JAVA application running inside a container.","title":"Debugging a Java application in OpenShift."},{"content":"","date":null,"permalink":"/tags/dot-net/","section":"Tags","summary":"","title":"Dot Net"},{"content":"","date":null,"permalink":"/tags/java/","section":"Tags","summary":"","title":"Java"},{"content":"Sometimes writing code that runs is not enough. We might want to know what goes on internally, such as memory allocation, consequences of using one coding approach over another, implications of concurrent executions, areas to improve performance, etc. We can use profilers for this.\nIn this post, I\u0026rsquo;ll discuss using YourKit-JavaProfiler inside a container.\nSince my sample application is built using OpenShift S2I process and pushed into OpenShift internal registry, I\u0026rsquo;ll have to pull the image locally.\ndocker login -p $(oc whoami --show-token) -u admin docker-registry.example.com docker pull docker-registry.example.com/myproject/sample-app:latest Create a new Dockerfile, add a few lines to install YourKit Java Profiler agents, and expose the profiler agent port.\nFROM docker-registry.example.com/myproject/sample-app:latest RUN wget https://www.yourkit.com/download/docker/YourKit-JavaProfiler-2019.1-docker.zip -P /tmp/ \u0026amp;\u0026amp; \\ unzip /tmp/YourKit-JavaProfiler-2019.1-docker.zip -d /usr/local \u0026amp;\u0026amp; \\ rm /tmp/YourKit-JavaProfiler-2019.1-docker.zip EXPOSE 10001 Build and push the image into the registry\ndocker build . -t docker-registry.example.com/myproject/sample-app-profiler:latest docker push docker-registry.example.com/myproject/sample-app-profiler:latest Update the image in the deployment configuration.\nLoad the agent into the JVM by adding a JAVA_TOOL_OPTIONS environment variable in the deployment configuration.\n$ oc set env dc/MY_APP_NAME JAVA_TOOL_OPTIONS=-agentpath:/usr/local/YourKit-JavaProfiler-2019.01/bin/linux-x86-64/libyjpagent.so=port=10001,listen=all oc rollout latest dc/MY_APP_NAME Once the deployment is completed, run oc port forwarding from your local machine to the application pod.\n$ oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 1/1 Running 0 6s ... $ oc port-forward MY_APP_NAME-3-1xrsp 10001:10001 Add a connection in the profiler with localhost:10001, and you\u0026rsquo;re all set.\n","date":"14 May 2019","permalink":"/blog/javaprofiler-openshift/","section":"Blogs","summary":"Sometimes writing code that runs is not enough.","title":"Profiling an application in OpenShift container."},{"content":"","date":null,"permalink":"/tags/yourkit-javaprofiler/","section":"Tags","summary":"","title":"YourKit-JavaProfiler"},{"content":"üöÄ About Me üí° #Greetings, tech aficionados! I\u0026rsquo;m Vikas Pogu, a seasoned code maestro with an 8-year odyssey in the ever-evolving realms of software. By day, I wield my keyboard to architect digital wonders, but by night, I transform into a passionate home lab alchemist, brewing experiments in the enchanting world of personal tech sanctuaries.\nüåê Journey Through Code #With over 8 years of industry experience, my coding adventures have traversed languages like Java, Golang, and JavaScript. I am a consulting architect at Red Hat, focusing primarily on helping enterprise organizations adopt OpenShift, Kubernetes, Ansible, and native cloud application development.\nüè° Home Lab Enthusiast #Beyond the corporate code, I\u0026rsquo;m a fervent enthusiast of the home lab cosmos. From servers humming in the corner to IoT devices orchestrating a smart haven, my home lab is a playground for tech experiments. Dive into my blog to explore the intersection of professional expertise and the joy of tinkering within the cozy confines of my home lab.\nüöÄ Tech Arsenal # Languages: Fluent in the dialects of Java, Basic to Intermediate Golang, and JavaScript. Scripting: Bash. Databases: SQL and NoSQL. Cloud Wizardry: OpenShift, Kubernetes, AWS, Azure and Docker. Version Control: Git DevOps Charms: FluxCD, ArgoCD, TektonCD, Helm, Jenkins and Ansible. This blog is not just a chronicle of my journey; it\u0026rsquo;s an invitation for you to embark on your own tech adventures alongside me.\n","date":"1 January 0001","permalink":"/about/","section":"Welcome to Tech Chronicles! üéâ","summary":"üöÄ About Me üí° #Greetings, tech aficionados!","title":"About"}]