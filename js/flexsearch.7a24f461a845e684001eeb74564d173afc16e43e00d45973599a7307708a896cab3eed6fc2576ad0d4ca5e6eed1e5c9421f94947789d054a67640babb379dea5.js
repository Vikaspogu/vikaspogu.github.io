(()=>{var ae=Object.create;var te=Object.defineProperty;var ie=Object.getOwnPropertyDescriptor;var se=Object.getOwnPropertyNames;var re=Object.getPrototypeOf,ue=Object.prototype.hasOwnProperty;var le=(e,n)=>()=>(n||e((n={exports:{}}).exports,n),n.exports);var ce=(e,n,o,i)=>{if(n&&typeof n=="object"||typeof n=="function")for(let s of se(n))!ue.call(e,s)&&s!==o&&te(e,s,{get:()=>n[s],enumerable:!(i=ie(n,s))||i.enumerable});return e};var pe=(e,n,o)=>(o=e!=null?ae(re(e)):{},ce(n||!e||!e.__esModule?te(o,"default",{value:e,enumerable:!0}):o,e));var ne=le((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let n=Array(e);for(let o=0;o<e;o++)n[o]=v();return n}function v(){return Object.create(null)}function ba(e,n){return n.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,n){var o=da;if(e&&(n&&(e=E(e,n)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),o||o==="")){if(e=e.split(o),this.filter){n=this.filter,o=e.length;let i=[];for(let s=0,r=0;s<o;s++){let l=e[s];l&&!n[l]&&(i[r++]=l)}e=i}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,n){let o=Object.keys(e),i=o.length,s=[],r="",l=0;for(let p=0,h,g;p<i;p++)h=o[p],(g=e[h])?(s[l++]=F(n?"(?!\\b)"+h+"(\\b|_)":h),s[l++]=g):r+=(r?"|":"")+h;return r&&(s[l++]=F(n?"(?!\\b)("+r+")(\\b|_)":"("+r+")"),s[l]=""),s}function E(e,n){for(let o=0,i=n.length;o<i&&(e=e.replace(n[o],n[o+1]),e);o+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let n="",o="";for(let i=0,s=e.length,r;i<s;i++)(r=e[i])!==o&&(n+=o=r);return n}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,n){e[n+"Async"]=function(){let o=this,i=arguments;var s=i[i.length-1];let r;return D(s)&&(r=s,delete i[i.length-1]),s=new Promise(function(l){setTimeout(function(){o.async=!0;let p=o[n].apply(o,i);o.async=!1,l(p)})}),r?(s.then(r),this):s}}function ma(e,n,o,i){let s=e.length,r=[],l,p,h=0;i&&(i=[]);for(let g=s-1;0<=g;g--){let f=e[g],A=f.length,k=v(),w=!l;for(let m=0;m<A;m++){let y=f[m],j=y.length;if(j)for(let q=0,_,R;q<j;q++)if(R=y[q],l){if(l[R]){if(!g){if(o)o--;else if(r[h++]=R,h===n)return r}(g||i)&&(k[R]=1),w=!0}if(i&&(p[R]=(_=p[R])?++_:_=1,_<s)){let H=i[_-2]||(i[_-2]=[]);H[H.length]=R}}else k[R]=1}if(i)l||(p=k);else if(!w)return[];l=k}if(i)for(let g=i.length-1,f,A;0<=g;g--){f=i[g],A=f.length;for(let k=0,w;k<A;k++)if(w=f[k],!l[w]){if(o)o--;else if(r[h++]=w,h===n)return r;l[w]=1}}return r}function na(e,n){let o=v(),i=v(),s=[];for(let r=0;r<e.length;r++)o[e[r]]=1;for(let r=0,l;r<n.length;r++){l=n[r];for(let p=0,h;p<l.length;p++)h=l[p],o[h]&&!i[h]&&(i[h]=1,s[s.length]=h)}return s}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,n,o){C(e)&&(e=e.query);let i=this.cache.get(e);return i||(i=this.search(e,n,o),this.cache.set(e,i)),i}J.prototype.set=function(e,n){if(!this.cache[e]){var o=this.h.length;for(o===this.l?delete this.cache[this.h[o-1]]:o++,--o;0<o;o--)this.h[o]=this.h[o-1];this.h[0]=e}this.cache[e]=n},J.prototype.get=function(e){let n=this.cache[e];if(this.l&&n&&(e=this.h.indexOf(e))){let o=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=o}return n};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,n,o,i,s,r){setTimeout(function(){let l=e(o,JSON.stringify(r));l&&l.then?l.then(function(){n.export(e,n,o,i,s+1)}):n.export(e,n,o,i,s+1)})}function K(e,n){if(!(this instanceof K))return new K(e);var o;if(e){x(e)?e=qa[e]:(o=e.preset)&&(e=Object.assign({},o[o],e)),o=e.charset;var i=e.lang;x(o)&&(o.indexOf(":")===-1&&(o+=":default"),o=G[o]),x(i)&&(i=ka[i])}else e={};let s,r,l=e.context||{};if(this.encode=e.encode||o&&o.encode||ia,this.register=n||v(),this.D=s=e.resolution||9,this.G=n=o&&o.G||e.tokenize||"strict",this.depth=n==="strict"&&l.depth,this.l=u(l.bidirectional),this.s=r=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=r?aa(s):v(),this.A=s=l.resolution||1,this.h=r?aa(s):v(),this.F=o&&o.F||e.rtl,this.H=(n=e.matcher||i&&i.H)&&fa(n,!1),this.J=(n=e.stemmer||i&&i.J)&&fa(n,!0),o=n=e.filter||i&&i.filter){o=n,i=v();for(let p=0,h=o.length;p<h;p++)i[o[p]]=1;o=i}this.filter=o,this.cache=(n=e.cache)&&new J(n)}t=K.prototype,t.append=function(e,n){return this.add(e,n,!0)},t.add=function(e,n,o,i){if(n&&(e||e===0)){if(!i&&!o&&this.register[e])return this.update(e,n);if(n=this.encode(n),i=n.length){let g=v(),f=v(),A=this.depth,k=this.D;for(let w=0;w<i;w++){let m=n[this.F?i-1-w:w];var s=m.length;if(m&&s>=this.B&&(A||!f[m])){var r=L(k,i,w),l="";switch(this.G){case"full":if(3<s){for(r=0;r<s;r++)for(var p=s;p>r;p--)if(p-r>=this.B){var h=L(k,i,w,s,r);l=m.substring(r,p),M(this,f,l,h,e,o)}break}case"reverse":if(2<s){for(p=s-1;0<p;p--)l=m[p]+l,l.length>=this.B&&M(this,f,l,L(k,i,w,s,p),e,o);l=""}case"forward":if(1<s){for(p=0;p<s;p++)l+=m[p],l.length>=this.B&&M(this,f,l,r,e,o);break}default:if(this.C&&(r=Math.min(r/this.C(n,m,w)|0,k-1)),M(this,f,m,r,e,o),A&&1<i&&w<i-1){for(s=v(),l=this.A,r=m,p=Math.min(A+1,i-w),s[r]=1,h=1;h<p;h++)if((m=n[this.F?i-1-w-h:w+h])&&m.length>=this.B&&!s[m]){s[m]=1;let y=this.l&&m>r;M(this,g,y?r:m,L(l+(i/2>l?0:1),i,w,p-1,h-1),e,o,y?m:r)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,n,o,i,s){return o&&1<e?n+(i||0)<=e?o+(s||0):(e-1)/(n+(i||0))*(o+(s||0))+1|0:0}function M(e,n,o,i,s,r,l){let p=l?e.h:e.map;(!n[o]||l&&!n[o][l])&&(e.s&&(p=p[i]),l?(n=n[o]||(n[o]=v()),n[l]=1,p=p[l]||(p[l]=v())):n[o]=1,p=p[o]||(p[o]=[]),e.s||(p=p[i]||(p[i]=[])),r&&p.indexOf(s)!==-1||(p[p.length]=s,e.m&&(e=e.register[s]||(e.register[s]=[]),e[e.length]=p)))}t.search=function(e,n,o){o||(!n&&C(e)?(o=e,e=o.query):C(n)&&(o=n));let i=[],s,r,l=0;if(o){n=o.limit,l=o.offset||0;var p=o.context;r=o.suggest}if(e&&(e=this.encode(e),s=e.length,1<s)){o=v();var h=[];for(let f=0,A=0,k;f<s;f++)if((k=e[f])&&k.length>=this.B&&!o[k])if(this.s||r||this.map[k])h[A++]=k,o[k]=1;else return i;e=h,s=e.length}if(!s)return i;n||(n=100),p=this.depth&&1<s&&p!==!1,o=0;let g;p?(g=e[0],o=1):1<s&&e.sort(ba);for(let f,A;o<s;o++){if(A=e[o],p?(f=sa(this,i,r,n,l,s===2,A,g),r&&f===!1&&i.length||(g=A)):f=sa(this,i,r,n,l,s===1,A),f)return f;if(r&&o===s-1){if(h=i.length,!h){if(p){p=0,o=-1;continue}return i}if(h===1)return ta(i[0],n,l)}}return ma(i,n,l,r)};function sa(e,n,o,i,s,r,l,p){let h=[],g=p?e.h:e.map;if(e.s||(g=ua(g,l,p,e.l)),g){let f=0,A=Math.min(g.length,p?e.A:e.D);for(let k=0,w=0,m,y;k<A&&!((m=g[k])&&(e.s&&(m=ua(m,l,p,e.l)),s&&m&&r&&(y=m.length,y<=s?(s-=y,m=null):(m=m.slice(s),s=0)),m&&(h[f++]=m,r&&(w+=m.length,w>=i))));k++);if(f){if(r)return ta(h,i,0);n[n.length]=h;return}}return!o&&h}function ta(e,n,o){return e=e.length===1?e[0]:[].concat.apply([],e),o||e.length>n?e.slice(o,o+n):e}function ua(e,n,o,i){return o?(i=i&&n>o,e=(e=e[i?n:o])&&e[i?o:n]):e=e[n],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,n){return this.remove(e).add(e,n)},t.remove=function(e,n){let o=this.register[e];if(o){if(this.m)for(let i=0,s;i<o.length;i++)s=o[i],s.splice(s.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(n||delete this.register[e],this.cache){n=this.cache;for(let i=0,s,r;i<n.h.length;i++)r=n.h[i],s=n.cache[r],s.indexOf(e)!==-1&&(n.h.splice(i--,1),delete n.cache[r])}}return this};function N(e,n,o,i,s){let r=0;if(e.constructor===Array)if(s)n=e.indexOf(n),n!==-1?1<e.length&&(e.splice(n,1),r++):r++;else{s=Math.min(e.length,o);for(let l=0,p;l<s;l++)(p=e[l])&&(r=N(p,n,o,i,s),i||r||delete e[l])}else for(let l in e)(r=N(e[l],n,o,i,s))||delete e[l];return r}t.searchCache=oa,t.export=function(e,n,o,i,s){let r,l;switch(s||(s=0)){case 0:if(r="reg",this.m){l=v();for(let p in this.register)l[p]=1}else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:return}return ra(e,n||this,o?o+"."+r:r,i,s,l),!0},t.import=function(e,n){if(n)switch(x(n)&&(n=JSON.parse(n)),e){case"cfg":this.s=!!n.opt;break;case"reg":this.m=!1,this.register=n;break;case"map":this.map=n;break;case"ctx":this.h=n}},la(K.prototype);function va(e){e=e.data;var n=self._index;let o=e.args;var i=e.task;switch(i){case"init":i=e.options||{},e=e.factory,n=i.encode,i.cache=!1,n&&n.indexOf("function")===0&&(i.encode=Function("return "+n)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(i),delete self.FlexSearch):self._index=new K(i);break;default:e=e.id,n=n[i].apply(n,o),postMessage(i==="search"?{id:e,msg:n}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var n;e?D(n=e.encode)&&(e.encode=n.toString()):e={},(n=(self||window)._factory)&&(n=n.toString());let o=self.exports,i=this;this.o=xa(n,o,e.worker),this.h=v(),this.o&&(o?this.o.on("message",function(s){i.h[s.id](s.msg),delete i.h[s.id]}):this.o.onmessage=function(s){s=s.data,i.h[s.id](s.msg),delete i.h[s.id]},this.o.postMessage({task:"init",factory:n,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let n=this,o=[].slice.call(arguments);var i=o[o.length-1];let s;return D(i)&&(s=i,o.splice(o.length-1,1)),i=new Promise(function(r){setTimeout(function(){n.h[++wa]=r,n.o.postMessage({task:e,id:wa,args:o})})}),s?(i.then(s),this):i}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var n=e.document||e.doc||e,o;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(o=n.key||n.id)&&S(o,this.A)||"id",this.m=u(e.fastupdate),this.C=(o=n.store)&&o!==!0&&[],this.store=o&&v(),this.I=(o=n.tag)&&S(o,this.A),this.l=o&&v(),this.cache=(o=e.cache)&&new J(o),e.cache=!1,this.o=e.worker,this.async=!1,o=v();let i=n.index||n.field||n;x(i)&&(i=[i]);for(let s=0,r,l;s<i.length;s++)r=i[s],x(r)||(l=r,r=r.field),l=C(l)?Object.assign({},e,l):e,this.o&&(o[r]=new O(l),o[r].o||(this.o=!1)),this.o||(o[r]=new K(l,this.register)),this.K[s]=S(r,this.A),this.h[s]=r;if(this.C)for(e=n.store,x(e)&&(e=[e]),n=0;n<e.length;n++)this.C[n]=S(e[n],this.A);this.index=o}function S(e,n){let o=e.split(":"),i=0;for(let s=0;s<o.length;s++)e=o[s],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(n[i]=!0),e&&(o[i++]=e);return i<o.length&&(o.length=i),1<i?o:o[0]}function T(e,n){if(x(n))e=e[n];else for(let o=0;e&&o<n.length;o++)e=e[n[o]];return e}function U(e,n,o,i,s){if(e=e[s],i===o.length-1)n[s]=e;else if(e)if(e.constructor===Array)for(n=n[s]=Array(e.length),s=0;s<e.length;s++)U(e,n,o,i,s);else n=n[s]||(n[s]=v()),s=o[++i],U(e,n,o,i,s)}function V(e,n,o,i,s,r,l,p){if(e=e[l])if(i===n.length-1){if(e.constructor===Array){if(o[i]){for(n=0;n<e.length;n++)s.add(r,e[n],!0,!0);return}e=e.join(" ")}s.add(r,e,p,!0)}else if(e.constructor===Array)for(l=0;l<e.length;l++)V(e,n,o,i,s,r,l,p);else l=n[++i],V(e,n,o,i,s,r,l,p)}t=Q.prototype,t.add=function(e,n,o){if(C(e)&&(n=e,e=T(n,this.key)),n&&(e||e===0)){if(!o&&this.register[e])return this.update(e,n);for(let i=0,s,r;i<this.h.length;i++)r=this.h[i],s=this.K[i],x(s)&&(s=[s]),V(n,s,this.A,0,this.index[r],e,s[0],o);if(this.I){let i=T(n,this.I),s=v();x(i)&&(i=[i]);for(let r=0,l,p;r<i.length;r++)if(l=i[r],!s[l]&&(s[l]=1,p=this.l[l]||(this.l[l]=[]),!o||p.indexOf(e)===-1)&&(p[p.length]=e,this.m)){let h=this.register[e]||(this.register[e]=[]);h[h.length]=p}}if(this.store&&(!o||!this.store[e])){let i;if(this.C){i=v();for(let s=0,r;s<this.C.length;s++)r=this.C[s],x(r)?i[r]=n[r]:U(n,i,r,0,r[0])}this.store[e]=i||n}}return this},t.append=function(e,n){return this.add(e,n,!0)},t.update=function(e,n){return this.remove(e).add(e,n)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var n=0;n<this.h.length&&(this.index[this.h[n]].remove(e,!this.o),!this.m);n++);if(this.I&&!this.m)for(let o in this.l){n=this.l[o];let i=n.indexOf(e);i!==-1&&(1<n.length?n.splice(i,1):delete this.l[o])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,n,o,i){o||(!n&&C(e)?(o=e,e=o.query):C(n)&&(o=n,n=0));let s=[],r=[],l,p,h,g,f,A,k=0;if(o)if(o.constructor===Array)h=o,o=null;else{if(h=(l=o.pluck)||o.index||o.field,g=o.tag,p=this.store&&o.enrich,f=o.bool==="and",n=o.limit||100,A=o.offset||0,g&&(x(g)&&(g=[g]),!e)){for(let m=0,y;m<g.length;m++)(y=ya.call(this,g[m],n,A,p))&&(s[s.length]=y,k++);return k?s:[]}x(h)&&(h=[h])}h||(h=this.h),f=f&&(1<h.length||g&&1<g.length);let w=!i&&(this.o||this.async)&&[];for(let m=0,y,j,q;m<h.length;m++){let _;if(j=h[m],x(j)||(_=j,j=j.field),w)w[m]=this.index[j].searchAsync(e,n,_||o);else{if(i?y=i[m]:y=this.index[j].search(e,n,_||o),q=y&&y.length,g&&q){let R=[],H=0;f&&(R[0]=[y]);for(let X=0,ee,$;X<g.length;X++)ee=g[X],(q=($=this.l[ee])&&$.length)&&(H++,R[R.length]=f?[$]:$);H&&(y=f?ma(R,n||100,A||0):na(y,R),q=y.length)}if(q)r[k]=j,s[k++]=y;else if(f)return[]}}if(w){let m=this;return new Promise(function(y){Promise.all(w).then(function(j){y(m.search(e,n,o,j))})})}if(!k)return[];if(l&&(!p||!this.store))return s[0];for(let m=0,y;m<r.length;m++){if(y=s[m],y.length&&p&&(y=za.call(this,y)),l)return y;s[m]={field:r[m],result:y}}return s};function ya(e,n,o,i){let s=this.l[e],r=s&&s.length-o;if(r&&0<r)return(r>n||o)&&(s=s.slice(o,o+n)),i&&(s=za.call(this,s)),{tag:e,result:s}}function za(e){let n=Array(e.length);for(let o=0,i;o<e.length;o++)i=e[o],n[o]={id:i,doc:this.store[i]};return n}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,n){return this.store[e]=n,this},t.searchCache=oa,t.export=function(e,n,o,i,s){if(s||(s=0),i||(i=0),i<this.h.length){let r=this.h[i],l=this.index[r];n=this,setTimeout(function(){l.export(e,n,s?r.replace(":","-"):"",i,s++)||(i++,s=1,n.export(e,n,r,i,s))})}else{let r;switch(s){case 1:o="tag",r=this.l;break;case 2:o="store",r=this.store;break;default:return}ra(e,this,o,i,s,r)}},t.import=function(e,n){if(n)switch(x(n)&&(n=JSON.parse(n)),e){case"tag":this.l=n;break;case"reg":this.m=!1,this.register=n;for(let i=0,s;i<this.h.length;i++)s=this.index[this.h[i]],s.register=n,s.m=!1;break;case"store":this.store=n;break;default:e=e.split(".");let o=e[0];e=e[1],o&&e&&this.index[o].import(e,n)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var n=e;return n.normalize&&(n=n.normalize("NFD").replace(ea,"")),ca.call(this,n.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let n=[];if(e){let o=e.split(Fa),i=o.length;for(let s=0,r,l=0;s<i;s++)if((e=o[s])&&(!this.filter||!this.filter[e])){r=e[0];let p=Ga[r]||r,h=p;for(let g=1;g<e.length;g++){r=e[g];let f=Ga[r]||r;f&&f!==h&&(p+=f,h=f)}n[l++]=p}}return n}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,n){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),n||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,n){G[e]=n},registerLanguage:function(e,n){ka[e]=n}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var oe=pe(ne());var z=document.getElementById("search__text"),B=document.getElementById("search__suggestions");z!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),z.focus()):e.key==="Escape"&&(z.blur(),B.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{B.contains(e.target)||B.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(B.classList.contains("search__suggestions--hidden"))return;let o=[...B.querySelectorAll("a")];if(o.length===0)return;let i=o.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let s=i+1<o.length?i+1:i;o[s].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=i>0?i-1:0,o[nextIndex].focus())});(function(){let e=new oe.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/blog/argo-operator-ksops-age/",title:"ArgoCD with Kustomize and KSOPS using Age encryption",description:`I am a big fan of FluxCD and its integration with secrets management; however, recently, I decided to tinker with ArgoCD. One of the challenges I encountered was secrets management. Although ArgoCD provides flexible integration with most secrets management tools, it requires little extra configuration.
So I started my journey to configure ArgoCD with SOPS; there isn&rsquo;t a direct integration with SOPS. So we will have to use the Kustomize plugin called ksops, which is the suggested tool on Argo&rsquo;s website and it has pretty good instructions for Argo integration.`,content:`I am a big fan of FluxCD and its integration with secrets management; however, recently, I decided to tinker with ArgoCD. One of the challenges I encountered was secrets management. Although ArgoCD provides flexible integration with most secrets management tools, it requires little extra configuration.
So I started my journey to configure ArgoCD with SOPS; there isn&rsquo;t a direct integration with SOPS. So we will have to use the Kustomize plugin called ksops, which is the suggested tool on Argo&rsquo;s website and it has pretty good instructions for Argo integration.
What is KSOPS? # KSOPS, or kustomize-SOPS, is a kustomize plugin for managing SOPS encrypted resources. KSOPS is used to decrypt any Kubernetes resources but is most commonly used to decrypt encrypted Kubernetes Secrets and ConfigMaps. The main goal of KSOPS is to manage encrypted resources the same way we manage the Kubernetes manifests.
Setup # I am setting up this integration on my OpenShift cluster and deploying ArgoCD via Operator. For file encryption, I&rsquo;ll use Age encryption tool recommended over PGP by SOPS documentation.
Install and Generate the age key # Install age package
Generating an age key using age-keygen:
$ age-keygen -o age.agekey Public key: age1helqcqsh9464r8chnwc2fzj8uv7vr5ntnsft0tn45v2xtz0hpfwq98cmsg Create a secret with the age private key; the key name must end with .agekey to be detected as an age key: Create a secret with the age key in openshift-gitops project # $ cat age.agekey | kubectl create secret generic sops-age --namespace=openshift-gitops \\ --from-file=key.txt=/dev/stdin Update the repo definition of ArgoCD CR to configure KSOPS custom tooling # Below configuration mounts the age secret and installs the KSOPS tool using initContainer
repo: env: - name: XDG_CONFIG_HOME value: /.config - name: SOPS_AGE_KEY_FILE value: /.config/sops/age/keys.txt volumes: - name: custom-tools emptyDir: {} - name: sops-age secret: secretName: sops-age initContainers: - name: install-ksops image: viaductoss/ksops:v3.0.2 command: [&#34;/bin/sh&#34;, &#34;-c&#34;] args: - &#39;echo &#34;Installing KSOPS...&#34;; cp ksops /custom-tools/; cp $GOPATH/bin/kustomize /custom-tools/; echo &#34;Done.&#34;;&#39; volumeMounts: - mountPath: /custom-tools name: custom-tools volumeMounts: - mountPath: /usr/local/bin/kustomize name: custom-tools subPath: kustomize - mountPath: /.config/kustomize/plugin/viaduct.ai/v1/ksops/ksops name: custom-tools subPath: ksops - mountPath: /.config/sops/age/keys.txt name: sops-age subPath: keys.txt Here is a guide for creating and testing the resources
`}).add({id:1,href:"/blog/influxdb-grafana/",title:"Configure Influxdb with Grafana",description:`I recently started using proxmox and was planning on monitoring metrics using InfluxDB and Grafana, which I have already deployed on my Kubernetes cluster. However, during that process, I encountered two issues:
Authentication &ldquo;Database not found&rdquo; Authentication # For the Authentication issue, I found out that you have to use the Authorization header with the Token yourAuthToken value to access the bucket. So, configure the header name in the jsonData field, and we should configure the header value in secureJsonData for grafana helm chart values.`,content:`I recently started using proxmox and was planning on monitoring metrics using InfluxDB and Grafana, which I have already deployed on my Kubernetes cluster. However, during that process, I encountered two issues:
Authentication &ldquo;Database not found&rdquo; Authentication # For the Authentication issue, I found out that you have to use the Authorization header with the Token yourAuthToken value to access the bucket. So, configure the header name in the jsonData field, and we should configure the header value in secureJsonData for grafana helm chart values.
secureJsonData: httpHeaderValue1: &#39;Token \${INFLUXDB_TOKEN}&#39; jsonData: httpHeaderName1: &#39;Authorization&#39; Complete datasources helm values
datasources: datasources.yaml: apiVersion: 1 deleteDatasources: - name: Loki orgId: 1 datasources: - name: Prometheus type: prometheus url: http://prometheus-operated:9090 access: proxy isDefault: true - name: InfluxDB type: influxdb url: &#34;http://influxdb.default.svc.cluster.local:80&#34; database: &#34;default&#34; secureJsonData: httpHeaderValue1: &#39;Token \${INFLUXDB_TOKEN}&#39; access: proxy jsonData: httpHeaderName1: &#39;Authorization&#39; httpMode: GET &ldquo;Database not found&rdquo; # This one took me a while to figure out. In InfluxDB v1, data gets stored in databases and retention policies. In InfluxDB v2, data gets stored in buckets. Because InfluxQL uses the v1 data model, before querying in InfluxQL, we must map a bucket to a database and retention policy. Find more information here.
As shown below, I am using the InfluxDB API method to create DBRP mappings for unmapped buckets.
curl --request POST http://localhost:8086/api/v2/dbrps \\ --header &#34;Authorization: Token YourAuthToken&#34; \\ --header &#39;Content-type: application/json&#39; \\ --data &#39;{ &#34;bucketID&#34;: &#34;00oxo0oXx000x0Xo&#34;, &#34;database&#34;: &#34;example-db&#34;, &#34;default&#34;: true, &#34;orgID&#34;: &#34;00oxo0oXx000x0Xo&#34;, &#34;retention_policy&#34;: &#34;example-rp&#34; }&#39; Make sure to update Host, Token, bucketID, database, orgID, retention_policy with your values.
Lastly, go to Grafana, choose the InfluxDB data source, and try to run a query to see that the setup process was successful.
`}).add({id:2,href:"/blog/tftp-udm-pxe/",title:"PXE boot with Synology NAS and UDM router",description:`In this post, I have documented the steps I followed to install RHEL 8 by booting from a PXE server over the network with a Kickstart file using Synology NAS as TFTP, HTTP server, and UDM as DHCP.
Install &amp; Configure TFTP Install &amp; Configure HTTP server Enable network boot on UDM PXE Boot setup Prepare Installation Repository Prepare kickstart file Perform PXE boot Install and Configure TFTP # Trivial File Transfer Protocol (TFTP) is a simple file transfer protocol, generally used for transferring configurations or boot files when authentication is not required.`,content:`In this post, I have documented the steps I followed to install RHEL 8 by booting from a PXE server over the network with a Kickstart file using Synology NAS as TFTP, HTTP server, and UDM as DHCP.
Install &amp; Configure TFTP Install &amp; Configure HTTP server Enable network boot on UDM PXE Boot setup Prepare Installation Repository Prepare kickstart file Perform PXE boot Install and Configure TFTP # Trivial File Transfer Protocol (TFTP) is a simple file transfer protocol, generally used for transferring configurations or boot files when authentication is not required.
Synology NAS comes with TFTP file services.
To configure TFTP # Go to Control Panel &gt; File Services &gt; Advanced
Select Enable TFTP Service In the TFTP root folder field, specify which folder on the Synology NAS can be accessed by TFTP clients Install and Configure HTTP server # We also need a service to host our image repository; we can use FTP or HTTP, or NFS to host our image repository. Synology NAS comes with web hosting features. In Web Station, we will create a port-based virtual host to host the contents of our image repository.
Install the following packages from the Synology package center
Web Station Apache HTTP Server 2.4 Create a virtual host # Open Web Station and go to Web Service Portal &gt; Select Create Service Portal
Select Virtual Host
In the virtual host wizard
Select Port-based with protocol as HTTP and enter port number 8080 Select the Document root directory, which is the exact PXE location as TFTP Select Apache HTTP Server as the back-end server Click Create to save settings We need to enable simple directory browsing on the virtual host to access the contents of images and configuration files to perform PXE Boot.
In the Document root directory, create a file named .htaccess with the following content.
Options +Indexes Navigate to IP:8080, and you should see a similar screen with files in the folder.
PXE Boot Setup # Prepare Installation Repository # Download the ISO image file of the complete installation DVD Create a directory named images/rhel/8.6 under the root directory of the TFTP/HTTP server Mount the ISO image Copy all the files from the ISO to the previously created directory on TFTP/HTTP server. You will need around 10GB of storage to copy all the files from the ISO to the local directory. mount rhel-8.6-x86_64-dvd.iso /mnt scp -pr /mnt/* nas-user@nas-address:/media/PXE/images/rhel/8.6 To perform the UEFI PXE Boot installation, we will need the following PXE boot files.
grubx64.efi provided by grub2-efi-x64 rpm shimx64.efi provided by shim-x64 rpm BOOTX64.EFI provided by shim-x64 rpm cp -pr /mnt/BaseOS/Packages/grub2-efi-x64-2.02-81.el8.x86_64.rpm /tmp cp -pr /mnt/BaseOS/Packages/shim-x64-15-11.el8.x86_64.rpm /tmp/ Extract the packages
cd /tmp rpm2cpio shim-x64-15-11.el8.x86_64.rpm | cpio -dimv rpm2cpio grub2-efi-x64-2.02-81.el8.x86_64.rpm | cpio -dimv Copy the EFI boot images from the /tmp directory to the PXE root directory
scp /tmp/boot/efi/EFI/BOOT/BOOTX64.EFI nas-user@nas-address:/media/PXE scp /tmp/boot/efi/EFI/centos/shimx64.efi nas-user@nas-address:/media/PXE scp /tmp/boot/efi/EFI/centos/grubx64.efi nas-user@nas-address:/media/PXE Add a configuration file named grub.cfg to the TFTP/HTTP root directory. We need initrd and vmlinuz files to load the Operating System until the hard disk and other interfaces are detected. In the grub.cfg take note of linuxefi and initrdefi. Please update it to TFTP/HTTP image directory location.
set timeout=30 menuentry &#39;Install RHEL 8.6 on T7910&#39; { linuxefi images/rhel/8.6/images/pxeboot/vmlinuz inst.stage2=http://192.168.100.160:8080/images/rhel/8.6 quiet initrdefi images/rhel/8.6/images/pxeboot/initrd.img } Prepare kickstart file # Next, we will create our kickstart file for an unattended automated installation. I have used the kickstart configuration tool available at https://access.redhat.com/labs/kickstartconfig/ in the Red Hat Customer Portal Labs. This tool will walk you through basic configuration and allows you to download the resulting Kickstart file.
Create a new directory named ks under the root directory of the TFTP/HTTP server to store the kickstart file for the UEFI PXE Boot purpose Copy the downloaded kickstart file into the previously created location scp kickstart.cfg nas-user@nas-address:/media/PXE/ks Update the grub.cfg to use the kickstart file during installation. For the inst.ks= boot option, specify kickstart location The final grub.cfg will look like this:
set timeout=30 menuentry &#39;Install RHEL 8.6 on T7910&#39; { linuxefi images/rhel/8.6/images/pxeboot/vmlinuz inst.ks=http://192.168.100.160:8080/ks/rhel8-t7910.cfg inst.stage2=http://192.168.100.160:8080/images/rhel/8.6 quiet initrdefi images/rhel/8.6/images/pxeboot/initrd.img } Perform PXE boot # We are ready to perform UEFI PXE Boot. The shortcut button to boot over the network may vary for different hardware, F12 on most common hardware.
If your UEFI PXE boot server configuration is accurate, then the TFTP files should acquire successfully.
`}).add({id:3,href:"/blog/helm-dict-function/",title:"Helm alternative for multiple If-Else conditions",description:"Helm function alternative for multiple if-else conditions",content:`Recently I was in a situation where I ended up writing multiple if-else statements in the helm template. As a result, the code block wasn&rsquo;t clean, and I began to explore alternative ways to achieve the same behavior concisely.
Helm dict function perfectly fits my use case.
Before
{{- if eq .Values.imageType &#34;java&#34; }} name: openjdk:latest {{- else if eq .Values.imageType &#34;nodejs&#34; }} name: nodejs:latest {{- else if eq .Values.imageType &#34;golang&#34; }} name: golang:latest {{- else if eq .Values.imageType &#34;python&#34; }} name: python:latest {{- end }} After
{{- $imageTypes := dict }} {{- $_ := set $imageTypes &#34;java&#34; &#34;openjdk:latest&#34; }} {{- $_ := set $imageTypes &#34;nodejs&#34; &#34;nodejs:latest&#34; }} {{- $_ := set $imageTypes &#34;python&#34; &#34;openjdk:latest&#34; }} {{- $_ := set $imageTypes &#34;java&#34; &#34;openjdk:latest&#34; }} ... name: {{ get $imageTypes .Values.imageType }} `}).add({id:4,href:"/blog/tekton-multiarch-buildx/",title:"Build multi-arch docker image using Tekton",description:"multi-arch builds in Tekton using docker buildx",content:"There are three different strategies to build multi-platform images that Buildx and Dockerfiles support:\nUsing the QEMU emulation support in the kernel Building on multiple native nodes using the same builder instance Using a stage in Dockerfile to cross-compile to different architectures I&rsquo;ll focus on the first option, cross-building with emulation using buildx.\ndocker buildx build --platform linux/amd64,linux/arm64 . Setup # Steps might differ from platform to platform, but the pipelines will remain the same. So I&rsquo;ve constructed a basic buildx setup for the TektonCD task.\nPipeline # I am taking the upstream docker-build task and modifying it to my needs\nWhat&rsquo;s modified\nEnable DOCKER_BUILDKIT and DOCKER_CLI_EXPERIMENTAL Install docker-buildx plugin Run the latest docker/binfmt tag to use its qemu parts Docker login (using secret for values) Docker build with docker buildx build --platform Create a secret holding docker username and token kubectl create secret generic docker-token \\ --from-literal=username=&#34;${CONTAINER_REGISTRY_USER}&#34; \\ --from-literal=password=&#34;${CONTAINER_REGISTRY_PASSWORD}&#34; apiVersion: tekton.dev/v1beta1 kind: ClusterTask metadata: name: docker-buildx labels: app.kubernetes.io/version: &#34;0.1&#34; annotations: tekton.dev/pipelines.minVersion: &#34;0.12.1&#34; tekton.dev/tags: docker, build-image, push-image, dind, buildx, multi-arch tekton.dev/displayName: docker-buildx spec: description: &gt;- This task will build and push an image using docker buildx. The task will build an out image out of a Dockerfile. This image will be pushed to an image registry. The image will be built and pushed using a dind sidecar over TCP+TLS. params: - name: image description: Reference of the image docker will produce. - name: builder_image description: The location of the docker builder image. default: docker.io/library/docker:19.03.14 - name: dockerfile description: Path to the Dockerfile to build. default: ./Dockerfile - name: context description: Path to the directory to use as context. default: . - name: build_extra_args description: Extra parameters passed for the build command when building images. default: &#34;--platform linux/amd64,linux/arm/v7 --no-cache&#34; - name: push_extra_args description: Extra parameters passed for the push command when pushing images. default: &#34;--push&#34; - name: insecure_registry description: Allows the user to push to an insecure registry that has been specified default: &#34;&#34; - name: docker-token-secret type: string description: name of the secret holding the docker-token default: docker-token workspaces: - name: source results: - name: IMAGE_DIGEST description: Digest of the image just built. steps: - name: buildx-build-push image: $(params.builder_image) env: # Connect to the sidecar over TCP, with TLS. - name: DOCKER_HOST value: tcp://localhost:2376 # Verify TLS. - name: DOCKER_TLS_VERIFY value: &#39;1&#39; # Use the certs generated by the sidecar daemon. - name: DOCKER_CERT_PATH value: /certs/client - name: DOCKERHUB_USER valueFrom: secretKeyRef: name: $(params.docker-token-secret) key: username - name: DOCKERHUB_PASS valueFrom: secretKeyRef: name: $(params.docker-token-secret) key: password workingDir: $(workspaces.source.path) script: | # install depends apk add curl jq # enable experimental buildx features export DOCKER_BUILDKIT=1 export DOCKER_CLI_EXPERIMENTAL=enabled # Download latest buildx bin from github mkdir -p ~/.docker/cli-plugins/ BUILDX_LATEST_BIN_URI=$(curl -s -L https://github.com/docker/buildx/releases/latest | grep &#39;linux-amd64&#39; | grep &#39;href&#39; | sed &#39;s/.*href=&#34;/https:\\/\\/github.com/g; s/amd64&#34;.*/amd64/g&#39;) curl -s -L ${BUILDX_LATEST_BIN_URI} -o ~/.docker/cli-plugins/docker-buildx chmod a+x ~/.docker/cli-plugins/docker-buildx # Get and run the latest docker/binfmt tag to use its qemu parts BINFMT_IMAGE_TAG=$(curl -s https://registry.hub.docker.com/v2/repositories/docker/binfmt/tags | jq &#39;.results | sort_by(.last_updated)[-1].name&#39; -r) docker run --rm --privileged docker/binfmt:${BINFMT_IMAGE_TAG} docker context create tls-environment # create the multibuilder docker buildx create --name multibuilder --use tls-environment docker buildx use multibuilder # login to a registry echo ${DOCKERHUB_PASS} | docker login -u ${DOCKERHUB_USER} --password-stdin # build the containers and push them to the registry then display the images docker buildx build $(params.build_extra_args) \\ -f $(params.dockerfile) -t $(params.image) $(params.context) $(params.push_extra_args) volumeMounts: - mountPath: /certs/client name: dind-certs sidecars: - image: docker:19.03.14-dind name: server args: - --storage-driver=vfs - --userland-proxy=false - --debug securityContext: privileged: true env: # Write generated certs to the path shared with the client. - name: DOCKER_TLS_CERTDIR value: /certs volumeMounts: - mountPath: /certs/client name: dind-certs # Wait for the dind daemon to generate the certs it will share with the # client. readinessProbe: periodSeconds: 1 exec: command: [&#39;ls&#39;, &#39;/certs/client/ca.pem&#39;] volumes: - name: dind-certs emptyDir: {} Make sure you have a base image that supports multi-arch like alpine or one of these base images\n"}).add({id:5,href:"/blog/tekton-triggers-cel-interception/",title:"Tekton triggers and Interceptors",description:`Tekton Triggers work by having EventListeners receive incoming webhook notifications, processing them using an Interceptor, and creating Kubernetes resources from templates if the interceptor allows it, with the extraction of fields from the body of the webhook
CEL Interceptors can filter or modify incoming events. For example, you can truncate the commit id from the webhook body.
apiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: shared-listener namespace: default spec: serviceAccountName: build-bot triggers: - name: shared-pipeline-trigger interceptors: - cel: overlays: - key: intercepted.`,content:`Tekton Triggers work by having EventListeners receive incoming webhook notifications, processing them using an Interceptor, and creating Kubernetes resources from templates if the interceptor allows it, with the extraction of fields from the body of the webhook
CEL Interceptors can filter or modify incoming events. For example, you can truncate the commit id from the webhook body.
apiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: shared-listener namespace: default spec: serviceAccountName: build-bot triggers: - name: shared-pipeline-trigger interceptors: - cel: overlays: - key: intercepted.commit_id_short expression: &#34;body.head_commit.id.truncate(7)&#34; bindings: - ref: pipeline-binding template: ref: pipeline-template The applied overlay uses an extension body in the binding. As shown in below example as $(extensions.&lt;overlay_key&gt;)
apiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: pipeline-binding namespace: default spec: params: - name: git-repo-name value: $(body.repository.name) - name: git-repo-url value: $(body.repository.url) - name: image-tag value: $(extensions.intercepted.commit_id_short) `}).add({id:6,href:"/blog/tekton-argocd-gitops/",title:"GitOps with Tekton and ArgoCD",description:"build and release pipeline with Tekton, ArgoCD using GitOps principals.",content:`In this post, I will:
Use Tekton to build and publish an image to the docker registry Trigger a Tekton pipeline from GitHub Use ArgoCD to deploy an application Getting Started # Pre-requisites:
An OpenShift 4 cluster with ArgoCD and OpenShift Pipelines installed. If not, you follow instructions to OpenShift Pipelines and ArgoCD Operator Basic understanding of ArgoCD and Tekton concepts Create an OpenShift project called node-web-project
oc new-project node-web-project Using Private Registries # Create a docker secret with registry authentication details
oc create secret docker-registry container-registry-secret \\ --docker-server=$CONTAINER_REGISTRY_SERVER \\ --docker-username=$CONTAINER_REGISTRY_USER \\ --docker-password=$CONTAINER_REGISTRY_PASSWORD -n node-web-project Create a service account named build-bot
oc create sa -n node-web-project build-bot serviceaccount/build-bot created Add docker secret container-registry-secret to newly created service account build-bot
oc patch serviceaccount build-bot \\ -p &#39;{&#34;secrets&#34;: [{&#34;name&#34;: &#34;container-registry-secret&#34;}]}&#39; serviceaccount/build-bot patched Verify if the service account has the secret added:
oc get sa -n node-web-project build-bot -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: &#34;2021-01-12T03:34:32Z&#34; name: build-bot namespace: node-web-project resourceVersion: &#34;53879&#34; selfLink: /api/v1/namespaces/node-web-project/serviceaccounts/build-bot uid: 628067fd-91d1-4cdd-b6a6-88b4f7280ff0 secrets: - name: container-registry-secret - name: build-bot-token-8nl2v Create Pipeline # A Pipeline is a collection of tasks you want to run as part of your workflow. Each Task in a Pipeline gets executed in a pod and runs in parallel by default. However, you can specify the order by using runAfter.
The below pipeline consists of three tasks.
Cloning source code Build and push images using the Buildah cluster task Synchronize Argo deployment apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: node-web-app-pipeline namespace: node-web-project spec: workspaces: - name: shared-workspace params: - name: node-web-app-source - name: node-web-app-image tasks: - name: fetch-repository taskRef: kind: ClusterTask name: git-clone workspaces: - name: output workspace: shared-workspace params: - name: url value: $(params.node-web-app-source) - name: build-and-publish-image params: - name: IMAGE value: $(params.node-web-app-image) - name: TLSVERIFY value: &#39;false&#39; taskRef: kind: ClusterTask name: buildah runAfter: - fetch-repository workspaces: - name: source workspace: shared-workspace - name: sync-application taskRef: name: argocd-task-sync-and-wait params: - name: application-name value: node-web-app - name: flags value: --insecure --grpc-web - name: argocd-version value: v1.7.11 runAfter: - build-and-publish-image Triggers # Now that the pipeline is ready, the next step is to set up a GitHub webhook to trigger the pipeline. But, first, we need to create the following resources:
A TriggerTemplate act as a blueprint for pipeline resources. We can also use it to define parameters that can be substituted anywhere within the resource template(s).
apiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerTemplate metadata: name: pipeline-template namespace: node-web-project spec: params: - name: git-repo-url - name: git-repo-name - name: git-revision - name: image-name resourcetemplates: - apiVersion: tekton.dev/v1beta1 kind: PipelineRun metadata: generateName: $(tt.params.git-repo-name)-pipeline-run- namespace: node-web-project spec: params: - name: source value: $(tt.params.git-repo-url) - name: image value: $(tt.params.image-name) pipelineRef: name: node-web-app-pipeline serviceAccountName: build-bot timeout: 1h0m0s workspaces: - name: shared-workspace persistentVolumeClaim: claimName: tekton-workspace-pvc A TriggerBinding binds the incoming event data to the template (i.e., git URL, repo name, revision, etc&hellip;.)
apiVersion: triggers.tekton.dev/v1alpha1 kind: TriggerBinding metadata: name: pipeline-binding namespace: node-web-project spec: params: - name: git-repo-name value: $(body.repository.name) - name: git-repo-url value: $(body.repository.url) - name: git-revision value: $(body.head_commit.id) - name: image-name value: docker.io/vikaspogu/$(body.repository.name):$(body.head_commit.id) An EventListener that will create a pod application bringing together a binding and a template
apiVersion: triggers.tekton.dev/v1alpha1 kind: EventListener metadata: name: el-node-web-app namespace: default spec: serviceAccountName: build-bot triggers: - name: node-web-app-trigger bindings: - ref: pipeline-binding template: ref: pipeline-template An OpenShift Route to expose the Event Listener
$ oc expose svc el-node-web-app $ echo &#34;URL: $(oc get route el-node-web-app --template=&#39;http://{{.spec.host}}&#39;)&#34; http://el-node-web-app-node-web-project.apps.cluster-7d51.sandbox659.opentlc.com You can learn about Tekton Triggers and OpenShift Pipelines
Create ArgoCD App for Web App Resources # Create an ArgoCD application via the GUI or command line
Project: default cluster: (URL Of your OpenShift Cluster) namespace should be the name of your OpenShift Project repo URL: git repo Target Revision: Head PATH: deployment AutoSync Disabled Configure Webhooks # GitHub webHook for Tekton event listener to start a Tekton build on git push.
Make a code change and commit, look at the build # Push an empty commit to the repo
git commit -m &#34;empty-commit&#34; --allow-empty &amp;&amp; git push In OpenShift Console, you should see a pipeline run
Once the pipeline is finished. Use the OpenShift route to verify the app
Workspace PVC # It would be best if you had a ReadWriteMany PVC to use with multiple pipelines.
apiVersion: v1 kind: PersistentVolumeClaim metadata: name: tekton-workspace-pvc namespace: default spec: accessModes: - ReadWriteMany resources: requests: storage: 2Gi `}).add({id:7,href:"/blog/docker-buildx-setup/",title:"Setting up docker buildx on Linux",description:"Docker buildx setup on linux",content:` Docker buildx on Linux # Installation instructions for Ubuntu
To execute docker commands without sudo. We need to add the username to the docker group.
Find the current username by typing who Add username to docker group Reboot system $ who vikaspogu :0 2020-12-05 10:10 (:0) $ sudo usermod -aG docker vikaspogu $ sudo reboot Enable buildx # Create a new config.json file under the ~.docker folder and enable the experimental feature
$ mkdir ~/.docker $ vim ~/.docker/config.json { &#34;experimental&#34;: &#34;enabled&#34; } Now you should be able to do multi-arch builds
docker buildx build -t vikaspogu/test:v0.0.1 . --push --platform linux/arm64 If you encounter the below error when trying to build the image using buildx. It would help if you created new build instance
failed to solve: rpc error: code = Unknown desc = failed to solve with frontend dockerfile.v0: failed to load LLB: runtime execution on platform linux/arm64 not supported Create build instance # Follow the below steps to create a new build instance for arm64.
List existing buildx platforms Create a new build instance named arm64 Inspect build instance to verify arm64 platform exists Set builder instance Build multi-platform image $ docker buildx ls NAME/NODE DRIVER/ENDPOINT STATUS PLATFORMS default docker default default running linux/amd64, linux/386 # create the builder $ docker buildx create --name amr64 --platform linux/amd64,linux/arm64 # start the buildx $ docker buildx inspect arm64 --bootstrap Name: arm64 Driver: docker-container Nodes: Name: arm640 Endpoint: unix:///var/run/docker.sock Status: running Platforms: linux/amd64*, linux/arm64*, linux/riscv64, linux/ppc64le, linux/386, linux/arm/v7, linux/arm/v6, linux/s390x # set current builder instance $ docker buildx use amr64 # build the multi images $ docker buildx build -t vikaspogu/test:v0.0.1 . --push --platform linux/arm64 If you encounter /bin/sh: Invalid ELF image for this architecture error. Run the following docker image and then build
docker run --rm --privileged multiarch/qemu-user-static --reset -p yes `}).add({id:8,href:"/blog/boot-from-usb-through-grub-menu/",title:"Boot from USB through grub menu",description:"Boot OS from grub menu",content:`First, make sure you have secure boot disabled from the firmware settings. Once you are in the grub command line, type ls to list all partitions
grub&gt;ls (hd0) (hd0,gpt1) (hd1) (hd1,gpt8) (cd0)) Type ls (cd0) to get the UUID of the device.
grub&gt;ls (hd0,gpt1) Partition hd0,gpt1: Filesystem type fat - Label \`CES_X64FREV\`, UUID 4099-DBD9 Partition start-512 Sectors... Note the UUID of your USB drive, shown in the above command
Type the following commands
insmod part_gpt insmod fat insmod search_fs_uuid insmod chain search --fs-uuid --set=root 409-DBD9 Replace the UUID of your device
Now we select the EFI file to boot from
chainloader /efi/boot/bootx64.efi boot That&rsquo;s it; that should boot the USB drive.
`}).add({id:9,href:"/blog/servicemesh-jwt-auth-authz-keycloack/",title:"End User Auth and Authz with OpenShift Service Mesh and Keycloak",description:"End User Authentication and Authorization with OpenShift Service Mesh and Keycloak",content:`In this article, I will share the setup for enabling Authentication and Authorization in OpenShift Service Mesh with Keycloak.
Installing OpenShift Service Mesh # Follow the Installing Red Hat OpenShift Service Mesh guide for setup
Enable the following configuration in your ServiceMeshControlPlane resource
Strict mTLS across the mesh Automatic istio route creation apiVersion: maistra.io/v2 kind: ServiceMeshControlPlane spec: version: v1.1 security: controlPlane: mtls: true gateways: OpenShiftRoute: enabled: true Keycloak # Keycloak is an open-source identity and access management application that uses open protocols and is easily integrated with other providers. It is the open-source project base of Red Hat Single Sign-on
Deploying Red Hat Single Sign-on # The easiest way to deploy SSO is from the operator hub.
Follow the Keycloak identity provider article for adding a new security realm, client, role, user
Deploying Bookinfo example application # Create a new namespace
oc new-project bookinfo Edit the default Service Mesh Member Roll YAML and add bookinfo to the member&rsquo;s list
apiVersion: maistra.io/v1 kind: ServiceMeshMemberRoll metadata: name: default spec: members: - bookinfo From the CLI, deploy the Bookinfo application in the bookinfo project by applying the bookinfo.yaml file
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/platform/kube/bookinfo.yaml Create the ingress gateway by applying the bookinfo-gateway.yaml file
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/bookinfo-gateway.yaml Set the value for the GATEWAY_URL parameter
oc get routes -n istio-system | grep bookinfo export GATEWAY_URL=$(oc -n istio-system get route bookinfo-gateway-pl2rw -o jsonpath=&#39;{.spec.host}&#39;) Adding default destination rules
I have enabled global mutual TLS in the control plane, so I\u2019ll deploy the destination rule with all mtls
oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-2.0/samples/bookinfo/networking/destination-rule-all-mtls.yaml Verifying the Bookinfo installation
Run the following command to confirm that the Bookinfo application is up and running
curl -o /dev/null -s -w &#34;%{http_code}\\n&#34; http://$GATEWAY_URL/productpage Authentication # Enabling User-End Authentication # Now it is time to enable end-user authentication.
The first thing you need to do is validate that it is possible to communicate between all services without authentication.
curl -k -o /dev/null -w &#34;%{http_code}&#34; http://$GATEWAY_URL/productpage 200 You can create the end-user authentication policy.
cat &lt;&lt;EOF | oc apply -n bookinfo -f - apiVersion: authentication.istio.io/v1alpha1 kind: Policy metadata: name: &#34;productpage-jwt&#34; namespace: &#34;bookinfo&#34; spec: targets: - name: productpage peers: - mtls: {} origins: - jwt: issuer: &#34;https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio&#34; jwksUri: &#34;https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio/protocol/openid-connect/certs&#34; audiences: - customer triggerRules: - excludedPaths: - prefix: /healthz principalBinding: USE_ORIGIN EOF NOTE: If you see Origin authentication failed. after passing the correct access token and URL. Verify your istio-pilot pods for any x509: certificate signed by unknown authority; if that is the case, follow this workaround
Then let\u2019s rerun the curl.
curl -k http://$GATEWAY_URL/productpage And you will see something like
Origin authentication failed. Set the value for the TOKEN parameter from keyclock
export TOKEN=$(curl -sk --data &#34;username=demo&amp;password=demo&amp;grant_type=password&amp;client_id=istio&#34; https://keycloak-sso.apps.amp01.lab.amp.aapaws/auth/realms/istio/protocol/openid-connect/token | jq &#34;.access_token&#34;) Then let\u2019s rerun the curl, this time with the token.
curl -k -o /dev/null -w &#34;%{http_code}&#34; -H &#34;Authorization: Bearer $TOKEN&#34; http://$GATEWAY_URL/productpage 200 Authorization # Create a deny-all policy in the namespace. The policy doesn\u2019t have a selector field, which applies to every workload in the namespace. The spec: field in the policy has the empty value {}, this means that no traffic is permitted, effectively denying all requests
$ cat &lt;&lt;EOF | oc apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: deny-all spec: {} EOF Once the policy takes effect, verify that mesh rejected the curl connection to the workload.
$ curl -k -H &#34;Authorization: Bearer $TOKEN&#34; http://$GATEWAY_URL/productpage RBAC: access denied To give read access to the product page workload, create the policy that applies to the workload with label app: product page and allows users with roles customer to access it with all method.
$ cat &lt;&lt;EOF | oc apply -n bookinfo -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: &#34;productpage-authz&#34; namespace: &#34;bookinfo&#34; spec: selector: matchLabels: app: productpage rules: - to: - operation: methods: [&#34;*&#34;] when: - key: request.auth.claims[roles] values: [&#34;customer&#34;] EOF Wait for the newly defined policy to take effect.
After the policy takes effect, verify the connection to the httpbin workload succeeds.
curl -k -o /dev/null -w &#34;%{http_code}&#34; -H &#34;Authorization: Bearer $TOKEN&#34; http://$GATEWAY_URL/productpage 200 However, you can see the following errors on the page.
Error fetching product details Error fetching product reviews on the page Run the following command to create the details-viewer policy to allow the productpage workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-productpage service account, to access the details workload via GET methods.
oc apply -f - &lt;&lt;EOF apiVersion: &#34;security.istio.io/v1beta1&#34; kind: &#34;AuthorizationPolicy&#34; metadata: name: &#34;details-viewer&#34; namespace: bookinfo spec: selector: matchLabels: app: details rules: - from: - source: principals: [&#34;cluster.local/ns/bookinfo/sa/bookinfo-productpage&#34;] to: - operation: methods: [&#34;GET&#34;] EOF Run the following command to create a policy reviews-viewer to allow the productpage workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-productpage service account, to access the reviews workload through GET methods
oc apply -f - &lt;&lt;EOF apiVersion: &#34;security.istio.io/v1beta1&#34; kind: &#34;AuthorizationPolicy&#34; metadata: name: &#34;reviews-viewer&#34; namespace: bookinfo spec: selector: matchLabels: app: reviews rules: - from: - source: principals: [&#34;cluster.local/ns/bookinfo/sa/bookinfo-productpage&#34;] to: - operation: methods: [&#34;GET&#34;] EOF Point your browser at the Bookinfo productpage (http://$GATEWAY_URL/productpage). Now, you should see the \u201CBookinfo Sample\u201D page with \u201CBook Details\u201D on the lower left part and \u201CBook Reviews\u201D on the lower right part.
However, in the \u201CBook Reviews\u201D section, you&rsquo;ll see an error Rating service is currently unavailable. This error is because the reviews workload doesn\u2019t permit access to the ratings workload. To fix this issue, you need to grant the reviews workload access to the ratings workload. Next, configure a policy to grant access to the reviews workload.
Run the following command to create the ratings-viewer policy to allow the reviews workload, which issues requests using the cluster.local/ns/bookinfo/sa/bookinfo-reviews service account, to access the ratings workload through GET methods
oc apply -f - &lt;&lt;EOF apiVersion: &#34;security.istio.io/v1beta1&#34; kind: &#34;AuthorizationPolicy&#34; metadata: name: &#34;ratings-viewer&#34; namespace: bookinfo spec: selector: matchLabels: app: ratings rules: - from: - source: principals: [&#34;cluster.local/ns/bookinfo/sa/bookinfo-reviews&#34;] to: - operation: methods: [&#34;GET&#34;] EOF Point your browser at the Bookinfo productpage (http://$GATEWAY_URL/productpage). You should see the \u201Cblack\u201D and \u201Cred\u201D ratings in the \u201CBook Reviews\u201D section.
`}).add({id:10,href:"/blog/loki-rsyslog-k3s/",title:"Syslog with Loki Promtail",description:"Setup rsyslog in k3s with promtail chart",content:`Enable Syslog, do this on each host, and replace target IP (and maybe port) with your Syslog externalIP that is in helm values for promtail
promtail: serviceMonitor: enabled: true extraScrapeConfigs: - job_name: syslog syslog: listen_address: 0.0.0.0:1514 label_structured_data: yes labels: job: &#34;syslog&#34; relabel_configs: - source_labels: [&#34;__syslog_connection_ip_address&#34;] target_label: &#34;ip_address&#34; - source_labels: [&#34;__syslog_message_severity&#34;] target_label: &#34;severity&#34; - source_labels: [&#34;__syslog_message_facility&#34;] target_label: &#34;facility&#34; - source_labels: [&#34;__syslog_message_hostname&#34;] target_label: &#34;host&#34; syslogService: enabled: true type: LoadBalancer port: 1514 externalIPs: - 192.168.42.155 Create file /etc/rsyslog.d/50-promtail.conf with the following content:
module(load=&#34;omprog&#34;) module(load=&#34;mmutf8fix&#34;) action(type=&#34;mmutf8fix&#34; replacementChar=&#34;?&#34;) action(type=&#34;omfwd&#34; protocol=&#34;tcp&#34; target=&#34;192.168.42.155&#34; port=&#34;1514&#34; Template=&#34;RSYSLOG_SyslogProtocol23Format&#34; TCP_Framing=&#34;octet-counted&#34; KeepAlive=&#34;on&#34;) Restart rsyslog and view status
sudo systemctl restart rsyslog sudo systemctl status rsyslog In Grafana, on the explore tab, you should now be able to view your host&rsquo;s logs, e.g., this query {host=&quot;k3s-master&quot;}.
`}).add({id:11,href:"/blog/helm-join-function/",title:"Helm join strings in a named template",description:"How to join strings in a named template",content:`The key benefit of Helm is that it helps reduce the configuration a user needs to provide to deploy applications to Kubernetes. With Helm, we can have a single chart that can deploy all the microservices.
Unique ServiceAccount # Recently we wanted to create a unique service account for each microservice, so all microservices don&rsquo;t share the same service account using the helm template. In our example, we will append the release name with the service account name.
Using join function # {{ (list .Values.serviceAccount.name (include &#34;openjdk.fullname&#34; .) | join &#34;-&#34;) }} Adding fail condition # {{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include &#34;openjdk.fullname&#34; .) | join &#34;-&#34;) }} {{- else -}} {{- fail &#34;Please enter a name for service account to create.&#34; }} {{- end -}} Full example
{{- define &#34;openjdk.serviceAccountName&#34; -}} {{- if .Values.serviceAccount.create -}} {{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include &#34;openjdk.fullname&#34; .) | join &#34;-&#34;) }} {{- else -}} {{- fail &#34;Please enter a name for service account to create.&#34; }} {{- end -}} {{- else -}} {{- if .Values.serviceAccount.name -}} {{ (list .Values.serviceAccount.name (include &#34;openjdk.fullname&#34; .) | join &#34;-&#34;) }} {{- else -}} &#34;default&#34; {{- end -}} {{- end -}} {{- end -}} Another way to manipulate strings in helpers.tpl is using printf
{{- define &#34;ocp-openjdk.hostname&#34; -}} {{- printf &#34;%s-%s%s&#34; .Release.Name .Release.Namespace (.Values.subdomain | default &#34;.apps.amp01.nonprod&#34; ) -}} {{- end -}} Resources # Sprig functions `}).add({id:12,href:"/blog/adguardhome-kubernetes/",title:"AdGuard on Kubernetes",description:"Deploying AdGuard home on Kubernetes",content:` AdGuard # AdGuard Home is a network-wide software for blocking ads &amp; tracking
Adguard is similar to Pi-Hole with more features. Comparison of Adguard to Pi-Hole
The below configuration has worked for me. I am a big fan of helm charts, and I&rsquo;ll be using AdGuard chart
Configure chart # Pull chart locally
helm repo add billimek https://billimek.com/billimek-charts/ helm fetch billimek/adguard-home Update deployment to use hostNetwork
#templates/deployment.yaml ... spec: hostNetwork: true securityContext: ... Enable configAsCode, update bind_host to Kubernetes host IP in values.yaml
# values.yaml configAsCode: enabled: true config: bind_host: 192.168.0.101 bind_port: 3000 dns: bind_host: 192.168.0.101 Update securityContext to run as a privileged pod, drop all capabilities and add NET_BIND_SERVICE
# values.yaml securityContext: privileged: true capabilities: drop: - ALL add: - NET_BIND_SERVICE Add nodeSelector to assign a pod to that node.
# values.yaml nodeSelector: kubernetes.io/hostname: node3 Deploy helm chart
helm install adguard-home Wait for AdGuard pods.
kubectl get pods NAME READY STATUS RESTARTS AGE adguard-adguard-home-67975f7768-v6bg9 1/1 Running 0 43h Configure your devices to use your AdGuard Home # Once we&rsquo;ve established that AdGuard Home has deployed, you can use it on other computers in your network by changing their system DNS settings to use the Kubernetes node&rsquo;s IP address (which is 192.168.0.101 in our case).
`}).add({id:13,href:"/blog/jenkins-seed-job/",title:"Configure Jenkins pipeline job",description:"Configure bitbucket pipeline job using Jenkins config as code",content:`In one of my previous posts, I discussed configuring multibranch pipeline seed jobs using Jenkins configuration as code plugin
Below is an example of configuring a declarative pipeline job from a bitbucket repo, running at midnight every day
- script: &gt; pipelineJob(&#39;sample-job&#39;) { definition { cpsScm { scriptPath &#39;job1/Jenkinsfile&#39; ## If the Jenkins job is in a nested folder scm { git { remote { url &#39;https://github.com/Vikaspogu/sample-repo&#39; credentials &#39;sample-creds&#39; } branch &#39;*/master&#39; extensions {} } } triggers { cron(&#39;@midnight&#39;) } } } } `}).add({id:14,href:"/blog/opa-gatekeeper-openshift/",title:"OPA Gatekeeper on OpenShift",description:"OpenShift policy management with OPA gatekeeper.",content:`Every organization has policies. Some are essential to meet governance and legal requirements. Others help ensure adherence to best practices and institutional conventions. Attempting to ensure compliance manually would be error-prone and frustrating.
OPA allows users to a specific policy as code using OPA&rsquo;s policy language Rego
In this post, I&rsquo;ll share my experience deploying OPA Gatekeeper on OpenShift and creating a few policies for demonstrations. This post is not an introduction to OPA refer to for an intro
Gatekeeper introduces native Kubernetes CRDs for instantiating policies.
Installation # For installation, make sure you have cluster-admin permissions.
Let&rsquo;s start by adding the admission.gatekeeper.sh/ignore label to non-user namespaces so that all the resources in the labeled project are exempted from the admission webhook.
oc login --token=l4xjpLh0e722B2_i7iWAbPsUNOb6vPDaAXnqhH563oU --server=https://api.cluster-1d4d.sandbox702.opentlc.com:6443 for namespace in $(oc get namespaces -o jsonpath=&#39;{.items[*].metadata.name}&#39; | xargs); do if [[ &#34;\${namespace}&#34; =~ OpenShift.* ]] || [[ &#34;\${namespace}&#34; =~ kube.* ]] || [[ &#34;\${namespace}&#34; =~ default ]]; then oc patch namespace/\${namespace} -p=&#39;{&#34;metadata&#34;:{&#34;labels&#34;:{&#34;admission.gatekeeper.sh/ignore&#34;:&#34;true&#34;}}}&#39; else # Probably a user project, so leave it alone echo &#34;Skipping: \${namespace}&#34; fi done Deploying a Release using Prebuilt Image # Deploy Gatekeeper with a prebuilt image
oc apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml Remove securityContext and annotations from the deployments.
oc patch Deployment/gatekeeper-audit --type json -p=&#39;[{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/spec/template/metadata/annotations&#34;}]&#39; -n gatekeeper-system oc patch Deployment/gatekeeper-controller-manager --type json -p=&#39;[{&#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/spec/template/metadata/annotations&#34;}]&#39; -n gatekeeper-system oc patch Deployment/gatekeeper-audit --type json --patch &#39;[{ &#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/securityContext&#34; }]&#39; -n gatekeeper-system oc patch Deployment/gatekeeper-controller-manager --type json --patch &#39;[{ &#34;op&#34;: &#34;remove&#34;, &#34;path&#34;: &#34;/spec/template/spec/containers/0/securityContext&#34; }]&#39; -n gatekeeper-system Wait for Gatekeeper to be ready.
oc get pods -n gatekeeper-system NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c84869dbf-r5p87 1/1 Running 0 2m34s gatekeeper-controller-manager-ff58b6688-9tbxx 1/1 Running 0 3m23s gatekeeper-controller-manager-ff58b6688-njfnd 1/1 Running 0 2m54s gatekeeper-controller-manager-ff58b6688-vlrsl 1/1 Running 0 3m11s Gatekeeper uses the OPA Constraint Framework to describe and enforce policy
Defining constraints # Users can define constraints by creating a CRD (CustomResourceDefinition) with the template of the constraint they want. For example, let\u2019s look at a template that only enforces users must create secure routes on the cluster.
apiVersion: templates. Gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8sallowedroutes spec: crd: spec: names: kind: K8sAllowedRoutes targets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedroutes violation[{&#34;msg&#34;: msg}] { not input.review.object.spec.tls msg := sprintf(&#34;&#39;%v&#39; route must be a secured route. non secured routes are not permitted&#34;, [input.review.object.metadata.name]) } It will get the route and check if the tls object is specified. If this is not true, the violation block continues, and the violation is triggered with the corresponding message.
Enforcing constraints # Define a template using earlier constraints to enforce.
apiVersion: constraints. Gatekeeper.sh/v1beta1 kind: K8sAllowedRoutes metadata: name: secure-route spec: match: kinds: - apiGroups: [&#34;route.OpenShift.io&#34;] kinds: [&#34;Route&#34;] The above policy uses the CRD \u201CK8sAllowedRoutes\u201D, which we had already defined. Enforcement takes place by matching the API group.
Some constraints are impossible to write without access to more states than the object under test. For example, it is impossible to know if a route&rsquo;s hostname is unique among all routes unless a rule has access to all other routes. To make such laws possible, we enable syncing of data into OPA.
apiVersion: config. Gatekeeper.sh/v1alpha1 kind: Config metadata: name: config namespace: &#34;gatekeeper-system&#34; spec: sync: syncOnly: - group: &#34;&#34; version: &#34;v1&#34; kind: &#34;Namespace&#34; - group: &#34;route.OpenShift.io&#34; version: &#34;v1&#34; kind: &#34;Route&#34; Let&rsquo;s create another policy that prevents conflicting routes from being created.
apiVersion: templates. Gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8suniqueroutehost spec: crd: spec: names: kind: K8sUniqueRouteHost targets: - target: admission.k8s.gatekeeper.sh rego: | package k8suniqueroutehost identical(obj, review) { obj.metadata.namespace == review.object.metadata.namespace obj.metadata.name == review.object.metadata.name } violation[{&#34;msg&#34;: msg}] { input.review.kind.kind == &#34;Route&#34; re_match(&#34;^(route.OpenShift.io)$&#34;, input.review.kind.group) host := input.review.object.spec.host other := data.inventory.namespace[ns][otherapiversion][&#34;Route&#34;][name] re_match(&#34;^(route.OpenShift.io)/.+$&#34;, otherapiversion) other.spec.host == host not identical(other, input.review) msg := sprintf(&#34;Route host conflicts with an existing route &lt;%v&gt;&#34;, [host]) } apiVersion: constraints. Gatekeeper.sh/v1beta1 kind: K8sUniqueRouteHost metadata: name: unique-route-host spec: match: kinds: - apiGroups: [&#34;route.OpenShift.io&#34;] kinds: [&#34;Route&#34;] Resources # Open Policy Agent OPA Gatekeeper Test framework `}).add({id:15,href:"/blog/jenkins-date-difference/",title:"Jenkins pipeline date helper functions",description:"Jenkins pipeline function to get days between dates",content:`The below snippet shows how to calculate days between two dates in the Jenkins pipeline. @NonCPS annotation is functional when you have methods that use objects that aren&rsquo;t serializable.
import java.text.SimpleDateFormat stages { stage(&#34;Calculate days&#34;) { steps { script { def daysRemaining = getRemainingDays(&#34;2020-06-25&#34;) } } } } } @NonCPS def getRemainingDays(previousDate){ def currentDate = new Date() String currentTimeFormat= currentDate.format(&#34;yyyy-MM-dd&#34;) def oldDate = new SimpleDateFormat(&#34;yyyy-MM-dd&#34;).parse(previousDate) return currentTimeFormat-oldDate } Calculate if the date is greater than 14 days
import java.time.LocalDate import java.time.format.DateTimeFormatter stages { stage(&#34;Check if date is older than 14 days&#34;) { steps { script { if (checkIfOtherThan(&#34;2020-06-25&#34;)){ println &#34;Date is greater than 14 days&#34; } } } } } } @NonCPS def checkIfOlderThan(previousDate){ def dateFormat = DateTimeFormatter.ofPattern(&#34;yyyy-MM-dd&#34;) def currentDate = LocalDate.now().format(dateFormat); def projectpreviousDate = LocalDate.parse(previousDate, dateFormat) if (LocalDate.parse(currentDate, dateFormat).minusDays(14) &gt; projectpreviousDate) { return true; } return false; } `}).add({id:16,href:"/blog/jenkins-config-as-code-openshift/",title:"OpenShift Jenkins configuration via JCasC plugin",description:"Configuring OpenShift jenkins via jenkins configuration as code plugin",content:`Deploying Jenkins on Kubernetes provides significant benefits over a standard VM-based deployment. For example, we are gaining the ability to have project-specific Jenkins slaves (agents) on demand instead of having a pool of VMs idle waiting for a job.
In vanilla Kubernetes, we can deploy Jenkins using Helm, and In OpenShift, we can deploy Jenkins via the developer&rsquo;s catalog.
As everyone has experienced, setting up Jenkins is a complex process. Both Jenkins and its plugins require some tuning and Configuration, with dozens of parameters to set within the web UI manage section.
Jenkins Config as code # Configuration as code gives you an opinionated way to configure Jenkins based on yaml files
This post will cover Jenkins configuration code on OpenShift.
Mount Jenkins config as a configmap and load configuration Updated configuration file to automate the creation of credentials script approval signatures shared libraries multibranch pipeline seed jobs Install Plugin # First, let us install configuration-as-code plugins in Jenkins. In OpenShift, you can easily install plugins by adding the INSTALL_PLUGINS environment variable to the deployment config
.... env: - name: INSTALL_PLUGINS value: &#34;configuration-as-code:1.35,configuration-as-code-support:1.18,configuration-as-code-groovy:1.1 Create ConfigMap # Create a configmap from jenkins-config yaml and mount it as a volume at /var/jenkins_config location. Configuration can be now loaded from /var/jenkins_config/jenkins-config.yaml path
oc create configmap jenkins-config --from-file jenkins-config.yaml .... volumeMounts: - mountPath: /var/jenkins_config name: jenkins-config dnsPolicy: ClusterFirst restartPolicy: Always volumes: - name: jenkins-config configMap: name: jenkins-config Mount Secret # Since we can have Jenkins configuration in the git repo, we don&rsquo;t want to hardcode the secrets as it is a security risk. The easiest way to automate credentials in Jenkins configuration is to create an OpenShift secret, and add that secret as an environment variable to the deployment config
oc create secret generic github-ssh --from-file=ssh-privatekey=github-ssh/ oc create secret generic jenkins-credentials --from-literal=OCP_SA=&#34;qwerty26sds99ie9kcsd&#34; --from-literal=GITHUB_PASSWORD=&#34;dummypassword&#34; Mount secrets into the container
.... env: .... - name: OCP_SA valueFrom: secretKeyRef: name: jenkins-credentials key: OCP_SA - name: GITHUB_SSH valueFrom: secretKeyRef: name: github-ssh key: ssh-privatekey - name: GITHUB_PASSWORD valueFrom: secretKeyRef: name: jenkins-credentials key: GITHUB_PASSWORD .... credentials: system: domainCredentials: - credentials: - fileSystemServiceAccountCredential: id: &#34;1a12dfa4-7fc5-47a7-aa17-cc56572a41c7&#34; scope: GLOBAL - basicSSHUserPrivateKey: description: &#34;github ssh credentials&#34; id: &#34;github-ssh&#34; privateKeySource: directEntry: privateKey: \${GITHUB_SSH} scope: GLOBAL username: &#34;github-user&#34; - OpenShiftToken: id: &#34;jenkins-ocp-token&#34; scope: GLOBAL secret: \${OCP_SA} - usernamePassword: description: &#34;github user credentials&#34; id: &#34;github-user&#34; password: \${GITHUB_PASSWORD} scope: GLOBAL username: &#34;github-user&#34; Example Configs # Security script approval # security: scriptApproval: approvedSignatures: - &#34;method java.text.DateFormat parse java.lang.String&#34; Global shared library # unclassified: globalLibraries: libraries: - defaultVersion: &#34;master&#34; name: &#34;shared-pipeline&#34; retriever: modernSCM: scm: git: credentialsId: &#34;github-ssh&#34; id: &#34;shared-pipeline&#34; remote: &#34;git@github.com:vikaspogu/jenkins-shared.git&#34; Multibranch job # Multibranch seed job with periodic polling, traits for branch discovery
jobs: - script: &gt; multibranchPipelineJob(&#34;sample-repo&#34;) { branchSources { branchSource { source { github { //This is a unique identifier; if not set, the pipeline will be //indexed, and jobs will be kicked off every time new config is applied id(&#34;5bb970c2-766b-4588-8cf8-e077bfec23a0&#34;) credentialsId(&#34;github-user&#34;) repoOwner(&#34;vikaspogu&#34;) repository(&#34;sample-repo&#34;) traits { cloneOptionTrait { extension { shallow (false) noTags (false) reference (null) depth(1) honorRefspec (false) timeout (10) } } } } } } } configure { def traits = it / sources / data / &#39;jenkins.branch.BranchSource&#39; / source / traits traits &lt;&lt; &#39;com.cloudbees.jenkins.plugins.bitbucket.BranchDiscoveryTrait&#39; { strategyId(3) // detect all branches -refer the plugin source code for various options } } configure { def traits = it / sources / data / &#39;jenkins.branch.BranchSource&#39; / source / traits traits &lt;&lt; &#39;com.cloudbees.jenkins.plugins.bitbucket.OriginPullRequestDiscoveryTrait&#39; { strategyId(2) } } configure { def traits = it / sources / data / &#39;jenkins.branch.BranchSource&#39; / source / traits traits &lt;&lt; &#39;com.cloudbees.jenkins.plugins.bitbucket.TagDiscoveryTrait&#39; {} } orphanedItemStrategy { discardOldItems { numToKeep(15) } } triggers { periodic(2) } } `}).add({id:17,href:"/blog/pi-garage-k3s/",title:"Raspberry Pi garage opener on k3s cluster",description:"Raspberry Pi Garage door opener on Kubernetes cluster using NodeJS",content:`Many articles are out there which demonstrate how to use a raspberry pi as a DIY garage door opener project. Few are outdated and not deployed using container images. I found a few reasonable solutions on google, but I couldn&rsquo;t run them on the Kubernetes cluster due to older packages or insufficient information. I decided to build my solution from different sources of information I found
What we&rsquo;ll cover in this post
setup nodejs project to simulate a button create a container from nodejs application deploy the container on the Kubernetes cluster Kudos! to the author for this excellent article for showing us how to connect the relay and magnetic switch to raspberry pi for our purpose.
Once finished wiring up all components. Let&rsquo;s start setting up our Nodejs application.
Application setup # Create an npm project
mkdir garage-pi &amp;&amp; cd garage-pi npm init -y We&rsquo;ll be using node-rpio package, which provides access to the Raspberry Pi GPIO interface
Install node-rpio and express packages
npm i rpio express -S Create an express app that starts on port 8080 in server.js file
&#34;use strict&#34;; const express = require(&#34;express&#34;); const rpio = require(&#34;rpio&#34;); const app = express(); const PORT = 8080; app.use(&#34;/assets&#34;, express.static(&#34;assets&#34;)); app.listen(PORT); console.log(&#34;Running on http://localhost:&#34; + PORT); The below code lets you simulate a button press; In our case, PIN is 19. First, we want to output to low and set the pin to high after 1000ms. Please refer to rpio repo for more explanation
const openPin = process.env.OPEN_PIN || 19; const relayPin = process.env.RELAY_PIN || 11; app.get(&#34;/relay&#34;, function (req, res) { // Simulate a button press rpio.write(relayPin, rpio.LOW); setTimeout(function () { rpio.write(relayPin, rpio.HIGH); res.send(&#34;done&#34;); }, 1000); }); To get the state of pin.
function getState() { return { open: !rpio.read(openPin), }; } app.get(&#34;/status&#34;, function (req, res) { res.send(JSON.stringify(getState())); }); Complete server.js file
&#34;use strict&#34;; const express = require(&#34;express&#34;); const rpio = require(&#34;rpio&#34;); const app = express(); const PORT = 8080; const openPin = process.env.OPEN_PIN || 19; const relayPin = process.env.RELAY_PIN || 11; app.use(&#34;/assets&#34;, express.static(&#34;assets&#34;)); function getState() { return { open: !rpio.read(openPin), }; } app.get(&#34;/status&#34;, function (req, res) { res.send(JSON.stringify(getState())); }); app.get(&#34;/relay&#34;, function (req, res) { // Simulate a button press rpio.write(relayPin, rpio.LOW); setTimeout(function () { rpio.write(relayPin, rpio.HIGH); res.send(&#34;done&#34;); }, 1000); }); app.listen(PORT); console.log(&#34;Running on http://localhost:&#34; + PORT); Container image # Create a dockerfile with multi-stage builds using nodejs arm image Install the necessary python package for rpio Build docker image Publish the docker image to your repo in docker hub Create new Kubernetes deployment and service # Fetch node_modules for backend; nothing here except # the node_modules dir ends up in the final image FROM arm32v7/node:12.18-alpine as builder RUN mkdir /app WORKDIR /app ENV PATH /app/node_modules/.bin:$PATH COPY package.json /app/package.json RUN apk add --no-cache make gcc g++ python &amp;&amp; \\ npm install --production --silent &amp;&amp; \\ apk del make gcc g++ python RUN npm install # Add the files to the arm image FROM arm32v7/node:12.18-alpine RUN mkdir /app WORKDIR /app ENV PATH /app/node_modules/.bin:$PATH # Same as earlier, be specific or copy everything ADD package.json /app/package.json ADD package-lock.json /app/package-lock.json ADD . /app COPY --from=builder /app/node_modules /app/node_modules ENV PORT=8080 EXPOSE 8080 CMD [ &#34;npm&#34;, &#34;start&#34; ] Docker buildx feature lets you build arm-based images on mac or windows system
docker buildx build --platform linux/arm64 -t &lt;docker-username&gt;/garage-pi . docker push &lt;docker-username&gt;/garage-pi Create a new deployment named garage-pi that runs the earlier published image.
kind: Deployment apiVersion: apps/v1 metadata: name: garage-pi labels: app.kubernetes.io/name: garage-pi spec: replicas: 1 selector: matchLabels: app.kubernetes.io/instance: garage-pi app.kubernetes.io/name: garage-pi template: metadata: creationTimestamp: null labels: app.kubernetes.io/instance: garage-pi app.kubernetes.io/name: garage-pi spec: volumes: - name: dev-snd hostPath: path: /dev/mem type: &#39;&#39; containers: - name: garage-pi image: &#39;&lt;docker-username&gt;/garage-pi:latest&#39; ##update username here ports: - name: http containerPort: 8080 protocol: TCP resources: {} volumeMounts: - name: dev-snd mountPath: /dev/mem livenessProbe: httpGet: path: / port: http scheme: HTTP readinessProbe: httpGet: path: / port: http scheme: HTTP imagePullPolicy: Always securityContext: privileged: true restartPolicy: Always Create a service for a garage-pi deployment, which serves on port 8080 and connects to the containers on port 8080.
kubectl expose deployment nginx --port=8080 --target-port=8080 That&rsquo;s it; you should be able to hit the endpoint and simulate button click!
Frontend # For frontend of application, we&rsquo;ll use pugjs templating engine
Install pug package
npm i pug -S add the views folder in the root directory and create a new index.pug file in the views folder wire up templating engine with application render index page on root / endpoint integrate button with /relay endpoint, which will open/closed garage door doctype html head meta(charset=&#39;utf-8&#39;) meta(name=&#39;viewport&#39;, content=&#39;width=device-width, initial-scale=1, shrink-to-fit=no&#39;) meta(name=&#39;description&#39;, content=&#39;&#39;) meta(name=&#39;author&#39;, content=&#39;&#39;) title Garage Opener .text-center form.form-signin(method=&#39;POST&#39;, action=&#39;/relay&#39;) h1.h1.mb-2.font-weight-normal(style=&#39;color: #FFFFFF&#39;) Garage Door .text-center .form-signin .text-center #open h1.h2.mb-3.font-weight-normal(style=&#39;color: #CF6679&#39;) The Door is Open &amp;#xF62B; button#mainButton.btn.btn-lg.btn-primary.btn-block(type=&#39;submit&#39;) Close app.set(&#34;views&#34;, path.join(__dirname, &#34;views&#34;)); app.set(&#34;view engine&#34;, &#34;pug&#34;); app.get(&#34;/&#34;, function (req, res) { res.render(&#34;index&#34;, getState()); }); Twilio Integration (Optional) # Install dotenv, twilio and node-schedule packages
npm i dotenv node-schedule twilio -S Create a .env file at the root of the project and add your Twilio auth key, account sid, and phone number
TWILIO_ACCOUNT_SID= TWILIO_AUTH_TOKEN= TWILIO_PHONE_NUMBER= Create a twilio.js file, add the below code
require(&#34;dotenv&#34;).config(); const accountSid = process.env.TWILIO_ACCOUNT_SID; const authToken = process.env.TWILIO_AUTH_TOKEN; const sendSms = (phone, message) =&gt; { const client = require(&#34;twilio&#34;)(accountSid, authToken); client.messages .create({ body: message, from: process.env.TWILIO_PHONE_NUMBER, to: phone, }) .then((message) =&gt; console.log(message.sid)); }; module.exports = sendSms; Schedule a job for every 15mins to check the state of the garage door. If the door is open, we&rsquo;ll send a message
const sendSms = require(&#34;./twilio&#34;); ... ... schedule.scheduleJob(&#34;*/15 * * * *&#34;, function () { var status = JSON.parse(JSON.stringify(getState())); if (status.open) { sendSms(&#34;&lt;YOUR-NUMBER&gt;&#34;, &#34;Garage door is open \u{1F525}&#34;); } }); `}).add({id:18,href:"/blog/nodejs-slack-bot/",title:"Slack bot with Nodejs",description:"Building a slack bot using bolt library in NodeJS",content:`Build your slack bot in a few steps. In this post, we&rsquo;ll navigate the process of creating the bot.
Slack setup # First, create a slack workspace
Give your workspace a name Create a new bot at slack apps
Give your new application a name Choose the workspace you created before installing the bot application Then go to the Features &gt; OAuth &amp; Permissions screen to scroll down to Bot Token Scopes to specify the OAuth scopes, and select app_mentions and chat_write to enable the bot to send messages.
Before jumping into the application setup, copy the signing secret and verification token from the basic information page. We&rsquo;ll be using this later in our NodeJS application.
Application setup # Create an npm project, install @slack/bolt and dotenv packages
mkdir test-bot &amp;&amp; cd test-bot npm init -y npm i dotenv @slack/bolt -S Add start command to scripts if necessary.
... &#34;scripts&#34;: { &#34;start&#34;: &#34;node index.js&#34; } Create a .env file and add SLACK_SIGNING_SECRET, SLACK_BOT_TOKEN
Note: Don&rsquo;t commit this file to any repo
SLACK_BOT_TOKEN= #token goes here SLACK_SIGNING_SECRET= #signing secret goes here In your index.js file, require the Bolt package, and initialize an app with credentials.
require(&#34;dotenv&#34;).config(); const { App } = require(&#34;@slack/bolt&#34;); const bot = new App({ signingSecret: process.env.SLACK_SIGNING_SECRET, token: process.env.SLACK_BOT_TOKEN, endpoints: &#34;/slack/events&#34;, }); (async () =&gt; { // Start the app await bot.start(process.env.PORT || 3000); console.log(&#34;\u26A1\uFE0F Bolt app is running!&#34;); })(); Deploy the application to a live server like ngrok.
Event Setup # We&rsquo;ll need to subscribe to events so that when a Slack event happens (like a user mentions an app), the app server will receive an event payload.
Go to Event Subscriptions from the left-hand menu, and turn the toggle switch on to enable events
Enter your Request URL
Subscribe to app_mention event
Install app to workspace
You should see the bot in your workspace now!
Handling Events # To listen to any Events API events from Slack, use the event() method. This method allows your app to take action on Slack events. In this scenario, it&rsquo;s triggered when a user mentions the app.
bot.event(&#34;app_mention&#34;, async ({ context, event }) =&gt; { try { const command = event.text; let reply; if (command.includes(&#34;Hi&#34;)) { reply = \`Hi &lt;@\${event.user}&gt;, you mentioned me\`; } else { reply = &#34;How can I help you?&#34;; } await bot.client.chat.postMessage({ token: context.botToken, channel: event.channel, text: \`\${reply}\`, }); } catch (e) { console.log(\`error responding \${e}\`); } }); Okay, let&rsquo;s try the app!
Add the app to a channel and mention the app. You should see a response from the bot!
Troubleshooting # Reinstall the app if you don&rsquo;t see any responses from the bot
`}).add({id:19,href:"/blog/pi-hole-kubernetes/",title:"Pi Hole on k3s cluster",description:"Deploying Pi-Hole on k3s cluster.",content:`Pi-hole is a fantastic tool that blocks DNS requests to ad servers. That means you can surf the web without looking at ads on every page.
Pi-Hole in Kubernetes # We are going to deploy a modified version of this pihole helm chart
Let&rsquo;s start by cloning the repo.
git clone https://github.com/ChrisPhillips-cminion/pihole-helm.git cd pihole-helm We now need to make a few updates to the chart.
Update ServerIP with container host IP Update image to v5.1.1 tag Add WEB_PASSWORD, and TZ environment variables if needed Update values.yaml accordingly spec: replicas: 1 template: metadata: labels: app: {{ template &#34;fullname&#34; . }} spec: # hostNetwork: true hostAliases: - ip: 127.0.0.1 hostnames: - pi.hole nodeSelector: kubernetes.io/hostname: randomstore containers: - name: {{ .Chart.Name }} image: pihole/pihole:v5.1.1 imagePullPolicy: {{ .Values.image.pullPolicy }} stdin: true tty: true resources: limits: memory: 1Gi env: - name: &#39;ServerIP&#39; value: &#39;192.168.1.132&#39; - name: &#39;DNS1&#39; value: &#39;8.8.8.8&#39; - name: &#39;DNS2&#39; value: &#39;8.8.4.4&#39; - name: TZ value: &#34;America/New_York&#34; - name: WEBPASSWORD value: &#34;somepassword&#34; #values.yaml configData: |- server=/local/192.168.1.1 address=/.vikaspogu.com/192.168.1.132 ingress: host: pi-hole.vikaspogu.com Install chart # Create a new namespace (optional) Install chart in namespace kubectl create ns pi-hole helm install pi-hole. Wait for pods kubectl get pods NAME READY STATUS RESTARTS AGE pi-hole-pihole-5bb56b5bd-b2wl7 1/1 Running 0 61m Navigate to the ingress route in my case (pi-hole.vikaspogu.com) and log in with WEBPASSWORD used in the deployment DNS Server # Configure Verizon FiOS router to use Pi-Hole as the DNS server:
On the top navigation menu
Click My Network On the left menu list
Click Network Connections Click Broadband Connection (Ethernet/Coax)&gt;Settings
Click the drop-down for DNS Server and select &ldquo;Use The Following DNS Server Addresses&rdquo;
Type in the static IP Address of your pi (Or Pi-hole server)
Click Apply
`}).add({id:20,href:"/blog/openshift-jenkins-oauth-ssl/",title:"Jenkins OpenShift OAuth SSL",description:"Add SSL certificates for OpenShift Jenkins Authentication using OpenShift OAuth plugin.",content:` Jenkins SSL # Problem # I recently encountered an issue while authenticating to OpenShift Jenkins using the OpenShift OAuth plugin, where trusted certificates provided by CA aren&rsquo;t included in the default JRE TrustStore.
Logs from Jenkins pod
2020-02-10 21:19:07.335+0000 [id=17] INFO o.o.j.p.o.OpenShiftOAuth2SecurityRealm#transportToUse: OpenShift OAuth got an SSL error when accessing the issuer&#39;s token endpoint when using the SA certificate2020-02-10 21:19:07.348+0000 [id=17] INFO o.o.j.p.o.OpenShiftOAuth2SecurityRealm#transportToUse: OpenShift OAuth provider token endpoint failed unexpectedly using the JVMs default keystore sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at java.base/sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141) at java.base/sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126) at java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297) at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:434) Caused: sun.security.validator.ValidatorException: PKIX path building failed at java.base/sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439) at java.base/sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306) at java.base/sun.security.validator.Validator.validate(Validator.java:264) at java.base/sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:313) at java.base/sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:222) at java.base/sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:129) at java.base/sun.security.ssl.CertificateMessage$T12CertificateConsumer.checkServerCerts(CertificateMessage.java:629) Caused: javax.net.ssl.SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target at java.base/sun.security.ssl.Alert.createSSLException(Alert.java:131) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:320) at java.base/sun.security.ssl.TransportContext.fatal(TransportContext.java:263) Solution # By default, Java Applications (as Jenkins) use the JVM TrustStore. Therefore, if a Java Application needs to use an additional TrustStore, it must be configured.
First, create a secret with the JKS Keystore and password.
oc create secret generic jenkins-https-jks --from-literal=https-jks-password=changeit \\ --from-file=custom-keystore.jks Mount jenkins-https-jks keystore secret as a volume to /var/jenkins_keystore location.
... terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/jenkins name: jenkins-data - mountPath: /var/jenkins_keystore name: jenkins-https-keystore serviceAccount: jenkins serviceAccountName: jenkins volumes: - name: jenkins-data persistentVolumeClaim: claimName: jenkins - name: jenkins-https-keystore secret: defaultMode: 420 items: - key: custom-keystore.jks path: custom-keystore.jks secretName: jenkins-https-jks ... Add the certificate to Jenkins as startup parameters; we can configure the Jenkins server to add the following JAVA properties to the JAVA_TOOL_OPTIONS environment variable.
-Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks \\ -Djavax.net.ssl.trustStorePassword=changeit spec: containers: - env: - name: JENKINS_HTTPS_KEYSTORE_PASSWORD valueFrom: secretKeyRef: key: https-jks-password name: jenkins-https-jks - name: JAVA_TOOL_OPTIONS value: -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true -Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks -Djavax.net.ssl.trustStorePassword=$(JENKINS_HTTPS_KEYSTORE_PASSWORD) The final deployment config should look like this:
apiVersion: apps.OpenShift.io/v1 kind: DeploymentConfig metadata: annotations: template.alpha.OpenShift.io/wait-for-ready: &#34;true&#34; creationTimestamp: &#34;2020-02-10T17:49:20Z&#34; generation: 10 labels: app: jenkins-persistent name: jenkins spec: replicas: 1 revisionHistoryLimit: 10 selector: name: jenkins strategy: activeDeadlineSeconds: 21600 recreateParams: timeoutSeconds: 600 resources: {} type: Recreate template: metadata: creationTimestamp: null labels: name: jenkins spec: containers: - env: - name: OpenShift_ENABLE_OAUTH value: &#34;true&#34; - name: OpenShift_ENABLE_REDIRECT_PROMPT value: &#34;true&#34; - name: DISABLE_ADMINISTRATIVE_MONITORS value: &#34;false&#34; - name: KUBERNETES_MASTER value: https://kubernetes.default:443 - name: KUBERNETES_TRUST_CERTIFICATES value: &#34;true&#34; - name: JENKINS_SERVICE_NAME value: jenkins - name: JNLP_SERVICE_NAME value: jenkins-jnlp - name: ENABLE_FATAL_ERROR_LOG_FILE value: &#34;false&#34; - name: JENKINS_UC_INSECURE value: &#34;false&#34; - name: JENKINS_HTTPS_KEYSTORE_PASSWORD valueFrom: secretKeyRef: key: https-jks-password name: jenkins-https-jks - name: JAVA_TOOL_OPTIONS value: -XX:+UnlockExperimentalVMOptions -Dsun.zip.disableMemoryMapping=true -Djavax.net.ssl.trustStore=/var/jenkins_keystore/custom-keystore.jks -Djavax.net.ssl.trustStorePassword=$(JENKINS_HTTPS_KEYSTORE_PASSWORD) image: image-registry.OpenShift-image-registry.svc:5000/OpenShift/jenkins@sha256:dd5f1c5d14a8a72aa4ca51224c26a661c2e4f19ea3e5f9b7d8343f4952de5f0d imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 2 httpGet: path: /login port: 8080 scheme: HTTP initialDelaySeconds: 420 periodSeconds: 360 successThreshold: 1 timeoutSeconds: 240 name: jenkins readinessProbe: failureThreshold: 3 httpGet: path: /login port: 8080 scheme: HTTP initialDelaySeconds: 3 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 240 resources: limits: memory: 1Gi securityContext: capabilities: {} privileged: false terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/lib/jenkins name: jenkins-data - mountPath: /var/jenkins_keystore name: jenkins-https-keystore dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: jenkins serviceAccountName: jenkins terminationGracePeriodSeconds: 30 volumes: - name: jenkins-data persistentVolumeClaim: claimName: jenkins - name: jenkins-https-keystore secret: defaultMode: 420 items: - key: custom-keystore.jks path: custom-keystore.jks secretName: jenkins-https-jks test: false triggers: [] status: {} Start a new deployment.
oc rollout latest jenkins Once new pods are running, login will redirect to the Jenkins home page.
`}).add({id:21,href:"/blog/podman-macos/",title:"Installing Podman remote client on macOS using vagrant",description:"Installing Podman remote client on macOS using vagrant.",content:`I am installing podman as a remote client on macOS using Vagrant. I will not cover the vagrant setup in this post.
Podman remote client # Podman is the tool to start and manage containers. On macOS, we have to use a thin remote client that connects to an actual Podman process running on a Linux host.
Here are the main steps how to configure the remote client to work with a Linux host:
Create a Linux machine using Vagrant Set key based ssh as root to the Linux host Install remote-client binary with Homebrew: brew cask install podman Create a fedora vagrant box.
mkdir fedora-box &amp;&amp; cd fedora-box echo &#34;Vagrant.configure(&#34;2&#34;) do |config| config.vm.box = &#34;generic/fedora30&#34; config.vm.hostname = &#34;fedora30&#34; config.vm.provider &#34;virtualbox&#34; do |v| v.memory = 1024 v.cpus = 1 end end&#34; &gt;&gt; Vagrantfile vagrant up &amp;&amp; vagrant ssh Create new ssh keys on macOS and copy the newly generated public key.
ssh-keygen cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3... Add ssh keys copied earlier on the Linux host to .ssh/authorized_keys
echo &#34;ssh-rsa AAAAB3...&#34; &gt;&gt; /root/.ssh/authorized_keys On the Linux host, install Podman and varlink socket. The remote client uses this to execute commands calling Podman\u2019s API.
sudo dnf --enablerepo=updates-testing install podman libvarlink-util libvarlink Install podman on macOS using Homebrew
brew cask install podman Once podman is installed, create a connection parameters in $HOME/.config/containers/podman-remote.conf
cat &lt;&lt;EOF &gt;$HOME/.config/containers/podman-remote.conf [connections] [connections.host1] destination = &#34;127.0.0.1&#34; username = &#34;root&#34; default = true port = 2222 EOF # With the remoting file configured, we can run podman simply as: podman images REPOSITORY TAG IMAGE ID CREATED SIZE Verify running a container:
podman run --name tomcat -d docker.io/tomcat Trying to pull docker.io/tomcat .... d2e6db3c7.... Building images:
Note: The podman-remote.conf file seems to be ignored by the podman build command, so we have to add --remote-host 127.0.0.1 --username root --port 2222 to each command
podman --remote-host 127.0.0.1 --username root --port 2222 build --tag mytag `}).add({id:22,href:"/blog/raspberry-pi-temp-telegraf/",title:"Measure Raspberry Pi temperature using Telegraf, Influxdb, Grafana on k3s",description:"Measure Raspberry Pi temperature using Telegraf, Influxdb, Grafana on k3s.",content:`In my previous post, I went through the k3s cluster home setup. Now, I&rsquo;ll show how to measure the temperature of those Raspberry Pi&rsquo;s using Telegraf, Influxdb, Grafana, and Helm charts.
Why Telegraf? # Telegraf has a plugin called exec, which can execute the commands on the host machine at a specific interval and parses those metrics from their output in any one of the accepted input data formats.
First, deploy the influxdb time series database chart.
apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: influxdb namespace: kube-system spec: chart: stable/influxdb targetNamespace: monitoring Get Pi temperature # I found this one-liner /sys/class/thermal/thermal_zone0/temp, which returns the temperature of the Pi; divide the output by 1000 to get a result in \xB0C and use awk to have a float value.
awk &#39;{print $1/1000}&#39; /sys/class/thermal/thermal_zone0/temp Update Chart values # Update chart values, add [inputs.exec] to config, and deploy it
apiVersion: helm.cattle.io/v1 kind: HelmChart metadata: name: telegraf namespace: kube-system spec: chart: stable/telegraf targetNamespace: monitoring valuesContent: |- replicaCount: 2 image: repo: &#34;telegraf&#34; tag: &#34;latest&#34; pullPolicy: IfNotPresent env: - name: HOSTNAME valueFrom: fieldRef: fieldPath: spec.nodeName config: inputs: - exec: commands: [&#34;awk &#39;{print $1/1000}&#39; /sys/class/thermal/thermal_zone0/temp&#34;] name_override: &#34;rpi_temp&#34; data_format: &#34;value&#34; data_type: &#34;float&#34; Add Datasource # Once influxdb and telegraf pods are ready, add influxdb Datasource in grafana.
Grafana # For Grafana visualization, import this dashboard.
`}).add({id:23,href:"/blog/kubernetes-home-cluster-traefik/",title:"k3s cluster with Raspberry-Pi, Traefik",description:"Walk through on setting up K3S cluster on raspberry pi and traefik ingress with cloudfare.",content:`In this post, I\u2019ll share my home lab setup for Rancher&rsquo;s k3s Kubernetes cluster.
Use-case:
A web interface to be accessible outside of my home so I could check and manage devices while away Way to manage dynamic DNS since I don\u2019t have a static I.P. Setup # Setting up a master + single node Kubernetes cluster Deploying DNS updater as a Kubernetes CronJob object Deploying Traefik as a Kubernetes Ingress Controller and configuring it to manage SSL with Let\u2019s Encrypt Setting up a Pi Kubernetes Cluster # I followed an excellent guide by Alex Ellis here to initialize a cluster on the master and then join a single node.
k3s kubectl get nodes NAME STATUS ROLES AGE VERSION pi-node1 Ready &lt;none&gt; 3d v1.16.3-k3s.2 pi-master Ready master 3d v1.16.3-k3s.2 DNS and Routing # Add a DNS entry for the wildcard domain *.home.vikaspogu.com to point at the dynamic I.P. Open ports 80 and 443 on the router\u2019s firewall At this point, a short dig on the domain should return your dynamic I.P.
$ dig +short test.home.vikaspogu.com X.X.X.X. I found this script online, which will update the DNS record if Dynamic I.P. is changed.
#!/bin/sh zone=example.com # dnsrecord is the A record which the script will update dnsrecord=www.example.com EMAIL=me@cloudflare.com API_KEY=1234567890abcdef1234567890abcdef # Get the current external I.P. address ip=$(dig +short &lt;CURRENT EXTERNAL IP&gt;) echo &#34;Current IP is $ip&#34; if host $dnsrecord 1.1.1.1 | grep &#34;has address&#34; | grep &#34;$ip&#34;; then echo &#34;$dnsrecord is currently set to $ip; no changes needed.&#34; exit fi # if here, the DNS record needs updating # get the zone id for the requested zone zoneid=$(curl -s -X GET &#34;https://api.cloudflare.com/client/v4/zones?name=$zone&amp;status=active&#34; \\ -H &#34;X-Auth-Email: $EMAIL&#34; \\ -H &#34;X-Auth-Key: $API_KEY&#34; \\ -H &#34;Content-Type: application/json&#34; | jq -r &#39;{&#34;result&#34;}[] | .[0] | .id&#39;) echo &#34;Zoneid for $zone is $zoneid&#34; # get the DNS record id dnsrecordid=$(curl -s -X GET &#34;https://api.cloudflare.com/client/v4/zones/$zoneid/dns_records?type=A&amp;name=$dnsrecord&#34; \\ -H &#34;X-Auth-Email: $EMAIL&#34; \\ -H &#34;X-Auth-Key: $API_KEY&#34; \\ -H &#34;Content-Type: application/json&#34; | jq -r &#39;{&#34;result&#34;}[] | .[0] | .id&#39;) echo &#34;DNSrecordid for $dnsrecord is $dnsrecordid&#34; # update the record curl -s -X PUT &#34;https://api.cloudflare.com/client/v4/zones/$zoneid/dns_records/$dnsrecordid&#34; \\ -H &#34;X-Auth-Email: $EMAIL&#34; \\ -H &#34;X-Auth-Key: $API_KEY&#34; \\ -H &#34;Content-Type: application/json&#34; \\ --data &#34;{\\&#34;type\\&#34;:\\&#34;A\\&#34;,\\&#34;name\\&#34;:\\&#34;$dnsrecord\\&#34;,\\&#34;content\\&#34;:\\&#34;$ip\\&#34;,\\&#34;ttl\\&#34;:1,\\&#34;proxied\\&#34;:false}&#34; | jq Create a configmap from the script, secret with Cloudflare EMAIL and GLOBAL_API_TOKEN.
$ k3s create configmap update-script --from-file=cloudfare-dns-update.sh $ k3s kubectl create secret generic cloudflare --from-literal=email=me@cloudflare.com \\ --from-literal=api_key=1234567890abcdef1234567890abcdef Now create a Kubernetes cronjob to update the DNS record to the correct address.
apiVersion: batch/v1beta1 kind: CronJob metadata: name: dns-update namespace: default spec: schedule: &#34;0 0 * * *&#34; concurrencyPolicy: Forbid jobTemplate: spec: template: spec: restartPolicy: OnFailure containers: - name: dns-update image: e2eteam/dnsutils:1.1-linux-arm command: [&#34;/bin/sh&#34;, &#34;-c&#34;, &#34;/scripts/cloudfare-dns-update.sh&#34;] env: - name: API_KEY valueFrom: secretKeyRef: name: cloudflare key: api_key - name: EMAIL valueFrom: secretKeyRef: name: cloudflare key: email volumeMounts: - name: config-volume mountPath: /scripts volumes: - name: config-volume configMap: name: update-script defaultMode: 0744 Traefik and Let\u2019s Encrypt # With a functioning cluster and the networking setup complete, the next task is to deploy a reverse proxy to manage the application routing.
In Kubernetes, we can deploy an Ingress Controller to achieve this. An Ingress Controller implements a reverse proxy that listens for changes to KubernetesIngress resources and updates its configuration accordingly.
Traefik provides detailed instructions on Kubernetes implementation, but I customized it slightly to get things working with my setup.
First, create RoleBinding.
k3s kubectl apply -f https://raw.githubusercontent.com/containous/traefik/v1.7/examples/k8s/traefik-rbac.yaml Configure Let\u2019s Encrypt to support HTTPS endpoint and automatically fetch certificates. I used Cloudflare as the DNS provider, configuring Traefik to use DNS records for domain validation.
[acme] email = &#34;me@cloudflare.com&#34; storage=&#34;./acme.json&#34; entryPoint = &#34;https&#34; acmeLogging=true [acme.dnsChallenge] provider = &#34;cloudflare&#34; delayBeforeCheck = 0 [[acme.domains]] main = &#34;*.home.vikaspogu.com&#34; sans = [&#34;home.vikaspogu.com&#34;] The Deployment objects look like this:
apiVersion: v1 data: traefik.toml: | # traefik.toml logLevel = &#34;info&#34; debug = true insecureSkipVerify = true defaultEntryPoints = [&#34;http&#34;,&#34;https&#34;] [entryPoints] [entryPoints.http] address = &#34;:80&#34; [entryPoints.http.redirect] entryPoint = &#34;https&#34; [entryPoints.https] address = &#34;:443&#34; [entryPoints.https.tls] [api] dashboard = true [kubernetes] [acme] email = &#34;me@cloudflare.com&#34; storage=&#34;./acme.json&#34; entryPoint = &#34;https&#34; acmeLogging=true [acme.dnsChallenge] provider = &#34;cloudflare&#34; delayBeforeCheck = 0 [[acme.domains]] main = &#34;*.home.vikaspogu.com&#34; sans = [&#34;home.vikaspogu.com&#34;] kind: ConfigMap metadata: labels: app: traefik name: traefik-config namespace: default --- apiVersion: v1 kind: ServiceAccount metadata: namespace: default name: traefik-ingress-controller labels: app: traefik --- kind: Deployment apiVersion: apps/v1 metadata: namespace: default name: traefik labels: app: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: labels: app: traefik spec: serviceAccountName: traefik-ingress-controller containers: - name: traefik image: traefik:1.7.4 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: admin containerPort: 8080 env: - name: CF_API_EMAIL valueFrom: secretKeyRef: name: cloudflare key: email - name: CF_API_KEY valueFrom: secretKeyRef: name: cloudflare key: api_key volumeMounts: - mountPath: &#34;/config&#34; name: &#34;config&#34; args: - --configfile=/config/traefik.toml volumes: - name: config configMap: name: traefik-config --- kind: Service apiVersion: v1 metadata: name: traefik-ingress-service namespace: default spec: selector: app: traefik ports: - protocol: TCP port: 80 name: http - protocol: TCP port: 443 name: https - protocol: TCP port: 8080 name: admin externalIPs: - 192.168.0.101 # This is the node address Deploy Ingress controller for traefik dashboard.
--- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: default spec: rules: - host: traefik.home.vikaspogu.com http: paths: - path: / backend: serviceName: traefik-ingress-service servicePort: admin Voila!
Deploy Kubernetes dashboard # Follow these instructions to deploy dashboard U.I..
Create an ingress controller to access the dashboard.
--- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard-ingress namespace: kubernetes-dashboard spec: rules: - host: kubernetes-dashboard.home.vikaspogu.com http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 443 `}).add({id:24,href:"/blog/ocp-docker-registry-500-err/",title:"Permission denied pushing to OpenShift Registry",description:"Permission denied on pushing build images into OpenShift internal registry",content:`Recently, I ran into an issue where pushing images to the docker registry after a build fails.
Pushing image docker-registry.default.svc:5000/simple-go-build/simple-go:latest ... Registry server Address: Registry server User Name: serviceaccount Registry server Email: serviceaccount@example.org Registry server Password: &lt;&lt;non-empty&gt;&gt; error: build error: Failed to push image: received unexpected HTTP status: 500 Internal Server Error Registry pods logs show permission denied.
err.code=UNKNOWN err.detail=&#34;filesystem: mkdir /registry/docker/registry/v2/repositories/simple-go-build/simple-go/_uploads/c34415b4-c6d8-42ba-9854-aee449efd984: permission denied&#34; One of the Red Hat solutions articles suggested verifying the file ownership of the files and directories in the volume and comparing it to the uid of the registry.
Changing the owner recursively to the uid of the registry fixed the issue
root@master# chown -R 1001 /exports/registry/docker/ `}).add({id:25,href:"/blog/golang-basicauth-gin/",title:"Basic Authentication in Go with Gin",description:"Basic user authentication with gin web framework in Golang",content:`This short post looks at adding basic authentication to GoLang applications. Below example application uses the gin web framework.
Let&rsquo;s start by creating a gin router with default middleware. By default, it serves on :8080 unless we define a PORT environment variable.
func main(){ r := gin.Default() r.GET(&#34;/getAllUsers&#34;, basicAuth, handlers.UsersList) _ = r.Run() } Now that we have our primary route let us create a method to add authentication logic. First, get basic auth credentials from the context request and validate them. The browser will prompt an authentication window for username and password details if the user is not authenticated.
func basicAuth(c *gin.Context) { // Get the Basic Authentication credentials user, password, hasAuth := c.Request.BasicAuth() if hasAuth &amp;&amp; user == &#34;testuser&#34; &amp;&amp; password == &#34;testpass&#34; { log.WithFields(log.Fields{ &#34;user&#34;: user, }).Info(&#34;User authenticated&#34;) } else { c.Abort() c.Writer.Header().Set(&#34;WWW-Authenticate&#34;, &#34;Basic realm=Restricted&#34;) return } } Run application
go run main.go Verify if authentication works
curl -X GET &#34;http://testuser:testpass@localhost:8080/getAllUsers&#34; `}).add({id:26,href:"/blog/create-nfs-server-rhel-rhv/",title:"Create and add NFS storage to RHV",description:`Set up NFS shares that will serve as storage domains on a Red Hat Enterprise Linux server.
Add a firewall rule to RHEL server.
firewall-cmd --permanent --add-service nfs firewall-cmd --permanent --add-service mountd firewall-cmd --reload Create and add group kvm; set the ownership of your exported directories to 36:36, which gives vdsm:kvm ownership; change the director&rsquo;s permission so that read and write access is granted to the owner.
groupadd kvm -g 36 useradd vdsm -u 36 -g 36 mkdir -pv /home/rhv-data-vol chown -R 36:36 /home/rhv-data-vol chmod 0755 /home/rhv-data-vol Add newly created directory to /etc/exports file, which controls file systems exported to remote hosts and specifies options.`,content:`Set up NFS shares that will serve as storage domains on a Red Hat Enterprise Linux server.
Add a firewall rule to RHEL server.
firewall-cmd --permanent --add-service nfs firewall-cmd --permanent --add-service mountd firewall-cmd --reload Create and add group kvm; set the ownership of your exported directories to 36:36, which gives vdsm:kvm ownership; change the director&rsquo;s permission so that read and write access is granted to the owner.
groupadd kvm -g 36 useradd vdsm -u 36 -g 36 mkdir -pv /home/rhv-data-vol chown -R 36:36 /home/rhv-data-vol chmod 0755 /home/rhv-data-vol Add newly created directory to /etc/exports file, which controls file systems exported to remote hosts and specifies options.
echo &#34;/home/rhv-data-vol 192.168.10.1(rw,sync)&#34; &gt;&gt; /etc/exports exportfs -av Start, enable NFS server; checklist of export server using showmount
systemctl start nfs systemctl enable nfs systemctl status nfs showmount -e `}).add({id:27,href:"/blog/couchbase-ssl-openshift/",title:"Configuring couchbase SSL for dynamic certificates in OpenShift",description:`Couchbase SSL # Suppose you have followed dynamic creation of java keystores in OpenShift post and wondered how to use similar concepts for couchbase database and a java application. This post will help you.
Couchbase setup # Here is the couchbase documentation for configuring server-side certificates, we are interested in last few steps since OpenShift will generate key and cert by adding an annotation to the couchbase service.
Note: By adding this annotation, you can dynamically create certificates service.`,content:` Couchbase SSL # Suppose you have followed dynamic creation of java keystores in OpenShift post and wondered how to use similar concepts for couchbase database and a java application. This post will help you.
Couchbase setup # Here is the couchbase documentation for configuring server-side certificates, we are interested in last few steps since OpenShift will generate key and cert by adding an annotation to the couchbase service.
Note: By adding this annotation, you can dynamically create certificates service.alpha.OpenShift.io/serving-cert-secret-name: couchbase-db-certs
couchbase service looks like this:
apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: &#34;&#34; service.alpha.OpenShift.io/serving-cert-secret-name: couchbase-db-certs labels: component: couchbase-db name: couchbase-db namespace: myproject spec: ports: - name: consolerest port: 8091 protocol: TCP targetPort: 8091 To convert certificates as couchbase expects, we will use an init container.
We will use an emptyDir volume to store the cert and key in the /opt/couchbase/var/lib/couchbase/inbox/ location so that couchbase can access them.
Init container will run a sequence of commands to split certificate, place them into /opt/couchbase/var/lib/couchbase/inbox/ location, and name cert file as chain.pem, key as pkey.key.
Init container will look as follows:
initContainers: - args: - &#34;-c&#34; - &gt;- csplit -z -f crt- $crtfile &#39;/-----BEGIN CERTIFICATE-----/&#39; &#39;{*}&#39; &amp;&amp; for file in crt-*; do cat $file &gt; /opt/couchbase/var/lib/couchbase/inbox/service-$file; done &amp;&amp; cat $crtfile &gt; /opt/couchbase/var/lib/couchbase/inbox/chain.pem &amp;&amp; cat $keyfile &gt; /opt/couchbase/var/lib/couchbase/inbox/pkey.key command: - /bin/bash env: - name: keyfile value: /var/run/secrets/OpenShift.io/services_serving_certs/tls.key - name: crtfile value: /var/run/secrets/OpenShift.io/services_serving_certs/tls.crt - name: password value: changeit image: registry.access.redhat.com/redhat-sso-7/sso72-OpenShift:latest imagePullPolicy: Always name: couchbase-ssl volumeMounts: - mountPath: /var/run/secrets/OpenShift.io/services_serving_certs name: couchbase-db-certs - mountPath: /opt/couchbase/var/lib/couchbase/inbox/ name: couchbase-ssl-volume volumes: - emptyDir: {} name: couchbase-ssl-volume - name: couchbase-db-certs secret: defaultMode: 420 secretName: couchbase-db-certs Next, add couchbase-ssl-volume emptyDir volume mount to the actual container so the file can be accessed by couchbase.
spec: containers: - env: ... volumeMounts: - mountPath: /opt/couchbase/var/lib/couchbase/inbox/ name: couchbase-ssl-volume I am using a rhel7-couchbase image; on startup, it runs an initialization script to set up the cluster; at that time, we will upload the certificate and activate it using these commands.
couchbase-cli ssl-manage -c http://localhost:8091 -u Administrator \\ -p password --upload-cluster-ca=\${SERVICE_CERT} couchbase-cli ssl-manage -c http://localhost:8091 -u Administrator \\ -p password --set-node-certificate Pass the cert location as environment variable SERVICE_CERT in deployment config.
- env: - name: SERVICE_CERT value: /opt/couchbase/var/lib/couchbase/inbox/service-crt-01 Verify logs on the container
SUCCESS: Uploaded cluster certificate to http://localhost:8091 SUCCESS: Node certificate set We can also verify in couchbase UI
At this point, we completed the couchbase setup.
Application setup # We will be using same steps as SSL client from dynamically-creating-java-keystores-OpenShift post
To make a secure connection to the couchbase, it will need the trust store generated by the pem-to-truststore initContainer. Here is the client\u2019s app deployment config:
- apiVersion: v1 kind: DeploymentConfig metadata: labels: app: ssl-client name: ssl-client spec: replicas: 1 selector: deploymentconfig: ssl-client template: metadata: labels: app: ssl-client deploymentconfig: ssl-client spec: containers: - name: ssl-client image: ssl-client imagePullPolicy: Always env: - name: JAVA_OPTIONS value: -Djavax.net.ssl.trustStore=/var/run/secrets/java.io/keystores/truststore.jks -Djavax.net.ssl.trustStorePassword=changeit - name: POD_NAMESPACE valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.namespace volumeMounts: - mountPath: /var/run/secrets/java.io/keystores name: keystore-volume initContainers: - name: pem-to-truststore image: registry.access.redhat.com/redhat-sso-7/sso71-OpenShift:1.1-16 env: - name: ca_bundle value: /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt - name: truststore_jks value: /var/run/secrets/java.io/keystores/truststore.jks - name: password value: changeit command: [&#34;/bin/bash&#34;] args: [ &#34;-c&#34;, &#34;csplit -z -f crt- $ca_bundle &#39;/-----BEGIN CERTIFICATE-----/&#39; &#39;{*}&#39; &amp;&amp; for file in crt-*; do keytool -import -noprompt -keystore $truststore_jks -file $file -storepass changeit -alias service-$file; done&#34;, ] volumeMounts: - mountPath: /var/run/secrets/java.io/keystores name: keystore-volume volumes: - emtpyDir: {} name: keystore-volume The next step is to enable encryption and pass the path and password of the truststore generated by the initContainer
CouchbaseEnvironment env = DefaultCouchbaseEnvironment.builder().sslEnabled(true) .sslTruststoreFile(&#34;/var/run/secrets/java.io/keystores/truststore.jks&#34;) .sslTruststorePassword(&#34;changeit&#34;).build(); cachedCluster = CouchbaseCluster.create(env, &#34;couchbase-db&#34;) .authenticate(&#34;Administrator&#34;, &#34;password&#34;); Deploy your application; if successful, you should see similar output in container logs
2019-08-28 15:33:27.952 INFO 1 --- [cTaskExecutor-1] com.couchbase.client.core.CouchbaseCore : CouchbaseEnvironment: {sslEnabled=true, sslKeystoreFile=&#39;null&#39;, sslTruststoreFile=&#39;/var/run/secrets/java.io/keystores/truststore.jks&#39;, sslKeystorePassword=false, sslTruststorePassword=true, sslKeystore=null, sslTruststore=null, bootstrapHttpEnabled=true, bootstrapCarrierEnabled=true, bootstrapHttpDirectPort=8091, ... `}).add({id:28,href:"/blog/intro-tektoncd-ocp/",title:"TektonCD on OpenShift",description:`Recently I came across tektoncd project, The Tekton Pipelines project provides Kubernetes-style resources for declaring CI/CD-style pipelines caught my attention, and I started playing with it.
Basic Concepts # To create a Tekton pipeline, one does the following:
Create custom or install existing reusable Tasks Create a Pipeline and PipelineResources to define your application&rsquo;s delivery pipeline Create a PipelineRun to instantiate and invoke the pipeline Installing Tekton on OpenShift # Log in as a user with cluster-admin privileges.`,content:`Recently I came across tektoncd project, The Tekton Pipelines project provides Kubernetes-style resources for declaring CI/CD-style pipelines caught my attention, and I started playing with it.
Basic Concepts # To create a Tekton pipeline, one does the following:
Create custom or install existing reusable Tasks Create a Pipeline and PipelineResources to define your application&rsquo;s delivery pipeline Create a PipelineRun to instantiate and invoke the pipeline Installing Tekton on OpenShift # Log in as a user with cluster-admin privileges.
oc new-project tekton-pipelines oc adm policy add-scc-to-user anyuid -z tekton-pipelines-controller oc apply --filename https://storage.googleapis.com/tekton-releases/latest/release.yaml Install dashboard # The dashboard is a general-purpose, web-based UI for Tekton Pipelines. Tekton also has cli client
curl -L https://github.com/tektoncd/dashboard/releases/download/v0/gcr-tekton-dashboard.yaml | oc apply -f - Expose tekton-dashboard service as a route
oc expose service tekton-dashboard \\ -n tekton-pipelines \\ --name &#34;tekton-dashboard&#34; \\ --port=&#34;http&#34; Create pipeline # This pipeline builds an image from the source and starts a new rollout.
Create a new project
oc new-project tektontutorial Create a service account for running pipelines and enable it to run privileged pods for building images
oc create serviceaccount pipeline oc adm policy add-scc-to-user privileged -z pipeline oc adm policy add-role-to-user edit -z pipeline Optionally, create OpenShift objects(i.e DeploymentConfig, ImageStream, Service, Route)
--- apiVersion: image.OpenShift.io/v1 kind: ImageStream metadata: labels: app: go-sample name: go-sample --- apiVersion: apps.OpenShift.io/v1 kind: DeploymentConfig metadata: labels: app: go-sample name: go-sample spec: replicas: 1 revisionHistoryLimit: 10 selector: app: go-sample deploymentconfig: go-sample strategy: activeDeadlineSeconds: 21600 resources: {} rollingParams: intervalSeconds: 1 maxSurge: 25% maxUnavailable: 25% timeoutSeconds: 600 updatePeriodSeconds: 1 type: Rolling template: metadata: labels: app: go-sample deploymentconfig: go-sample spec: containers: - image: go-sample:latest imagePullPolicy: Always livenessProbe: failureThreshold: 3 httpGet: path: / port: 8080 scheme: HTTP initialDelaySeconds: 45 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 name: go-sample ports: - containerPort: 8080 protocol: TCP readinessProbe: failureThreshold: 3 httpGet: path: / port: 8080 scheme: HTTP initialDelaySeconds: 45 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 5 resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} terminationGracePeriodSeconds: 30 test: false triggers: - imageChangeParams: containerNames: - go-sample from: kind: ImageStreamTag name: go-sample:latest namespace: tektontutorial type: ImageChange - type: ConfigChange --- apiVersion: v1 kind: Service metadata: labels: app: go-sample name: go-sample spec: ports: - name: 8080-tcp port: 8080 protocol: TCP targetPort: 8080 selector: app: go-sample deploymentconfig: go-sample sessionAffinity: None type: ClusterIP --- apiVersion: route.OpenShift.io/v1 kind: Route metadata: labels: app: go-sample name: go-sample spec: port: targetPort: 8080-tcp to: kind: Service name: go-sample weight: 100 oc apply -f go-sample-template.yml The deployment will not be complete since there are no container images for the go-sample app.
Create a s2i-go, OpenShift-cli task. You can find more examples of reusable tasks in the Tekton Catalog and OpenShift Catalog repositories.
Note: Tasks consist of several steps that get executed sequentially. The pipeline will perform each task in a separate container within the same pod.
oc apply -f https://raw.githubusercontent.com/OpenShift/pipelines-catalog/master/s2i-go/s2i-go-task.yaml oc create -f https://raw.githubusercontent.com/tektoncd/catalog/master/OpenShift-client/OpenShift-client-task.yaml Create pipeline resources
--- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: go-sample-image spec: type: image params: - name: url value: image-registry.OpenShift-image-registry.svc:5000/tektontutorial/go-sample --- apiVersion: tekton.dev/v1alpha1 kind: PipelineResource metadata: name: go-sample-git spec: type: git params: - name: url value: https://github.com/go-training/helloworld oc apply -f resources.yml Create pipeline; pipelines have two tasks here build and deploy. Build uses s2i-go task to create an image from source and deploy uses OpenShift-client task to rollout latest deployment of go-sample
apiVersion: tekton.dev/v1alpha1 kind: Pipeline metadata: name: go-sample-pipeline spec: resources: - name: app-git type: git - name: app-image type: image tasks: - name: build taskRef: name: s2i-go kind: Task params: - name: TLSVERIFY value: &#34;false&#34; resources: inputs: - name: source resource: app-git outputs: - name: image resource: app-image - name: deploy taskRef: name: OpenShift-client kind: ClusterTask runAfter: - build params: - name: ARGS value: &#34;rollout latest go-sample&#34; oc apply -f go-sample-pipeline.yml Start pipeline on the dashboard or by tektoncli
tkn pipeline start go-sample-pipeline \\ -r app-git=go-sample-git -r app-image=go-sample-image \\ -s pipeline Click on Create PipelineRun, select pipeline, resources, and service account and start the pipeline.
Once finished, you should see all tasks marked in green.
The go-sample deployment should have one running pod.
`}).add({id:29,href:"/blog/sso-jwt-golang/",title:"Go JWT Authentication with Keycloak",description:`I recently worked on a React project with Go backend using Gin web framework. Keycloak was the authentication mechanism for the front end; I also wanted to secure the back end using JSON Web Tokens, which Keycloak provided on every login. JWT verification setup in the Go application was easy.
First, copy the RS256 algorithm public key value from Keycloak.
Send the token as an Authorization header.
axios .get(BACKEND_URL.concat(&#34;sampleendpoint&#34;), { headers: { Authorization: this.`,content:`I recently worked on a React project with Go backend using Gin web framework. Keycloak was the authentication mechanism for the front end; I also wanted to secure the back end using JSON Web Tokens, which Keycloak provided on every login. JWT verification setup in the Go application was easy.
First, copy the RS256 algorithm public key value from Keycloak.
Send the token as an Authorization header.
axios .get(BACKEND_URL.concat(&#34;sampleendpoint&#34;), { headers: { Authorization: this.state.token } }) .then(res =&gt; {}); Now Go-backend setup; let&rsquo;s install the jwt-go, gin-cors libraries:
$ go get -u github.com/dgrijalva/jwt-go $ go get -u github.com/gin-contrib/cors Add cors config to the router to allow the authorization header.
router.Use(cors.New(cors.Config{ AllowOrigins: []string{&#34;*&#34;}, AllowMethods: []string{&#34;GET&#34;, &#34;POST&#34;, &#34;PUT&#34;, &#34;DELETE&#34;, &#34;OPTIONS&#34;, &#34;HEAD&#34;}, AllowHeaders: []string{&#34;Origin&#34;, &#34;content-type&#34;, &#34;accept&#34;, &#34;authorization&#34;}, ExposeHeaders: []string{&#34;Content-Length&#34;}, AllowCredentials: true, MaxAge: 12 * time.Hour, })) Let&rsquo;s create a custom handler; add the public key from Keycloak and pass it to ParseRSAPublicKeyFromPEM, which will return a key. The key and token are then validated.
func VerifyToken(c *gin.Context) { SecretKey := &#34;-----BEGIN CERTIFICATE-----\\n&#34;+ &#34;MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEApn ...... +wnyuCHaCHp8P1yCnwIDAQAB&#34; + &#34;\\n-----END CERTIFICATE-----&#34; reqToken := c.GetHeader(&#34;Authorization&#34;) key, er := jwt.ParseRSAPublicKeyFromPEM([]byte(SecretKey)) if er != nil { fmt.Println(er) c.Abort() c.Writer.WriteHeader(http.StatusUnauthorized) c.Writer.Write([]byte(&#34;Unauthorized&#34;)) return } token, err := jwt.Parse(reqToken, func(token *jwt.Token) (interface{}, error) { // Don&#39;t forget to validate the alg is what you expect: if _, ok := token.Method.(*jwt.SigningMethodRSA); !ok { return nil, fmt.Errorf(&#34;Unexpected signing method: %v&#34;, token.Header[&#34;alg&#34;]) } return key, nil }) if err != nil { fmt.Println(err) c.Abort() c.Writer.WriteHeader(http.StatusUnauthorized) c.Writer.Write([]byte(&#34;Unauthorized&#34;)) return } if _, ok := token.Claims.(jwt.MapClaims); ok &amp;&amp; token.Valid { fmt.Println(&#34;token is valid&#34;) } } Add the handler to the route.
router.GET(&#34;/sample&#34;, VerifyToken(), handlers.SampleEndpoint) That&rsquo;s it; if the token is valid, you will get the data from the backend, or else you&rsquo;ll see 401 Unauthorized.
`}).add({id:30,href:"/blog/redhat-sso-react/",title:"React App with RedHat SSO or keycloak",description:`This post will show you how to secure a React app using RedHat SSO (upstream Keycloak). In this case, OpenID-connect is my identity provider.
Install the official Keycloak js adapter
npm i keycloak-js --save Add host and port information to the client; in my case, it&rsquo;s localhost:9000
In App.js, add a JavaScript object with the required configuration; you will find these configurations under Clients-&gt;Installation.
//keycloak init options const initOptions = { url: &#34;https://localhost:8080/auth&#34;, realm: &#34;test&#34;, clientId: &#34;react-app&#34;, onLoad: &#34;login-required&#34; }; By default, to authenticate, you need to call the login function.`,content:`This post will show you how to secure a React app using RedHat SSO (upstream Keycloak). In this case, OpenID-connect is my identity provider.
Install the official Keycloak js adapter
npm i keycloak-js --save Add host and port information to the client; in my case, it&rsquo;s localhost:9000
In App.js, add a JavaScript object with the required configuration; you will find these configurations under Clients-&gt;Installation.
//keycloak init options const initOptions = { url: &#34;https://localhost:8080/auth&#34;, realm: &#34;test&#34;, clientId: &#34;react-app&#34;, onLoad: &#34;login-required&#34; }; By default, to authenticate, you need to call the login function. However, there are two options available to make the adapter automatically authenticate. First, you can pass login-required or check-sso to the init function. login-required will authenticate the client if the user is logged-in to {project_name} or display the login page if not. check-sso will only authenticate the client if the user is already logged-in; if the user is not logged in, the browser will be redirected back to the application and remain unauthenticated.
componentDidMount() { let keycloak = Keycloak(initOptions); keycloak.init({ onLoad: initOptions.onLoad }).success(authenticated =&gt; {}); } Finally
import * as Keycloak from &#34;keycloak-js&#34;; //keycloak init options const initOptions = { url: &#34;https://localhost:8080/auth&#34;, realm: &#34;test&#34;, clientId: &#34;react-app&#34;, onLoad: &#34;login-required&#34; }; class App extends React.Component { constructor(props) { super(props); this.state = { keycloak: null, authenticated: false }; } componentDidMount() { let keycloak = Keycloak(initOptions); keycloak.init({ onLoad: initOptions.onLoad }).success(authenticated =&gt; { this.setState({ keycloak: keycloak, authenticated: authenticated }); }); } render() { const { keycloak, authenticated } = this.state; if (keycloak) { if (authenticated) { return ( &lt;React.Fragment&gt; &lt;AppRouter /&gt; &lt;/React.Fragment&gt; ); } } return &lt;div&gt;Initializing SS0...&lt;/div&gt;; } } Run your app
$ npm run start Navigate to http://localhost:9000; you should see the login page if the user is not authenticated.
`}).add({id:31,href:"/blog/heketi-jwt-error/",title:"Heketi JWT token expired error",description:`Recently I encountered a JWT token expired error on the heketi pod in the OpenShift cluster.
[jwt] ERROR 2019/07/16 19:17:14 heketi/middleware/jwt.go:66:middleware.(*HeketiJwtClaims).Valid: exp validation failed: Token is expired by 1h48m59s After a lot of google searches, I synchronized clocks across the pod running heketi and the master nodes, which solved the issue
ntpdate -q 0.rhel.pool.ntp.org; systemctl restart ntpd `,content:`Recently I encountered a JWT token expired error on the heketi pod in the OpenShift cluster.
[jwt] ERROR 2019/07/16 19:17:14 heketi/middleware/jwt.go:66:middleware.(*HeketiJwtClaims).Valid: exp validation failed: Token is expired by 1h48m59s After a lot of google searches, I synchronized clocks across the pod running heketi and the master nodes, which solved the issue
ntpdate -q 0.rhel.pool.ntp.org; systemctl restart ntpd `}).add({id:32,href:"/blog/patternfly-setup-react/",title:"Patternfly setup in React Application",description:`To order to integrate Patternfly framework into a ReactJS application, create a new project or use an existing one
npx create-react-app patternfly-setup-react Install patternfly dependencies react-core, react-table and patternfly
npm i --save @patternfly/patternfly \\ @patternfly/react-core @patternfly/react-table Note: Import base.css and patternfly.css in your project, or some components may diverge in appearance
//These imports are a must to render CSS import &#34;@patternfly/react-core/dist/styles/base.css&#34;; import &#34;@patternfly/patternfly/patternfly.css&#34;; To make sure everything is working correctly, update App.`,content:`To order to integrate Patternfly framework into a ReactJS application, create a new project or use an existing one
npx create-react-app patternfly-setup-react Install patternfly dependencies react-core, react-table and patternfly
npm i --save @patternfly/patternfly \\ @patternfly/react-core @patternfly/react-table Note: Import base.css and patternfly.css in your project, or some components may diverge in appearance
//These imports are a must to render CSS import &#34;@patternfly/react-core/dist/styles/base.css&#34;; import &#34;@patternfly/patternfly/patternfly.css&#34;; To make sure everything is working correctly, update App.js with a demo layout from documentation
import React from &#34;react&#34;; import { Avatar, Brand, Button, ButtonVariant ... ... } from &#34;@patternfly/react-core&#34;; import accessibleStyles from &#34;@patternfly/react-styles/css/utilities/Accessibility/accessibility&#34;; import spacingStyles from &#34;@patternfly/react-styles/css/utilities/Spacing/spacing&#34;; import { css } from &#34;@patternfly/react-styles&#34;; import { BellIcon, CogIcon } from &#34;@patternfly/react-icons&#34;; //These imports are a must to render CSS import &#34;@patternfly/react-core/dist/styles/base.css&#34;; import &#34;@patternfly/patternfly/patternfly.css&#34;; class App extends React.Component { ... ... return ( &lt;React.Fragment&gt; &lt;Page header={Header} sidebar={Sidebar} isManagedSidebar skipToContent={PageSkipToContent} &gt; &lt;PageSection variant={PageSectionVariants.light}&gt; &lt;TextContent&gt; &lt;Text component=&#34;h1&#34;&gt;Main Title&lt;/Text&gt; &lt;Text component=&#34;p&#34;&gt; Body text should be Overpass Regular at 16px. It should have leading of 24px because &lt;br /&gt; of its relative line height of 1.5. &lt;/Text&gt; &lt;/TextContent&gt; &lt;/PageSection&gt; &lt;PageSection&gt; &lt;Gallery gutter=&#34;md&#34;&gt; {Array.apply(0, Array(10)).map((x, i) =&gt; ( &lt;GalleryItem key={i}&gt; &lt;Card&gt; &lt;CardBody&gt;This is a card&lt;/CardBody&gt; &lt;/Card&gt; &lt;/GalleryItem&gt; ))} &lt;/Gallery&gt; &lt;/PageSection&gt; &lt;/Page&gt; &lt;/React.Fragment&gt; ); } } export default App; Start the application
npm start You should see a patternfly design like this!
`}).add({id:33,href:"/blog/node-ldap-auth/",title:"Authenticate a Node application with LDAP",description:`This post demonstrates how to authenticate a user against LDAP.
Let&rsquo;s start by installing basic-auth and ldapauth-fork packages
npm install ldapauth-fork npm install basic-auth Steps for implementation;
Add packages Create an LDAP variable with authentication configuration Basic auth should prompt for your username and password. Once the user is found, verify the given password by trying to bind the user client with the found LDAP user object and the given password.`,content:`This post demonstrates how to authenticate a user against LDAP.
Let&rsquo;s start by installing basic-auth and ldapauth-fork packages
npm install ldapauth-fork npm install basic-auth Steps for implementation;
Add packages Create an LDAP variable with authentication configuration Basic auth should prompt for your username and password. Once the user is found, verify the given password by trying to bind the user client with the found LDAP user object and the given password. const auth = require(&#34;basic-auth&#34;); var LdapAuth = require(&#34;ldapauth-fork&#34;); var ldap = new LdapAuth({ url: &#34;ldap://ldap-url:389&#34;, bindDN: &#34;uid=rc,ou=AppAccounts,ou=People,ou=Entsys,dc=example.com&#34;, bindCredentials: &#34;credentials&#34;, searchBase: &#34;ou=entsys,dc=example.com&#34;, searchFilter: &#34;(uid={{username}})&#34;, reconnect: true }); app.use(&#34;/API/admin/&#34;, (req, res, next) =&gt; { const credentials = auth(req); if (credentials) { LDAP.authenticate(credentials.name, credentials.pass, function(err, user) { if (err) { console.log(err.message); return res .status(&#34;401&#34;) .set({ &#34;WWW-Authenticate&#34;: &#39;Basic realm=&#34;Access Denied&#34;&#39; }) .end(&#34;access denied&#34;); } req.user = user; next(); }); } else { return res .status(&#34;401&#34;) .set({ &#34;WWW-Authenticate&#34;: &#39;Basic realm=&#34;Access Denied&#34;&#39; }) .end(&#34;access denied&#34;); } }); Visit basic-auth, ldapauth-fork packages for more information on configuration.
`}).add({id:34,href:"/blog/project-terminating/",title:"Deleting an OpenShift project stuck in terminating state",description:`Recently I faced an issue where one of my projects got stuck in a terminating state for days. The workaround below fixed the problem.
Export OpenShift project as a JSON Object
oc get project delete-me -o json &gt; ns-without-finalizers.json Replace below from
spec: finalizers: - kubernetes to
spec: finalizers: [] On one of the master nodes, execute these commands.
kubectl proxy &amp; PID=$! curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize \\ -H &#34;Content-Type: application/json&#34; --data-binary @ns-without-finalizers.`,content:`Recently I faced an issue where one of my projects got stuck in a terminating state for days. The workaround below fixed the problem.
Export OpenShift project as a JSON Object
oc get project delete-me -o json &gt; ns-without-finalizers.json Replace below from
spec: finalizers: - kubernetes to
spec: finalizers: [] On one of the master nodes, execute these commands.
kubectl proxy &amp; PID=$! curl -X PUT http://localhost:8001/api/v1/namespaces/delete-me/finalize \\ -H &#34;Content-Type: application/json&#34; --data-binary @ns-without-finalizers.json kill $PID `}).add({id:35,href:"/blog/springboot-metrics-grafana/",title:"Spring Boot metrics with Prometheus and Grafana in OpenShift",description:`Spring Boot Metrics # This post will discuss how to monitor spring boot application metrics using Prometheus and Grafana.
Prometheus # Prometheus is a monitoring system that collects metrics from configured targets at intervals.
Grafana # Grafana is an open-source metric analytics &amp; visualization tool.
Micrometer # The micrometer is a metrics instrumentation library for JVM-based applications.
Spring Boot Actuator # Spring Boot Actuator helps you monitor and manage your application when it\u2019s pushed to production.`,content:` Spring Boot Metrics # This post will discuss how to monitor spring boot application metrics using Prometheus and Grafana.
Prometheus # Prometheus is a monitoring system that collects metrics from configured targets at intervals.
Grafana # Grafana is an open-source metric analytics &amp; visualization tool.
Micrometer # The micrometer is a metrics instrumentation library for JVM-based applications.
Spring Boot Actuator # Spring Boot Actuator helps you monitor and manage your application when it\u2019s pushed to production. You can control and monitor your application using HTTP or JMX endpoints.
Setup # Enable Prometheus metrics by adding dependencies in pom.xml
&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt; &lt;version&gt;1.0.6&lt;/version&gt; &lt;/dependency&gt; By default Prometheus endpoint is not available and must be enabled in application.properties. You can find more configurations at spring-boot docs
#Metrics related configurations management.endpoint.metrics.enabled=true management.endpoints.web.exposure.include=* management.endpoint.prometheus.enabled=true management.metrics.export.prometheus.enabled=true management.metrics.distribution.percentiles-histogram.http.server.requests=true management.metrics.distribution.sla.http.server.requests=1ms,5ms management.metrics.distribution.percentiles.http.server.requests=0.5,0.9,0.95,0.99,0.999 Optionally you can configure any number with the MeterRegistryCustomizer registry (such as applying common tags).
@Bean MeterRegistryCustomizer&lt;MeterRegistry&gt; metricsCommonTags() { return registry -&gt; registry.config().commonTags(&#34;application&#34;, &#34;sample-app&#34;); } Create a new project; deploy the application and Prometheus in OpenShift.
$ oc project myproject $ oc new-app redhat-openjdk18-OpenShift~&lt;git_repo_URL&gt; -n sample-app oc new-app prom/prometheus -n prometheus To keep the Prometheus image and configuration decoupled, use the ConfigMap object to inject the Prometheus deployment with the appropriate configuration data.
cat &lt;&lt;&#39;EOF&#39; &gt; prometheus.yml global: scrape_interval: 5s evaluation_interval: 5s scrape_configs: - job_name: &#39;sample-app&#39; metrics_path: &#39;/actuator/prometheus&#39; static_configs: - targets: [&#39;sample-app:8080&#39;] EOF oc create configmap prom-config-example --from-file=prometheus.yml Next, edit the deployment configuration for Prometheus to include this ConfigMap.
oc edit dc/prometheus Add new volume and volume mount.
- name: prom-config-example-volume configMap: name: prom-config-example defaultMode: 420 - name: prom-config-example-volume mountPath: /etc/prometheus/ Use an OpenShift Template to run Grafana with persistent storage.
$ oc process -f https://gist.githubusercontent.com/Vikaspogu/4a67495acf8dba5dc94837e031129fde/raw/e88f42515c6ed101c9554c7c2425794e80e10a64/OpenShift-grafana.yaml | oc apply -f- Once deployed, log in to Grafana using the Route provided in the Template and using the default account admin with password admin (it may be a good idea to change the password after this).
Grafana Data Source # The Grafana template automatically provisions a Prometheus data source, App-Prometheus, which connects to http://prometheus:9090 via a proxy connection.
This works if there is a Prometheus service (called Prometheus) in the same project as Grafana. If this is not the case, it is necessary to edit the data source to point to the appropriate location.
Grafana Dashboard # The Grafana template automatically provisions sample dashboards. These dashboards are not comprehensive, but you can use them as a starting point for further customization. You can find more official &amp; community built grafana dashboards here
`}).add({id:36,href:"/blog/debug-netcore-openshift/",title:"Debugging a .NET Core application running on OpenShift",description:`This post concerns remote debugging an ASP.NET Core application on OpenShift using Visual Studio Code. You can use any Microsoft proprietary debugger engine vsdbg with Visual Studio Code.
First, list the available .Net application pods using the oc command.
$ oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 0/1 Running 0 6s $ oc rsh MY_APP_NAME-3-1xrsp sh-4.2$ curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /opt/app-root/vsdbg -r linux-x64 Note: If your container is running behind a corporate proxy and cannot access the internet, you&rsquo;ll have to build a base dotnet image with the installed debugger engine vsdbg.`,content:`This post concerns remote debugging an ASP.NET Core application on OpenShift using Visual Studio Code. You can use any Microsoft proprietary debugger engine vsdbg with Visual Studio Code.
First, list the available .Net application pods using the oc command.
$ oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 0/1 Running 0 6s $ oc rsh MY_APP_NAME-3-1xrsp sh-4.2$ curl -sSL https://aka.ms/getvsdbgsh | bash /dev/stdin -v latest -l /opt/app-root/vsdbg -r linux-x64 Note: If your container is running behind a corporate proxy and cannot access the internet, you&rsquo;ll have to build a base dotnet image with the installed debugger engine vsdbg.
Create (or open) the .vscode/launch.json file inside the source directory of the application (i.e., at the same level as the project folder), then add the following:
{ &#34;version&#34;: &#34;0.1.0&#34;, &#34;configurations&#34;: [ { &#34;name&#34;: &#34;.NET Core OpenShift Pod Remote Attach&#34;, &#34;type&#34;: &#34;coreclr&#34;, &#34;request&#34;: &#34;attach&#34;, &#34;processId&#34;: &#34;1&#34;, &#34;pipeTransport&#34;: { &#34;pipeProgram&#34;: &#34;oc&#34;, &#34;pipeArgs&#34;: [&#34;exec&#34;, &#34;-it&#34;, &#34;&lt;replace-with-pod-name&gt;&#34;, &#34;--&#34;], &#34;quoteArgs&#34;: false, &#34;debuggerPath&#34;: &#34;/opt/app-root/vsdbg/vsdbg&#34;, &#34;pipeCwd&#34;: &#34;\${workspaceRoot}&#34; }, &#34;justMyCode&#34;: false, &#34;sourceFileMap&#34;: { &#34;/opt/app-root/src&#34;: &#34;\${workspaceRoot}&#34; } } ] } In Launch.json, replace &lt;replace-with-pod-name&gt; with the pod&rsquo;s name.
Confirm the PID of the dotnet process in the container. If different, replace the processId in launch.json with the appropriate value. Usually, this value is 1.
If the application is built with the Release configuration, the default for .NET Core S2I builder images, justMyCode should be false.
As S2I images build the source code in the /opt/app-root/src folder, we should specify this path for sourceFileMap.
Start debugging the .NET Core app by switching to the debug window, then select .NET Core OpenShift Pod Remote Attach as the configuration and click on the green play button (as shown below), or press the F5 key.
Debug .NET CORE Remember to add the breakpoints in the source code to see debugging in action!
`}).add({id:37,href:"/blog/debug-java-container/",title:"Debugging a Java application in OpenShift.",description:"This post will discuss debugging a JAVA application running inside a container.\nRed Hat container images # When you bootstrap your JVM, you should have a way to enable JVM to debug. For example, Red Hat S2I images allow you to control classpath and debugging via environment variables.\n# Set debug options if required if [ x&#34;${JAVA_DEBUG}&#34; != x ] &amp;&amp; [ &#34;${JAVA_DEBUG}&#34; != &#34;false&#34; ]; then java_debug_args=&#34;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=${JAVA_DEBUG_PORT:-5005}&#34; fi Setting the JAVA_DEBUG environment variable inside the container to true will append debug args to the JVM startup command Configure port forwarding so that you can connect to your application from a remote debugger If you are using the tomcat image, replace the JAVA_DEBUG environment variable with DEBUG",content:`This post will discuss debugging a JAVA application running inside a container.
Red Hat container images # When you bootstrap your JVM, you should have a way to enable JVM to debug. For example, Red Hat S2I images allow you to control classpath and debugging via environment variables.
# Set debug options if required if [ x&#34;\${JAVA_DEBUG}&#34; != x ] &amp;&amp; [ &#34;\${JAVA_DEBUG}&#34; != &#34;false&#34; ]; then java_debug_args=&#34;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=\${JAVA_DEBUG_PORT:-5005}&#34; fi Setting the JAVA_DEBUG environment variable inside the container to true will append debug args to the JVM startup command Configure port forwarding so that you can connect to your application from a remote debugger If you are using the tomcat image, replace the JAVA_DEBUG environment variable with DEBUG
Using the oc command, list the available deployment configurations:
oc get dc Enable Debug # Set the JAVA_DEBUG environment variable in the deployment configuration of your application to true, which configures the JVM to open the port number 5005 for debugging.
oc set env dc/MY_APP_NAME JAVA_DEBUG=true Disabling the health checks is not mandatory but recommended because a pod could be restarted during remote debugging while the process is paused. You can remove the readiness check to prevent a forced restart.
Redeploy # Redeploy the application if it is not set to redeploy automatically on configuration change.
oc rollout latest dc/MY_APP_NAME Configure port forwarding from your local machine to the application pod. List the currently running pods and find one containing your application. $LOCAL_PORT_NUMBER is an unused port number of your choice on your local machine. Remember this number for the remote debugger configuration.
If you are using the tomcat image, replace the port 5005 with 8000
oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 1/1 Running 0 6s ... oc port-forward MY_APP_NAME-3-1xrsp $LOCAL_PORT_NUMBER:5005 IntelliJ Config # Create a new debug configuration for your application in \`IntelliJ IDE:
Click Run \u2192 Edit Configurations
In the list of configurations, add Remote. Creates a new remote debugging configuration
Enter a suitable name for the configuration in the name field
Set the port field to the port number that your application is listening on for debugging
Click Apply
Click Run -&gt; Debug -&gt; Select Profile
When done debugging, unset the JAVA_DEBUG environment variable in your application pod.
oc set env dc/MY_APP_NAME JAVA_DEBUG- Non-Red Hat container images # If you are using the OpenJDK image to build an application, update ENTRYPOINT as below to pass options to the JVM through the $JAVA_OPTS environment variable
FROM openjdk:11.0.3-jdk-slim RUN mkdir /usr/myapp COPY target/java-kubernetes.jar /usr/myapp/app.jar WORKDIR /usr/myapp EXPOSE 8080 ENTRYPOINT [ &#34;sh&#34;, &#34;-c&#34;, &#34;java $JAVA_OPTS -jar app.jar&#34; ] And then set deployments JAVA_OPTS environment variable.
oc set env deployment MY_APP_NAME JAVA_OPTS=-agentlib:jdwp=transport=dt_socket,address=*:5005,server=y,suspend=n `}).add({id:38,href:"/blog/javaprofiler-openshift/",title:"Profiling an application in OpenShift container.",description:`Sometimes writing code that runs is not enough. We might want to know what goes on internally, such as memory allocation, consequences of using one coding approach over another, implications of concurrent executions, areas to improve performance, etc. We can use profilers for this.
In this post, I&rsquo;ll discuss using YourKit-JavaProfiler inside a container.
Since my sample application is built using OpenShift S2I process and pushed into OpenShift internal registry, I&rsquo;ll have to pull the image locally.`,content:`Sometimes writing code that runs is not enough. We might want to know what goes on internally, such as memory allocation, consequences of using one coding approach over another, implications of concurrent executions, areas to improve performance, etc. We can use profilers for this.
In this post, I&rsquo;ll discuss using YourKit-JavaProfiler inside a container.
Since my sample application is built using OpenShift S2I process and pushed into OpenShift internal registry, I&rsquo;ll have to pull the image locally.
docker login -p $(oc whoami --show-token) -u admin docker-registry.example.com docker pull docker-registry.example.com/myproject/sample-app:latest Create a new Dockerfile, add a few lines to install YourKit Java Profiler agents, and expose the profiler agent port.
FROM docker-registry.example.com/myproject/sample-app:latest RUN wget https://www.yourkit.com/download/docker/YourKit-JavaProfiler-2019.1-docker.zip -P /tmp/ &amp;&amp; \\ unzip /tmp/YourKit-JavaProfiler-2019.1-docker.zip -d /usr/local &amp;&amp; \\ rm /tmp/YourKit-JavaProfiler-2019.1-docker.zip EXPOSE 10001 Build and push the image into the registry
docker build . -t docker-registry.example.com/myproject/sample-app-profiler:latest docker push docker-registry.example.com/myproject/sample-app-profiler:latest Update the image in the deployment configuration.
Load the agent into the JVM by adding a JAVA_TOOL_OPTIONS environment variable in the deployment configuration.
$ oc set env dc/MY_APP_NAME JAVA_TOOL_OPTIONS=-agentpath:/usr/local/YourKit-JavaProfiler-2019.01/bin/linux-x86-64/libyjpagent.so=port=10001,listen=all oc rollout latest dc/MY_APP_NAME Once the deployment is completed, run oc port forwarding from your local machine to the application pod.
$ oc get pod NAME READY STATUS RESTARTS AGE MY_APP_NAME-3-1xrsp 1/1 Running 0 6s ... $ oc port-forward MY_APP_NAME-3-1xrsp 10001:10001 Add a connection in the profiler with localhost:10001, and you&rsquo;re all set.
`}),z.addEventListener("input",function(){let o=this.value,i=e.search(o,5,{enrich:!0}),s=new Map;for(let r of i.flatMap(l=>l.result))s.has(r.href)||s.set(r.doc.href,r.doc);if(B.innerHTML="",B.classList.remove("search__suggestions--hidden"),s.size===0&&o){let r=document.createElement("div");r.innerHTML=`No results for "<strong>${o}</strong>"`,r.classList.add("search__no-results"),B.appendChild(r);return}for(let[r,l]of s){let p=document.createElement("a");p.href=r,p.classList.add("search__suggestion-item"),B.appendChild(p);let h=document.createElement("div");h.textContent=l.title,h.classList.add("search__suggestion-title"),p.appendChild(h);let g=document.createElement("div");if(g.textContent=l.description,g.classList.add("search__suggestion-description"),p.appendChild(g),B.childElementCount===5)break}})})();})();
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
